{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9463f36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turing data json into pandas dataframe\n",
    "# wic_df = pd.read_json('WiC/train.jsonl', lines=True)\n",
    "# print(wic_df.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808bb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_transformers import *\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc07cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50d09989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 970'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "PATIENCE = 3\n",
    "# Prepare Torch to use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d070ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the jsonl files into json files\n",
    "def load_json_objects_from_file(filename):\n",
    "    json_objects = []\n",
    "    #Iterates throught the jsonl files and appends them to the json list\n",
    "    with open(filename, mode = \"r\") as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            json_objects.append(json.loads(line))\n",
    "    return json_objects\n",
    "\n",
    "#This will find the word within the given roBERTa tokenized sentence\n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "    decomposedWord = tokenizer.encode(word)\n",
    "    #This will iterate through the token ids, ie the given tokenized sentence, and find the word\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "            return (i,i+len(decomposedWord)-1)\n",
    "    # This is the ouput when no matching pattern is found\n",
    "    return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "    intList = []\n",
    "    for word in wordList:\n",
    "        if len(intList) == 0:\n",
    "            intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "        else:\n",
    "            afterLastInterval = intList[-1][1]+1\n",
    "            interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "            actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "            intList.append(actualPositions)\n",
    "    return intList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdf1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess the WiC data\n",
    "def wic_preprocessing(json_objects, training = True, shuffle_data = False, verbose = False):\n",
    "    wic_sentences = []\n",
    "    wic_encoded = []\n",
    "    wic_labels = []\n",
    "    wic_word_locs = []\n",
    "    wic_indexes = []\n",
    "    for index, example in enumerate(json_objects):\n",
    "        #wic_indexes.append(example['idx']) # Is it the index??\n",
    "        wic_indexes.append(index)\n",
    "        sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "        wic_sentences.append(sentence)\n",
    "        # Then encode the sentences\n",
    "        wic_encoded.append(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        # Find the word in each sentences\n",
    "        word = example['word']\n",
    "        word_locs = (-1, -1)\n",
    "        # Split the 2 sentences on space. (Also, lemmatize and uncapitilize each word)\n",
    "        sent1_split = example['sentence1'].split(' ')\n",
    "        sent2_split = example['sentence2'].split(' ')\n",
    "        # Get the index of word in both sentences\n",
    "        sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "        sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "        # Create a variable to keep track of the number of characters parsed in each sentence as we loop\n",
    "        sent_chars = 0\n",
    "        # Loop over the words in the first sentence\n",
    "        i, j = 0, 0\n",
    "        word1_not_found, word2_not_found = True, True\n",
    "        while word1_not_found and i < len(sent1_split):\n",
    "            word_len = len(sent1_split[i])\n",
    "            if sent_chars >= sent1_word_char_loc[0] or sent_chars + word_len >= sent1_word_char_loc[1]:\n",
    "                word_locs = (i, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            elif sent_chars > sent1_word_char_loc[1]:\n",
    "                # If we somehow got past the word. Assume it was the previous word\n",
    "                word_locs = (i - 1, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            else:\n",
    "                # Look at the next word\n",
    "                sent_chars += word_len + 1 # Plus one for the space\n",
    "                i += 1\n",
    "        # Loop over the words in the second\n",
    "        sent_chars = 0 # Reset\n",
    "        while word2_not_found and j < len(sent2_split):\n",
    "            word_len = len(sent2_split[j])\n",
    "            if sent_chars >= sent2_word_char_loc[0] or sent_chars + word_len >= sent2_word_char_loc[1]:\n",
    "                word_locs = (i, j) # Found the word in the sentence\n",
    "                word2_not_found = False\n",
    "            elif sent_chars > sent2_word_char_loc[1]:\n",
    "                # If we somehow got past the word. Assume it was the previous word\n",
    "                word_locs = (i, j - 1) # Found the word in the sentence\n",
    "                word2_not_found = False\n",
    "            else:\n",
    "                # Look at the next word\n",
    "                sent_chars += word_len + 1 # Plus one for the space\n",
    "                j += 1\n",
    "        # For testing\n",
    "        if verbose:\n",
    "            print(word)\n",
    "            print(sent1_split)\n",
    "            print(sent2_split)\n",
    "            print(word_locs)\n",
    "        # Now to find the word in the tokenized sentences\n",
    "        word1 = sent1_split[word_locs[0]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation (See https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "        word2 = sent2_split[word_locs[1]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n",
    "        token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "        wic_word_locs.append(token_word_locs)\n",
    "        # Get the label if we expect it to be there\n",
    "        if training:\n",
    "            if example['label']:\n",
    "                wic_labels.append(1)\n",
    "            else:\n",
    "                wic_labels.append(0)\n",
    "    # Pad the sequences and find the encoded word location in the combined input\n",
    "    max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "    wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "    for i in range(0, len(wic_encoded)):\n",
    "        enc_sentence = wic_encoded[i]\n",
    "        word_locs = wic_word_locs[i]\n",
    "        # Pad the sequences\n",
    "        ex_len = len(enc_sentence)\n",
    "        padded_sentence = enc_sentence.copy()\n",
    "        padded_sentence.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "        padded_mask = [1] * ex_len\n",
    "        padded_mask.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "        # Create the vector to get back the words after RoBERTa\n",
    "        token_word_locs = wic_word_locs[i]\n",
    "        first_word_loc = []\n",
    "        second_word_loc = []\n",
    "        len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "        len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "        for j in range(0, max_len):\n",
    "            if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "                #Part of the first word\n",
    "                first_word_loc.append(1.0 / len_first_word)\n",
    "            else:\n",
    "                first_word_loc.append(0.0)\n",
    "            if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "                #Part of the second word\n",
    "                second_word_loc.append(1.0 / len_second_word)\n",
    "            else:\n",
    "                second_word_loc.append(0.0)\n",
    "        # We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n",
    "        wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "        wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "        # token_type_ids is a mask that tells where the first and second sentences are\n",
    "        token_type_id = []\n",
    "        first_sentence = True\n",
    "        sentence_start = True\n",
    "        for token in padded_sentence:\n",
    "            if first_sentence and sentence_start and token == 0:\n",
    "                # Allows 0 at the start of the first sentence\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and token > 0:\n",
    "                if sentence_start:\n",
    "                    sentence_start = False\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and not sentence_start and token == 0:\n",
    "                first_sentence = False\n",
    "                # Start of second sentence\n",
    "                token_type_id.append(1)\n",
    "            else:\n",
    "                # Second sentence\n",
    "                token_type_id.append(1)\n",
    "        wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "    if training:\n",
    "        if shuffle_data:\n",
    "            # Shuffle the data\n",
    "            raw_set = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"labels\": [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : []}\n",
    "            raw_set[\"input_ids\"], raw_set[\"token_type_ids\"], raw_set[\"attention_mask\"], raw_set[\"labels\"], raw_set[\"word1_locs\"], raw_set[\"word2_locs\"], raw_set[\"index\"] = shuffle(\n",
    "              wic_padded[\"input_ids\"], wic_padded[\"token_type_ids\"], wic_padded[\"attention_mask\"], wic_labels, wic_padded[\"word1_locs\"], wic_padded[\"word2_locs\"], wic_padded[\"index\"])\n",
    "        else:\n",
    "            raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\n",
    "                     \"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\n",
    "                     \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    else: # No labels present (Testing set)\n",
    "        # Do not shuffle the testing set\n",
    "        raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \n",
    "               \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \n",
    "               \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    # Return the raw data (Need to put them in a PyTorch tensor and dataset)\n",
    "    return raw_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d19f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357.0\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "train_json_objs = load_json_objects_from_file(\"WiC/train.jsonl\")\n",
    "raw_train_set = wic_preprocessing(train_json_objs, shuffle_data=True, verbose = False) # We do not want to shuffle for now.\n",
    "print(len(raw_train_set[\"labels\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfca6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset for it\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(raw_train_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"index\"]).to(device)\n",
    ")\n",
    "# Create a sampler and loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35f4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json objects from each file\n",
    "test_json_objs = load_json_objects_from_file(\"WiC/test.jsonl\")\n",
    "valid_json_objs = load_json_objects_from_file(\"WiC/val.jsonl\")\n",
    "# Process the objects\n",
    "raw_test_set = wic_preprocessing(test_json_objs, training = False) # The labels for the testing set are unknown\n",
    "raw_valid_set = wic_preprocessing(valid_json_objs)\n",
    "# Create PyTorch datasets\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "# Create a sampler and loader for each\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812ecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in model\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, roberta_based_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        Keeps a reference to the provided RoBERTa model. \n",
    "        It then adds a linear layer that takes the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = roberta_based_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        Takes in the same argument as RoBERTa forward plus two tensors for the location of the 2 words to compare\n",
    "        \"\"\"\n",
    "        if word1_locs is None or word2_locs is None:\n",
    "            raise ValueError(\"The tensors (word1_locs, word1_locs) containing the location of the words to compare in the input vector must be provided.\")\n",
    "        elif input_ids is None:\n",
    "            raise ValueError(\"The input_ids tensor must be provided.\")\n",
    "        elif word1_locs.shape[0] != input_ids.shape[0] or word2_locs.shape[0] != input_ids.shape[0]:\n",
    "            raise ValueError(\"All provided vectors should have the same batch size.\")\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # Get the embeddings\n",
    "        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get the words\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        diff = word1s - word2s\n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        # Calculate the loss\n",
    "        if labels is not None:\n",
    "            #  We want seperation like a SVM so use Hinge loss\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa355be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE WiC MODEL\n",
    "class_model = WiC_Head(model, embedding_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f1b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    if return_predict_correctness:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "    else:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c09928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4502ddf87b86>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n",
      "C:\\Users\\green\\anaconda3\\lib\\site-packages\\pytorch_transformers\\optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1025.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 10: Loss: 0.6881631016731262; Accuracy: 0.5\n",
      "\t\tTraining Batch 20: Loss: 0.6880441904067993; Accuracy: 0.5\n",
      "\t\tTraining Batch 30: Loss: 0.7051643133163452; Accuracy: 0.25\n",
      "\t\tTraining Batch 40: Loss: 0.70003342628479; Accuracy: 0.25\n",
      "\t\tTraining Batch 50: Loss: 0.688072681427002; Accuracy: 0.75\n",
      "\t\tTraining Batch 60: Loss: 0.6815245151519775; Accuracy: 0.75\n",
      "\t\tTraining Batch 70: Loss: 0.6829959750175476; Accuracy: 0.75\n",
      "\t\tTraining Batch 80: Loss: 0.6893532276153564; Accuracy: 0.75\n",
      "\t\tTraining Batch 90: Loss: 0.6876422166824341; Accuracy: 0.75\n",
      "\t\tTraining Batch 100: Loss: 0.6964164972305298; Accuracy: 0.75\n",
      "\t\tTraining Batch 110: Loss: 0.6777034401893616; Accuracy: 0.5\n",
      "\t\tTraining Batch 120: Loss: 0.6960470676422119; Accuracy: 0.5\n",
      "\t\tTraining Batch 130: Loss: 0.6608750820159912; Accuracy: 0.5\n",
      "\t\tTraining Batch 140: Loss: 0.6653028726577759; Accuracy: 0.75\n",
      "\t\tTraining Batch 150: Loss: 0.6744694113731384; Accuracy: 1.0\n",
      "\t\tTraining Batch 160: Loss: 0.6807105541229248; Accuracy: 0.75\n",
      "\t\tTraining Batch 170: Loss: 0.7021108865737915; Accuracy: 0.25\n",
      "\t\tTraining Batch 180: Loss: 0.6761614084243774; Accuracy: 0.75\n",
      "\t\tTraining Batch 190: Loss: 0.696619987487793; Accuracy: 0.5\n",
      "\t\tTraining Batch 200: Loss: 0.686705470085144; Accuracy: 0.75\n",
      "\t\tTraining Batch 210: Loss: 0.6612553000450134; Accuracy: 0.75\n",
      "\t\tTraining Batch 220: Loss: 0.698503315448761; Accuracy: 0.5\n",
      "\t\tTraining Batch 230: Loss: 0.710491418838501; Accuracy: 0.5\n",
      "\t\tTraining Batch 240: Loss: 0.6616612076759338; Accuracy: 0.75\n",
      "\t\tTraining Batch 250: Loss: 0.6202207803726196; Accuracy: 1.0\n",
      "\t\tTraining Batch 260: Loss: 0.7019146680831909; Accuracy: 0.5\n",
      "\t\tTraining Batch 270: Loss: 0.6198537349700928; Accuracy: 1.0\n",
      "\t\tTraining Batch 280: Loss: 0.6841086745262146; Accuracy: 0.75\n",
      "\t\tTraining Batch 290: Loss: 0.7072659730911255; Accuracy: 0.5\n",
      "\t\tTraining Batch 300: Loss: 0.6676485538482666; Accuracy: 1.0\n",
      "\t\tTraining Batch 310: Loss: 0.6790193319320679; Accuracy: 0.75\n",
      "\t\tTraining Batch 320: Loss: 0.6833487749099731; Accuracy: 0.75\n",
      "\t\tTraining Batch 330: Loss: 0.6888718605041504; Accuracy: 0.75\n",
      "\t\tTraining Batch 340: Loss: 0.7388858795166016; Accuracy: 0.5\n",
      "\t\tTraining Batch 350: Loss: 0.6459355354309082; Accuracy: 0.75\n",
      "\t\tTraining Batch 360: Loss: 0.6945502758026123; Accuracy: 0.5\n",
      "\t\tTraining Batch 370: Loss: 0.6008238196372986; Accuracy: 1.0\n",
      "\t\tTraining Batch 380: Loss: 0.6444959044456482; Accuracy: 0.5\n",
      "\t\tTraining Batch 390: Loss: 0.6903461217880249; Accuracy: 0.75\n",
      "\t\tTraining Batch 400: Loss: 0.6934362649917603; Accuracy: 0.5\n",
      "\t\tTraining Batch 410: Loss: 0.6327304840087891; Accuracy: 0.75\n",
      "\t\tTraining Batch 420: Loss: 0.6848992109298706; Accuracy: 0.75\n",
      "\t\tTraining Batch 430: Loss: 0.6196507215499878; Accuracy: 0.75\n",
      "\t\tTraining Batch 440: Loss: 0.7061880826950073; Accuracy: 0.5\n",
      "\t\tTraining Batch 450: Loss: 0.7051444053649902; Accuracy: 0.5\n",
      "\t\tTraining Batch 460: Loss: 0.6875120401382446; Accuracy: 0.75\n",
      "\t\tTraining Batch 470: Loss: 0.8088468909263611; Accuracy: 0.0\n",
      "\t\tTraining Batch 480: Loss: 0.6403313279151917; Accuracy: 0.75\n",
      "\t\tTraining Batch 490: Loss: 0.6639449000358582; Accuracy: 0.75\n",
      "\t\tTraining Batch 500: Loss: 0.6333837509155273; Accuracy: 0.75\n",
      "\t\tTraining Batch 510: Loss: 0.7302326560020447; Accuracy: 0.5\n",
      "\t\tTraining Batch 520: Loss: 0.6103959679603577; Accuracy: 1.0\n",
      "\t\tTraining Batch 530: Loss: 0.713384747505188; Accuracy: 0.5\n",
      "\t\tTraining Batch 540: Loss: 0.4885413646697998; Accuracy: 1.0\n",
      "\t\tTraining Batch 550: Loss: 0.820591151714325; Accuracy: 0.25\n",
      "\t\tTraining Batch 560: Loss: 0.7432599067687988; Accuracy: 0.5\n",
      "\t\tTraining Batch 570: Loss: 0.5201840400695801; Accuracy: 1.0\n",
      "\t\tTraining Batch 580: Loss: 0.6021742820739746; Accuracy: 0.75\n",
      "\t\tTraining Batch 590: Loss: 0.5082554221153259; Accuracy: 1.0\n",
      "\t\tTraining Batch 600: Loss: 0.6157139539718628; Accuracy: 0.75\n",
      "\t\tTraining Batch 610: Loss: 0.6448544263839722; Accuracy: 0.5\n",
      "\t\tTraining Batch 620: Loss: 0.6965926885604858; Accuracy: 0.5\n",
      "\t\tTraining Batch 630: Loss: 0.7725762128829956; Accuracy: 0.25\n",
      "\t\tTraining Batch 640: Loss: 0.5603303909301758; Accuracy: 1.0\n",
      "\t\tTraining Batch 650: Loss: 0.7131065726280212; Accuracy: 0.5\n",
      "\t\tTraining Batch 660: Loss: 0.7221543788909912; Accuracy: 0.5\n",
      "\t\tTraining Batch 670: Loss: 0.5975356698036194; Accuracy: 1.0\n",
      "\t\tTraining Batch 680: Loss: 0.8171494603157043; Accuracy: 0.25\n",
      "\t\tTraining Batch 690: Loss: 0.7661133408546448; Accuracy: 0.5\n",
      "\t\tTraining Batch 700: Loss: 0.5859997868537903; Accuracy: 0.75\n",
      "\t\tTraining Batch 710: Loss: 0.5692008137702942; Accuracy: 0.5\n",
      "\t\tTraining Batch 720: Loss: 0.7508547306060791; Accuracy: 0.25\n",
      "\t\tTraining Batch 730: Loss: 0.5501995086669922; Accuracy: 0.75\n",
      "\t\tTraining Batch 740: Loss: 0.6278029680252075; Accuracy: 0.75\n",
      "\t\tTraining Batch 750: Loss: 0.6818739771842957; Accuracy: 0.5\n",
      "\t\tTraining Batch 760: Loss: 0.7880027294158936; Accuracy: 0.5\n",
      "\t\tTraining Batch 770: Loss: 0.558517336845398; Accuracy: 0.75\n",
      "\t\tTraining Batch 780: Loss: 0.615125834941864; Accuracy: 0.5\n",
      "\t\tTraining Batch 790: Loss: 0.5172761678695679; Accuracy: 0.75\n",
      "\t\tTraining Batch 800: Loss: 0.5441078543663025; Accuracy: 0.75\n",
      "\t\tTraining Batch 810: Loss: 0.43501222133636475; Accuracy: 1.0\n",
      "\t\tTraining Batch 820: Loss: 0.7475568056106567; Accuracy: 0.75\n",
      "\t\tTraining Batch 830: Loss: 0.639903724193573; Accuracy: 0.75\n",
      "\t\tTraining Batch 840: Loss: 0.8513388633728027; Accuracy: 0.25\n",
      "\t\tTraining Batch 850: Loss: 0.4726330637931824; Accuracy: 1.0\n",
      "\t\tTraining Batch 860: Loss: 0.7522412538528442; Accuracy: 0.5\n",
      "\t\tTraining Batch 870: Loss: 0.6251367926597595; Accuracy: 0.75\n",
      "\t\tTraining Batch 880: Loss: 0.8815718293190002; Accuracy: 0.25\n",
      "\t\tTraining Batch 890: Loss: 0.5759313702583313; Accuracy: 0.75\n",
      "\t\tTraining Batch 900: Loss: 0.6020427942276001; Accuracy: 0.75\n",
      "\t\tTraining Batch 910: Loss: 0.6085797548294067; Accuracy: 0.5\n",
      "\t\tTraining Batch 920: Loss: 0.5333791375160217; Accuracy: 0.75\n",
      "\t\tTraining Batch 930: Loss: 0.38405081629753113; Accuracy: 1.0\n",
      "\t\tTraining Batch 940: Loss: 0.7787841558456421; Accuracy: 0.5\n",
      "\t\tTraining Batch 950: Loss: 0.5558536052703857; Accuracy: 0.75\n",
      "\t\tTraining Batch 960: Loss: 0.4478394687175751; Accuracy: 1.0\n",
      "\t\tTraining Batch 970: Loss: 0.5258134007453918; Accuracy: 0.75\n",
      "\t\tTraining Batch 980: Loss: 0.539925217628479; Accuracy: 0.75\n",
      "\t\tTraining Batch 990: Loss: 0.3922862410545349; Accuracy: 1.0\n",
      "\t\tTraining Batch 1000: Loss: 0.4233984351158142; Accuracy: 1.0\n",
      "\t\tTraining Batch 1010: Loss: 0.3695366680622101; Accuracy: 1.0\n",
      "\t\tTraining Batch 1020: Loss: 0.6554118394851685; Accuracy: 0.5\n",
      "\t\tTraining Batch 1030: Loss: 0.4516908526420593; Accuracy: 1.0\n",
      "\t\tTraining Batch 1040: Loss: 0.518792450428009; Accuracy: 1.0\n",
      "\t\tTraining Batch 1050: Loss: 0.6223090291023254; Accuracy: 0.75\n",
      "\t\tTraining Batch 1060: Loss: 0.7150815725326538; Accuracy: 0.5\n",
      "\t\tTraining Batch 1070: Loss: 0.6681196689605713; Accuracy: 0.5\n",
      "\t\tTraining Batch 1080: Loss: 0.9255350232124329; Accuracy: 0.25\n",
      "\t\tTraining Batch 1090: Loss: 0.586274266242981; Accuracy: 0.75\n",
      "\t\tTraining Batch 1100: Loss: 0.45133569836616516; Accuracy: 0.75\n",
      "\t\tTraining Batch 1110: Loss: 0.7437433004379272; Accuracy: 0.5\n",
      "\t\tTraining Batch 1120: Loss: 0.3817657232284546; Accuracy: 1.0\n",
      "\t\tTraining Batch 1130: Loss: 0.4891059100627899; Accuracy: 1.0\n",
      "\t\tTraining Batch 1140: Loss: 0.49958235025405884; Accuracy: 0.75\n",
      "\t\tTraining Batch 1150: Loss: 0.6051627397537231; Accuracy: 0.75\n",
      "\t\tTraining Batch 1160: Loss: 0.46133899688720703; Accuracy: 1.0\n",
      "\t\tTraining Batch 1170: Loss: 0.4590629041194916; Accuracy: 0.75\n",
      "\t\tTraining Batch 1180: Loss: 0.39097702503204346; Accuracy: 1.0\n",
      "\t\tTraining Batch 1190: Loss: 0.5971944332122803; Accuracy: 0.75\n",
      "\t\tTraining Batch 1200: Loss: 0.3863643407821655; Accuracy: 1.0\n",
      "\t\tTraining Batch 1210: Loss: 0.585485577583313; Accuracy: 0.75\n",
      "\t\tTraining Batch 1220: Loss: 0.3460858166217804; Accuracy: 1.0\n",
      "\t\tTraining Batch 1230: Loss: 0.5115981101989746; Accuracy: 0.75\n",
      "\t\tTraining Batch 1240: Loss: 0.5099442005157471; Accuracy: 0.75\n",
      "\t\tTraining Batch 1250: Loss: 0.7929688096046448; Accuracy: 0.5\n",
      "\t\tTraining Batch 1260: Loss: 0.4790728688240051; Accuracy: 1.0\n",
      "\t\tTraining Batch 1270: Loss: 0.5791282057762146; Accuracy: 0.75\n",
      "\t\tTraining Batch 1280: Loss: 0.6166552305221558; Accuracy: 0.75\n",
      "\t\tTraining Batch 1290: Loss: 0.6859742999076843; Accuracy: 0.5\n",
      "\t\tTraining Batch 1300: Loss: 0.5812076926231384; Accuracy: 0.75\n",
      "\t\tTraining Batch 1310: Loss: 0.5405089855194092; Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 1320: Loss: 0.6387513279914856; Accuracy: 0.5\n",
      "\t\tTraining Batch 1330: Loss: 0.7209321856498718; Accuracy: 0.5\n",
      "\t\tTraining Batch 1340: Loss: 0.593829333782196; Accuracy: 0.75\n",
      "\t\tTraining Batch 1350: Loss: 0.5783306360244751; Accuracy: 0.75\n",
      "Training:\n",
      "\tLoss: 0.6466828497921445; Accuracy: 0.6285924834193073\n",
      "\t\tValidation Batch 10: Loss: 0.7363885641098022; Accuracy: 0.5\n",
      "\t\tValidation Batch 20: Loss: 0.5707270503044128; Accuracy: 0.75\n",
      "\t\tValidation Batch 30: Loss: 0.581841766834259; Accuracy: 0.75\n",
      "\t\tValidation Batch 40: Loss: 0.6844767332077026; Accuracy: 0.5\n",
      "\t\tValidation Batch 50: Loss: 0.5066680312156677; Accuracy: 0.75\n",
      "\t\tValidation Batch 60: Loss: 0.7180450558662415; Accuracy: 0.5\n",
      "\t\tValidation Batch 70: Loss: 0.4405924081802368; Accuracy: 1.0\n",
      "\t\tValidation Batch 80: Loss: 0.7519993782043457; Accuracy: 0.5\n",
      "\t\tValidation Batch 90: Loss: 0.6914889812469482; Accuracy: 0.5\n",
      "\t\tValidation Batch 100: Loss: 0.48350176215171814; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.5519759058952332; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.5608857274055481; Accuracy: 0.75\n",
      "\t\tValidation Batch 130: Loss: 0.8079768419265747; Accuracy: 0.5\n",
      "\t\tValidation Batch 140: Loss: 0.7813000679016113; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.5391830801963806; Accuracy: 0.75\n",
      "\t\tValidation Batch 160: Loss: 0.78300541639328; Accuracy: 0.5\n",
      "Validation:\n",
      "\tLoss=0.630168922431767; Accuracy: 0.675\n",
      "Training epoch #2\n",
      "\t\tTraining Batch 10: Loss: 0.4754202961921692; Accuracy: 0.75\n",
      "\t\tTraining Batch 20: Loss: 0.4320126175880432; Accuracy: 1.0\n",
      "\t\tTraining Batch 30: Loss: 0.5421543121337891; Accuracy: 0.75\n",
      "\t\tTraining Batch 40: Loss: 0.31992876529693604; Accuracy: 1.0\n",
      "\t\tTraining Batch 50: Loss: 0.4921236038208008; Accuracy: 1.0\n",
      "\t\tTraining Batch 60: Loss: 0.6058628559112549; Accuracy: 0.5\n",
      "\t\tTraining Batch 70: Loss: 0.3306024372577667; Accuracy: 1.0\n",
      "\t\tTraining Batch 80: Loss: 0.7752227187156677; Accuracy: 0.5\n",
      "\t\tTraining Batch 90: Loss: 0.3315895199775696; Accuracy: 1.0\n",
      "\t\tTraining Batch 100: Loss: 0.5588201284408569; Accuracy: 0.75\n",
      "\t\tTraining Batch 110: Loss: 0.6875514984130859; Accuracy: 0.5\n",
      "\t\tTraining Batch 120: Loss: 1.0096487998962402; Accuracy: 0.25\n",
      "\t\tTraining Batch 130: Loss: 0.3737146258354187; Accuracy: 1.0\n",
      "\t\tTraining Batch 140: Loss: 0.3634459674358368; Accuracy: 1.0\n",
      "\t\tTraining Batch 150: Loss: 0.5488283634185791; Accuracy: 0.75\n",
      "\t\tTraining Batch 160: Loss: 0.41473686695098877; Accuracy: 1.0\n",
      "\t\tTraining Batch 170: Loss: 0.4088730812072754; Accuracy: 1.0\n",
      "\t\tTraining Batch 180: Loss: 0.7643250226974487; Accuracy: 0.5\n",
      "\t\tTraining Batch 190: Loss: 0.431029736995697; Accuracy: 1.0\n",
      "\t\tTraining Batch 200: Loss: 0.8378927707672119; Accuracy: 0.25\n",
      "\t\tTraining Batch 210: Loss: 0.3449857532978058; Accuracy: 1.0\n",
      "\t\tTraining Batch 220: Loss: 0.7209396958351135; Accuracy: 0.5\n",
      "\t\tTraining Batch 230: Loss: 0.6444993019104004; Accuracy: 0.75\n",
      "\t\tTraining Batch 240: Loss: 0.35630571842193604; Accuracy: 1.0\n",
      "\t\tTraining Batch 250: Loss: 0.495749294757843; Accuracy: 0.75\n",
      "\t\tTraining Batch 260: Loss: 0.5848647356033325; Accuracy: 0.75\n",
      "\t\tTraining Batch 270: Loss: 0.5350025296211243; Accuracy: 0.75\n",
      "\t\tTraining Batch 280: Loss: 0.3605169653892517; Accuracy: 1.0\n",
      "\t\tTraining Batch 290: Loss: 0.6214075088500977; Accuracy: 0.5\n",
      "\t\tTraining Batch 300: Loss: 0.6268452405929565; Accuracy: 0.5\n",
      "\t\tTraining Batch 310: Loss: 0.39820295572280884; Accuracy: 1.0\n",
      "\t\tTraining Batch 320: Loss: 0.5401536226272583; Accuracy: 0.75\n",
      "\t\tTraining Batch 330: Loss: 0.5309845209121704; Accuracy: 0.75\n",
      "\t\tTraining Batch 340: Loss: 0.5352559685707092; Accuracy: 0.75\n",
      "\t\tTraining Batch 350: Loss: 0.9127436280250549; Accuracy: 0.25\n",
      "\t\tTraining Batch 360: Loss: 0.7653939723968506; Accuracy: 0.5\n",
      "\t\tTraining Batch 370: Loss: 0.4401364326477051; Accuracy: 1.0\n",
      "\t\tTraining Batch 380: Loss: 0.4249836206436157; Accuracy: 1.0\n",
      "\t\tTraining Batch 390: Loss: 0.5289424061775208; Accuracy: 0.75\n",
      "\t\tTraining Batch 400: Loss: 0.8416677117347717; Accuracy: 0.5\n",
      "\t\tTraining Batch 410: Loss: 0.49560680985450745; Accuracy: 0.75\n",
      "\t\tTraining Batch 420: Loss: 0.39284756779670715; Accuracy: 1.0\n",
      "\t\tTraining Batch 430: Loss: 0.4120253324508667; Accuracy: 1.0\n",
      "\t\tTraining Batch 440: Loss: 0.5422639846801758; Accuracy: 0.75\n",
      "\t\tTraining Batch 450: Loss: 0.6634306907653809; Accuracy: 0.75\n",
      "\t\tTraining Batch 460: Loss: 0.5661023855209351; Accuracy: 0.75\n",
      "\t\tTraining Batch 470: Loss: 0.5574765801429749; Accuracy: 0.5\n",
      "\t\tTraining Batch 480: Loss: 0.6433985233306885; Accuracy: 0.75\n",
      "\t\tTraining Batch 490: Loss: 0.43174707889556885; Accuracy: 1.0\n",
      "\t\tTraining Batch 500: Loss: 0.347173273563385; Accuracy: 1.0\n",
      "\t\tTraining Batch 510: Loss: 0.4537504315376282; Accuracy: 0.75\n",
      "\t\tTraining Batch 520: Loss: 0.5727224349975586; Accuracy: 0.75\n",
      "\t\tTraining Batch 530: Loss: 0.6954258680343628; Accuracy: 0.5\n",
      "\t\tTraining Batch 540: Loss: 0.6426591873168945; Accuracy: 0.75\n",
      "\t\tTraining Batch 550: Loss: 0.4860647916793823; Accuracy: 1.0\n",
      "\t\tTraining Batch 560: Loss: 0.7100673317909241; Accuracy: 0.5\n",
      "\t\tTraining Batch 570: Loss: 0.6521689295768738; Accuracy: 0.5\n",
      "\t\tTraining Batch 580: Loss: 0.5582987070083618; Accuracy: 0.75\n",
      "\t\tTraining Batch 590: Loss: 0.9903941750526428; Accuracy: 0.25\n",
      "\t\tTraining Batch 600: Loss: 0.5585953593254089; Accuracy: 0.75\n",
      "\t\tTraining Batch 610: Loss: 0.462510347366333; Accuracy: 0.75\n",
      "\t\tTraining Batch 620: Loss: 0.8236960768699646; Accuracy: 0.5\n",
      "\t\tTraining Batch 630: Loss: 0.6991256475448608; Accuracy: 0.5\n",
      "\t\tTraining Batch 640: Loss: 0.7731058597564697; Accuracy: 0.5\n",
      "\t\tTraining Batch 650: Loss: 0.3213790953159332; Accuracy: 1.0\n",
      "\t\tTraining Batch 660: Loss: 0.6476315259933472; Accuracy: 0.5\n",
      "\t\tTraining Batch 670: Loss: 0.3218868672847748; Accuracy: 1.0\n",
      "\t\tTraining Batch 680: Loss: 0.4586869776248932; Accuracy: 0.75\n",
      "\t\tTraining Batch 690: Loss: 0.5151516199111938; Accuracy: 0.75\n",
      "\t\tTraining Batch 700: Loss: 0.3801366090774536; Accuracy: 1.0\n",
      "\t\tTraining Batch 710: Loss: 0.6598995923995972; Accuracy: 0.75\n",
      "\t\tTraining Batch 720: Loss: 0.31916022300720215; Accuracy: 1.0\n",
      "\t\tTraining Batch 730: Loss: 0.6205203533172607; Accuracy: 0.75\n",
      "\t\tTraining Batch 740: Loss: 0.5847869515419006; Accuracy: 0.75\n",
      "\t\tTraining Batch 750: Loss: 0.360749214887619; Accuracy: 1.0\n",
      "\t\tTraining Batch 760: Loss: 0.49866896867752075; Accuracy: 0.75\n",
      "\t\tTraining Batch 770: Loss: 0.40171366930007935; Accuracy: 1.0\n",
      "\t\tTraining Batch 780: Loss: 0.5203409790992737; Accuracy: 0.75\n",
      "\t\tTraining Batch 790: Loss: 0.320761501789093; Accuracy: 1.0\n",
      "\t\tTraining Batch 800: Loss: 0.7601558566093445; Accuracy: 0.5\n",
      "\t\tTraining Batch 810: Loss: 0.39085835218429565; Accuracy: 1.0\n",
      "\t\tTraining Batch 820: Loss: 0.5240066647529602; Accuracy: 0.75\n",
      "\t\tTraining Batch 830: Loss: 0.49439799785614014; Accuracy: 0.75\n",
      "\t\tTraining Batch 840: Loss: 0.6295702457427979; Accuracy: 0.5\n",
      "\t\tTraining Batch 850: Loss: 0.4840901792049408; Accuracy: 0.75\n",
      "\t\tTraining Batch 860: Loss: 0.5128660202026367; Accuracy: 0.75\n",
      "\t\tTraining Batch 870: Loss: 0.32280609011650085; Accuracy: 1.0\n",
      "\t\tTraining Batch 880: Loss: 0.32932692766189575; Accuracy: 1.0\n",
      "\t\tTraining Batch 890: Loss: 0.7050013542175293; Accuracy: 0.5\n",
      "\t\tTraining Batch 900: Loss: 0.482295423746109; Accuracy: 0.75\n",
      "\t\tTraining Batch 910: Loss: 0.32442179322242737; Accuracy: 1.0\n",
      "\t\tTraining Batch 920: Loss: 0.5871117115020752; Accuracy: 0.75\n",
      "\t\tTraining Batch 930: Loss: 0.5598417520523071; Accuracy: 0.75\n",
      "\t\tTraining Batch 940: Loss: 0.5346630215644836; Accuracy: 0.75\n",
      "\t\tTraining Batch 950: Loss: 0.340977281332016; Accuracy: 1.0\n",
      "\t\tTraining Batch 960: Loss: 0.3219744861125946; Accuracy: 1.0\n",
      "\t\tTraining Batch 970: Loss: 0.662391185760498; Accuracy: 0.75\n",
      "\t\tTraining Batch 980: Loss: 0.5930206179618835; Accuracy: 0.75\n",
      "\t\tTraining Batch 990: Loss: 0.3959908187389374; Accuracy: 1.0\n",
      "\t\tTraining Batch 1000: Loss: 0.48752227425575256; Accuracy: 0.75\n",
      "\t\tTraining Batch 1010: Loss: 0.7646409869194031; Accuracy: 0.5\n",
      "\t\tTraining Batch 1020: Loss: 0.3621242940425873; Accuracy: 1.0\n",
      "\t\tTraining Batch 1030: Loss: 0.4188295006752014; Accuracy: 1.0\n",
      "\t\tTraining Batch 1040: Loss: 0.7633234858512878; Accuracy: 0.5\n",
      "\t\tTraining Batch 1050: Loss: 0.3947733938694; Accuracy: 1.0\n",
      "\t\tTraining Batch 1060: Loss: 0.3162544369697571; Accuracy: 1.0\n",
      "\t\tTraining Batch 1070: Loss: 0.5697406530380249; Accuracy: 0.75\n",
      "\t\tTraining Batch 1080: Loss: 0.3255494236946106; Accuracy: 1.0\n",
      "\t\tTraining Batch 1090: Loss: 0.3250519633293152; Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 1100: Loss: 0.6262776851654053; Accuracy: 0.75\n",
      "\t\tTraining Batch 1110: Loss: 0.3270082175731659; Accuracy: 1.0\n",
      "\t\tTraining Batch 1120: Loss: 0.36454126238822937; Accuracy: 1.0\n",
      "\t\tTraining Batch 1130: Loss: 0.3289235532283783; Accuracy: 1.0\n",
      "\t\tTraining Batch 1140: Loss: 0.32174021005630493; Accuracy: 1.0\n",
      "\t\tTraining Batch 1150: Loss: 0.317534476518631; Accuracy: 1.0\n",
      "\t\tTraining Batch 1160: Loss: 0.33529213070869446; Accuracy: 1.0\n",
      "\t\tTraining Batch 1170: Loss: 0.5210258364677429; Accuracy: 1.0\n",
      "\t\tTraining Batch 1180: Loss: 0.31546705961227417; Accuracy: 1.0\n",
      "\t\tTraining Batch 1190: Loss: 0.3348250985145569; Accuracy: 1.0\n",
      "\t\tTraining Batch 1200: Loss: 0.358184814453125; Accuracy: 1.0\n",
      "\t\tTraining Batch 1210: Loss: 0.5650465488433838; Accuracy: 0.75\n",
      "\t\tTraining Batch 1220: Loss: 0.7161327004432678; Accuracy: 0.5\n",
      "\t\tTraining Batch 1230: Loss: 0.4000394642353058; Accuracy: 1.0\n",
      "\t\tTraining Batch 1240: Loss: 1.0525469779968262; Accuracy: 0.25\n",
      "\t\tTraining Batch 1250: Loss: 0.3844912052154541; Accuracy: 1.0\n",
      "\t\tTraining Batch 1260: Loss: 0.32183825969696045; Accuracy: 1.0\n",
      "\t\tTraining Batch 1270: Loss: 0.3276934325695038; Accuracy: 1.0\n",
      "\t\tTraining Batch 1280: Loss: 0.4153595268726349; Accuracy: 1.0\n",
      "\t\tTraining Batch 1290: Loss: 0.6289689540863037; Accuracy: 0.75\n",
      "\t\tTraining Batch 1300: Loss: 0.7969944477081299; Accuracy: 0.5\n",
      "\t\tTraining Batch 1310: Loss: 0.5781952142715454; Accuracy: 0.75\n",
      "\t\tTraining Batch 1320: Loss: 0.7965912222862244; Accuracy: 0.5\n",
      "\t\tTraining Batch 1330: Loss: 0.5420424342155457; Accuracy: 0.75\n",
      "\t\tTraining Batch 1340: Loss: 0.3197092115879059; Accuracy: 1.0\n",
      "\t\tTraining Batch 1350: Loss: 0.786943793296814; Accuracy: 0.25\n",
      "Training:\n",
      "\tLoss: 0.5310505450255043; Accuracy: 0.7730287398673544\n",
      "\t\tValidation Batch 10: Loss: 0.5798935294151306; Accuracy: 0.75\n",
      "\t\tValidation Batch 20: Loss: 0.6736460328102112; Accuracy: 0.5\n",
      "\t\tValidation Batch 30: Loss: 0.5676725506782532; Accuracy: 0.75\n",
      "\t\tValidation Batch 40: Loss: 0.7799647450447083; Accuracy: 0.5\n",
      "\t\tValidation Batch 50: Loss: 0.48344919085502625; Accuracy: 1.0\n",
      "\t\tValidation Batch 60: Loss: 0.5075159072875977; Accuracy: 1.0\n",
      "\t\tValidation Batch 70: Loss: 0.42874273657798767; Accuracy: 0.75\n",
      "\t\tValidation Batch 80: Loss: 0.8265653848648071; Accuracy: 0.25\n",
      "\t\tValidation Batch 90: Loss: 0.5445793867111206; Accuracy: 0.75\n",
      "\t\tValidation Batch 100: Loss: 0.5663015246391296; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.5275027751922607; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.5846569538116455; Accuracy: 0.75\n",
      "\t\tValidation Batch 130: Loss: 0.8868913054466248; Accuracy: 0.25\n",
      "\t\tValidation Batch 140: Loss: 0.8078813552856445; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.5321898460388184; Accuracy: 0.75\n",
      "\t\tValidation Batch 160: Loss: 0.8127044439315796; Accuracy: 0.5\n",
      "Validation:\n",
      "\tLoss=0.6125999534502625; Accuracy: 0.6796875\n",
      "Training epoch #3\n",
      "\t\tTraining Batch 10: Loss: 0.8042291402816772; Accuracy: 0.5\n",
      "\t\tTraining Batch 20: Loss: 0.3167891502380371; Accuracy: 1.0\n",
      "\t\tTraining Batch 30: Loss: 0.39636853337287903; Accuracy: 1.0\n",
      "\t\tTraining Batch 40: Loss: 0.5598320364952087; Accuracy: 0.75\n",
      "\t\tTraining Batch 50: Loss: 0.3398211598396301; Accuracy: 1.0\n",
      "\t\tTraining Batch 60: Loss: 0.3637677729129791; Accuracy: 1.0\n",
      "\t\tTraining Batch 70: Loss: 0.3580532371997833; Accuracy: 1.0\n",
      "\t\tTraining Batch 80: Loss: 1.0318650007247925; Accuracy: 0.25\n",
      "\t\tTraining Batch 90: Loss: 0.557794451713562; Accuracy: 0.5\n",
      "\t\tTraining Batch 100: Loss: 0.31574249267578125; Accuracy: 1.0\n",
      "\t\tTraining Batch 110: Loss: 0.5725811123847961; Accuracy: 0.75\n",
      "\t\tTraining Batch 120: Loss: 0.5629223585128784; Accuracy: 0.75\n",
      "\t\tTraining Batch 130: Loss: 0.530624508857727; Accuracy: 0.75\n",
      "\t\tTraining Batch 140: Loss: 0.7122507095336914; Accuracy: 0.5\n",
      "\t\tTraining Batch 150: Loss: 0.5687644481658936; Accuracy: 0.75\n",
      "\t\tTraining Batch 160: Loss: 0.8056137561798096; Accuracy: 0.5\n",
      "\t\tTraining Batch 170: Loss: 0.3144957423210144; Accuracy: 1.0\n",
      "\t\tTraining Batch 180: Loss: 0.6145114898681641; Accuracy: 0.75\n",
      "\t\tTraining Batch 190: Loss: 0.6835578680038452; Accuracy: 0.5\n",
      "\t\tTraining Batch 200: Loss: 0.6140491962432861; Accuracy: 0.75\n",
      "\t\tTraining Batch 210: Loss: 0.3139071762561798; Accuracy: 1.0\n",
      "\t\tTraining Batch 220: Loss: 0.31733858585357666; Accuracy: 1.0\n",
      "\t\tTraining Batch 230: Loss: 0.32456645369529724; Accuracy: 1.0\n",
      "\t\tTraining Batch 240: Loss: 0.6229240894317627; Accuracy: 0.75\n",
      "\t\tTraining Batch 250: Loss: 0.37579625844955444; Accuracy: 1.0\n",
      "\t\tTraining Batch 260: Loss: 0.3145335614681244; Accuracy: 1.0\n",
      "\t\tTraining Batch 270: Loss: 0.3156374990940094; Accuracy: 1.0\n",
      "\t\tTraining Batch 280: Loss: 0.3145471513271332; Accuracy: 1.0\n",
      "\t\tTraining Batch 290: Loss: 0.4177495241165161; Accuracy: 0.75\n",
      "\t\tTraining Batch 300: Loss: 0.5544630289077759; Accuracy: 0.75\n",
      "\t\tTraining Batch 310: Loss: 0.6901916265487671; Accuracy: 0.5\n",
      "\t\tTraining Batch 320: Loss: 0.3758542835712433; Accuracy: 1.0\n",
      "\t\tTraining Batch 330: Loss: 0.5208946466445923; Accuracy: 0.75\n",
      "\t\tTraining Batch 340: Loss: 0.5671914219856262; Accuracy: 0.75\n",
      "\t\tTraining Batch 350: Loss: 0.6203986406326294; Accuracy: 0.75\n",
      "\t\tTraining Batch 360: Loss: 0.3199935555458069; Accuracy: 1.0\n",
      "\t\tTraining Batch 370: Loss: 0.3923690617084503; Accuracy: 1.0\n",
      "\t\tTraining Batch 380: Loss: 0.3148286044597626; Accuracy: 1.0\n",
      "\t\tTraining Batch 390: Loss: 0.33583712577819824; Accuracy: 1.0\n",
      "\t\tTraining Batch 400: Loss: 0.5802298784255981; Accuracy: 0.75\n",
      "\t\tTraining Batch 410: Loss: 0.8497089147567749; Accuracy: 0.5\n",
      "\t\tTraining Batch 420: Loss: 0.6658094525337219; Accuracy: 0.5\n",
      "\t\tTraining Batch 430: Loss: 0.35344254970550537; Accuracy: 1.0\n",
      "\t\tTraining Batch 440: Loss: 0.3211888372898102; Accuracy: 1.0\n",
      "\t\tTraining Batch 450: Loss: 0.8195230960845947; Accuracy: 0.5\n",
      "\t\tTraining Batch 460: Loss: 0.3146275579929352; Accuracy: 1.0\n",
      "\t\tTraining Batch 470: Loss: 0.3213128447532654; Accuracy: 1.0\n",
      "\t\tTraining Batch 480: Loss: 0.325173020362854; Accuracy: 1.0\n",
      "\t\tTraining Batch 490: Loss: 0.40376532077789307; Accuracy: 1.0\n",
      "\t\tTraining Batch 500: Loss: 0.5670177340507507; Accuracy: 0.75\n",
      "\t\tTraining Batch 510: Loss: 0.31662094593048096; Accuracy: 1.0\n",
      "\t\tTraining Batch 520: Loss: 0.31451237201690674; Accuracy: 1.0\n",
      "\t\tTraining Batch 530: Loss: 0.5682438611984253; Accuracy: 0.75\n",
      "\t\tTraining Batch 540: Loss: 0.39621609449386597; Accuracy: 1.0\n",
      "\t\tTraining Batch 550: Loss: 0.3158228397369385; Accuracy: 1.0\n",
      "\t\tTraining Batch 560: Loss: 0.5306259393692017; Accuracy: 0.75\n",
      "\t\tTraining Batch 570: Loss: 0.3135381042957306; Accuracy: 1.0\n",
      "\t\tTraining Batch 580: Loss: 0.583913266658783; Accuracy: 0.75\n",
      "\t\tTraining Batch 590: Loss: 0.5438235998153687; Accuracy: 0.75\n",
      "\t\tTraining Batch 600: Loss: 0.44520923495292664; Accuracy: 0.75\n",
      "\t\tTraining Batch 610: Loss: 0.32278597354888916; Accuracy: 1.0\n",
      "\t\tTraining Batch 620: Loss: 0.8324187994003296; Accuracy: 0.5\n",
      "\t\tTraining Batch 630: Loss: 0.3156278729438782; Accuracy: 1.0\n",
      "\t\tTraining Batch 640: Loss: 0.7476488947868347; Accuracy: 0.5\n",
      "\t\tTraining Batch 650: Loss: 0.815862774848938; Accuracy: 0.5\n",
      "\t\tTraining Batch 660: Loss: 0.7106144428253174; Accuracy: 0.5\n",
      "\t\tTraining Batch 670: Loss: 0.44685718417167664; Accuracy: 0.75\n",
      "\t\tTraining Batch 680: Loss: 0.6298232674598694; Accuracy: 0.75\n",
      "\t\tTraining Batch 690: Loss: 0.922812819480896; Accuracy: 0.25\n",
      "\t\tTraining Batch 700: Loss: 0.3137369453907013; Accuracy: 1.0\n",
      "\t\tTraining Batch 710: Loss: 0.39904874563217163; Accuracy: 1.0\n",
      "\t\tTraining Batch 720: Loss: 0.7641335725784302; Accuracy: 0.5\n",
      "\t\tTraining Batch 730: Loss: 0.32959288358688354; Accuracy: 1.0\n",
      "\t\tTraining Batch 740: Loss: 0.5436977744102478; Accuracy: 0.75\n",
      "\t\tTraining Batch 750: Loss: 0.41162076592445374; Accuracy: 0.75\n",
      "\t\tTraining Batch 760: Loss: 0.38267046213150024; Accuracy: 1.0\n",
      "\t\tTraining Batch 770: Loss: 0.32073062658309937; Accuracy: 1.0\n",
      "\t\tTraining Batch 780: Loss: 0.34600383043289185; Accuracy: 1.0\n",
      "\t\tTraining Batch 790: Loss: 0.3310319781303406; Accuracy: 1.0\n",
      "\t\tTraining Batch 800: Loss: 0.3694581985473633; Accuracy: 1.0\n",
      "\t\tTraining Batch 810: Loss: 0.3731899857521057; Accuracy: 1.0\n",
      "\t\tTraining Batch 820: Loss: 0.523515522480011; Accuracy: 0.75\n",
      "\t\tTraining Batch 830: Loss: 0.4525122046470642; Accuracy: 1.0\n",
      "\t\tTraining Batch 840: Loss: 0.55437171459198; Accuracy: 0.75\n",
      "\t\tTraining Batch 850: Loss: 0.5693913698196411; Accuracy: 0.75\n",
      "\t\tTraining Batch 860: Loss: 0.34568920731544495; Accuracy: 1.0\n",
      "\t\tTraining Batch 870: Loss: 0.4833354651927948; Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 880: Loss: 0.31482023000717163; Accuracy: 1.0\n",
      "\t\tTraining Batch 890: Loss: 0.7099626660346985; Accuracy: 0.5\n",
      "\t\tTraining Batch 900: Loss: 0.31866365671157837; Accuracy: 1.0\n",
      "\t\tTraining Batch 910: Loss: 0.4655955731868744; Accuracy: 0.75\n",
      "\t\tTraining Batch 920: Loss: 0.4028598666191101; Accuracy: 1.0\n",
      "\t\tTraining Batch 930: Loss: 0.4758474826812744; Accuracy: 1.0\n",
      "\t\tTraining Batch 940: Loss: 0.5676104426383972; Accuracy: 0.75\n",
      "\t\tTraining Batch 950: Loss: 0.31748053431510925; Accuracy: 1.0\n",
      "\t\tTraining Batch 960: Loss: 0.32019251585006714; Accuracy: 1.0\n",
      "\t\tTraining Batch 970: Loss: 0.42996299266815186; Accuracy: 1.0\n",
      "\t\tTraining Batch 980: Loss: 0.5887423157691956; Accuracy: 0.75\n",
      "\t\tTraining Batch 990: Loss: 0.43447205424308777; Accuracy: 1.0\n",
      "\t\tTraining Batch 1000: Loss: 0.5006168484687805; Accuracy: 0.75\n",
      "\t\tTraining Batch 1010: Loss: 0.41142570972442627; Accuracy: 1.0\n",
      "\t\tTraining Batch 1020: Loss: 0.3224804997444153; Accuracy: 1.0\n",
      "\t\tTraining Batch 1030: Loss: 0.4566463530063629; Accuracy: 1.0\n",
      "\t\tTraining Batch 1040: Loss: 0.3149058222770691; Accuracy: 1.0\n",
      "\t\tTraining Batch 1050: Loss: 0.3450225591659546; Accuracy: 1.0\n",
      "\t\tTraining Batch 1060: Loss: 0.3249336779117584; Accuracy: 1.0\n",
      "\t\tTraining Batch 1070: Loss: 0.5063731074333191; Accuracy: 0.75\n",
      "\t\tTraining Batch 1080: Loss: 0.5720120668411255; Accuracy: 0.75\n",
      "\t\tTraining Batch 1090: Loss: 0.3140164017677307; Accuracy: 1.0\n",
      "\t\tTraining Batch 1100: Loss: 0.8169015645980835; Accuracy: 0.5\n",
      "\t\tTraining Batch 1110: Loss: 0.8073791265487671; Accuracy: 0.5\n",
      "\t\tTraining Batch 1120: Loss: 0.5496208667755127; Accuracy: 0.75\n",
      "\t\tTraining Batch 1130: Loss: 0.3767177164554596; Accuracy: 1.0\n",
      "\t\tTraining Batch 1140: Loss: 0.40225139260292053; Accuracy: 1.0\n",
      "\t\tTraining Batch 1150: Loss: 0.33068954944610596; Accuracy: 1.0\n",
      "\t\tTraining Batch 1160: Loss: 0.31579679250717163; Accuracy: 1.0\n",
      "\t\tTraining Batch 1170: Loss: 0.4829707741737366; Accuracy: 0.75\n",
      "\t\tTraining Batch 1180: Loss: 0.4013575315475464; Accuracy: 1.0\n",
      "\t\tTraining Batch 1190: Loss: 0.56292325258255; Accuracy: 0.75\n",
      "\t\tTraining Batch 1200: Loss: 0.3892822265625; Accuracy: 1.0\n",
      "\t\tTraining Batch 1210: Loss: 0.39215511083602905; Accuracy: 1.0\n",
      "\t\tTraining Batch 1220: Loss: 0.3190876245498657; Accuracy: 1.0\n",
      "\t\tTraining Batch 1230: Loss: 0.31616640090942383; Accuracy: 1.0\n",
      "\t\tTraining Batch 1240: Loss: 0.33026421070098877; Accuracy: 1.0\n",
      "\t\tTraining Batch 1250: Loss: 0.7891136407852173; Accuracy: 0.5\n",
      "\t\tTraining Batch 1260: Loss: 0.7254154086112976; Accuracy: 0.5\n",
      "\t\tTraining Batch 1270: Loss: 0.33740508556365967; Accuracy: 1.0\n",
      "\t\tTraining Batch 1280: Loss: 0.3147265613079071; Accuracy: 1.0\n",
      "\t\tTraining Batch 1290: Loss: 0.31634196639060974; Accuracy: 1.0\n",
      "\t\tTraining Batch 1300: Loss: 0.3159002363681793; Accuracy: 1.0\n",
      "\t\tTraining Batch 1310: Loss: 0.3658668100833893; Accuracy: 1.0\n",
      "\t\tTraining Batch 1320: Loss: 0.42796462774276733; Accuracy: 1.0\n",
      "\t\tTraining Batch 1330: Loss: 0.4142996072769165; Accuracy: 1.0\n",
      "\t\tTraining Batch 1340: Loss: 0.33737388253211975; Accuracy: 1.0\n",
      "\t\tTraining Batch 1350: Loss: 0.4230111539363861; Accuracy: 0.75\n",
      "Training:\n",
      "\tLoss: 0.4675487410565378; Accuracy: 0.8413780397936624\n",
      "\t\tValidation Batch 10: Loss: 0.5753117799758911; Accuracy: 0.75\n",
      "\t\tValidation Batch 20: Loss: 0.575999915599823; Accuracy: 0.75\n",
      "\t\tValidation Batch 30: Loss: 0.568276047706604; Accuracy: 0.75\n",
      "\t\tValidation Batch 40: Loss: 0.9324405789375305; Accuracy: 0.25\n",
      "\t\tValidation Batch 50: Loss: 0.7793771028518677; Accuracy: 0.5\n",
      "\t\tValidation Batch 60: Loss: 0.49647313356399536; Accuracy: 1.0\n",
      "\t\tValidation Batch 70: Loss: 0.35433927178382874; Accuracy: 1.0\n",
      "\t\tValidation Batch 80: Loss: 0.6198978424072266; Accuracy: 0.75\n",
      "\t\tValidation Batch 90: Loss: 0.7730621695518494; Accuracy: 0.5\n",
      "\t\tValidation Batch 100: Loss: 0.568862795829773; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.4587392210960388; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.8553141355514526; Accuracy: 0.5\n",
      "\t\tValidation Batch 130: Loss: 0.775588870048523; Accuracy: 0.5\n",
      "\t\tValidation Batch 140: Loss: 0.8123325109481812; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.31363365054130554; Accuracy: 1.0\n",
      "\t\tValidation Batch 160: Loss: 0.7391699552536011; Accuracy: 0.5\n",
      "Validation:\n",
      "\tLoss=0.6244044123217464; Accuracy: 0.671875\n",
      "Best accuracy (0.6796875) obtained at epoch #2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING LOOP\n",
    "# Variable for minimal accuracy\n",
    "MIN_ACCURACY = 0.73 # Based on the average accuracy\n",
    "REACHED_MIN_ACCURACY = False\n",
    "best_weights = class_model.state_dict()\n",
    "# Want to maximize accuracy\n",
    "max_val_acc = (0, 0)\n",
    "# Put the model in CPU\n",
    "class_model.cuda()\n",
    "# Create the optimizer\n",
    "param_optimizer = list(class_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "# I use the one that comes with the models, but any other optimizer could be used\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "# Store our loss and accuracy for plotting\n",
    "fit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "epoch_number = 0\n",
    "epoch_since_max = 0\n",
    "continue_learning = True\n",
    "while epoch_number < EPOCHS and continue_learning:\n",
    "    epoch_number += 1\n",
    "    print(f\"Training epoch #{epoch_number}\")\n",
    "    # Tracking variables\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Training\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    class_model.train()\n",
    "    # Freeze RoBERTa weights\n",
    "    #class_model.embedder.eval()\n",
    "    class_model.embedder.requires_grad_ = False\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)   \n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        # Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "        # Append to fit history\n",
    "        fit_history[\"loss\"].append(loss.item()) \n",
    "        fit_history[\"accuracy\"].append(b_accuracy) \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        tr_accuracy += b_accuracy\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if nb_tr_steps%10 == 0:\n",
    "            print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n",
    "    print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n",
    "    # Validation\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    class_model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        # Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "        # Append to fit history\n",
    "        fit_history[\"val_loss\"].append(loss.item()) \n",
    "        fit_history[\"val_accuracy\"].append(b_accuracy) \n",
    "        # Update tracking variables\n",
    "        eval_loss += loss.item()\n",
    "        eval_accuracy += b_accuracy\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        if nb_eval_steps%10 == 0:\n",
    "            print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "    eval_acc = eval_accuracy/nb_eval_steps\n",
    "    if eval_acc >= max_val_acc[0]:\n",
    "        max_val_acc = (eval_acc, epoch_number)\n",
    "        continue_learning = True\n",
    "        epoch_since_max = 0 # New max\n",
    "        best_weights = copy.deepcopy(class_model.state_dict()) # Keep the best weights\n",
    "        # See if we have reached min_accuracy\n",
    "        if eval_acc >= MIN_ACCURACY:\n",
    "            REACHED_MIN_ACCURACY = True\n",
    "        # Save to file only if it has reached min acc\n",
    "        if REACHED_MIN_ACCURACY:\n",
    "            # Save the best weights to file\n",
    "            torch.save(best_weights, os.path.join(PATH,'WiCHead.pt'))\n",
    "            continue_learning = False # Stop learning. Reached baseline acc for this model\n",
    "    else:\n",
    "        epoch_since_max += 1\n",
    "        if epoch_since_max > PATIENCE:\n",
    "            continue_learning = False # Stop learning, starting to overfit\n",
    "    print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "print(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")\n",
    "# Reload the best weights (from memory)\n",
    "class_model.load_state_dict(best_weights)\n",
    "\n",
    "\n",
    "#I am getting the error CUDA out of memory, I think this is because the batch size is too big, \n",
    "#though I'm not too sure what to change it too or where to find that variable.\n",
    "#I think it might be from lines 41 to 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad570f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_predictions(preds):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return pred_flat == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1426adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4502ddf87b86>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.5798935294151306; Accuracy: 0.75\n",
      "\t\tValidation Batch 20: Loss: 0.6736460328102112; Accuracy: 0.5\n",
      "\t\tValidation Batch 30: Loss: 0.5676725506782532; Accuracy: 0.75\n",
      "\t\tValidation Batch 40: Loss: 0.7799647450447083; Accuracy: 0.5\n",
      "\t\tValidation Batch 50: Loss: 0.48344919085502625; Accuracy: 1.0\n",
      "\t\tValidation Batch 60: Loss: 0.5075159072875977; Accuracy: 1.0\n",
      "\t\tValidation Batch 70: Loss: 0.42874273657798767; Accuracy: 0.75\n",
      "\t\tValidation Batch 80: Loss: 0.8265653848648071; Accuracy: 0.25\n",
      "\t\tValidation Batch 90: Loss: 0.5445793867111206; Accuracy: 0.75\n",
      "\t\tValidation Batch 100: Loss: 0.5663015246391296; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.5275027751922607; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.5846569538116455; Accuracy: 0.75\n",
      "\t\tValidation Batch 130: Loss: 0.8868913054466248; Accuracy: 0.25\n",
      "\t\tValidation Batch 140: Loss: 0.8078813552856445; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.5321898460388184; Accuracy: 0.75\n",
      "\t\tValidation Batch 160: Loss: 0.8127044439315796; Accuracy: 0.5\n",
      "Validation:\n",
      "\tLoss=0.6125999534502625; Accuracy: 0.6796875\n",
      "OrderedDict([(0, True), (1, True), (2, True), (3, True), (4, True), (5, True), (6, True), (7, False), (8, True), (9, True), (10, True), (11, True), (12, True), (13, False), (14, False), (15, True), (16, True), (17, False), (18, False), (19, True), (20, True), (21, False), (22, True), (23, True), (24, True), (25, True), (26, False), (27, False), (28, False), (29, True), (30, True), (31, True), (32, True), (33, True), (34, False), (35, True), (36, False), (37, True), (38, True), (39, True), (40, False), (41, True), (42, True), (43, True), (44, True), (45, False), (46, False), (47, True), (48, True), (49, True), (50, True), (51, True), (52, False), (53, True), (54, True), (55, True), (56, True), (57, False), (58, False), (59, True), (60, True), (61, True), (62, False), (63, False), (64, True), (65, True), (66, True), (67, False), (68, True), (69, True), (70, False), (71, True), (72, True), (73, False), (74, False), (75, True), (76, False), (77, True), (78, False), (79, True), (80, False), (81, True), (82, True), (83, True), (84, True), (85, True), (86, True), (87, True), (88, False), (89, False), (90, False), (91, True), (92, False), (93, True), (94, False), (95, False), (96, True), (97, True), (98, True), (99, True), (100, True), (101, False), (102, True), (103, True), (104, False), (105, True), (106, False), (107, True), (108, True), (109, True), (110, True), (111, True), (112, True), (113, True), (114, True), (115, True), (116, False), (117, True), (118, True), (119, True), (120, True), (121, True), (122, True), (123, False), (124, True), (125, False), (126, False), (127, False), (128, True), (129, False), (130, True), (131, True), (132, True), (133, True), (134, True), (135, False), (136, True), (137, True), (138, True), (139, False), (140, True), (141, False), (142, True), (143, False), (144, False), (145, True), (146, True), (147, True), (148, True), (149, True), (150, True), (151, True), (152, True), (153, False), (154, False), (155, False), (156, True), (157, True), (158, False), (159, False), (160, True), (161, False), (162, True), (163, True), (164, True), (165, True), (166, False), (167, False), (168, False), (169, True), (170, True), (171, True), (172, False), (173, True), (174, True), (175, True), (176, True), (177, True), (178, True), (179, True), (180, True), (181, True), (182, False), (183, False), (184, True), (185, True), (186, True), (187, True), (188, True), (189, True), (190, True), (191, False), (192, True), (193, False), (194, True), (195, False), (196, True), (197, True), (198, True), (199, True), (200, True), (201, True), (202, True), (203, True), (204, True), (205, False), (206, True), (207, True), (208, True), (209, False), (210, True), (211, False), (212, True), (213, True), (214, True), (215, True), (216, False), (217, True), (218, True), (219, True), (220, True), (221, False), (222, True), (223, True), (224, False), (225, True), (226, False), (227, False), (228, True), (229, True), (230, True), (231, True), (232, True), (233, True), (234, True), (235, False), (236, True), (237, True), (238, True), (239, True), (240, False), (241, True), (242, True), (243, False), (244, True), (245, False), (246, True), (247, True), (248, True), (249, True), (250, False), (251, True), (252, True), (253, True), (254, True), (255, True), (256, True), (257, False), (258, True), (259, False), (260, False), (261, True), (262, True), (263, False), (264, False), (265, True), (266, True), (267, False), (268, True), (269, True), (270, True), (271, False), (272, True), (273, True), (274, True), (275, False), (276, False), (277, True), (278, True), (279, True), (280, True), (281, True), (282, True), (283, False), (284, True), (285, True), (286, True), (287, True), (288, True), (289, True), (290, True), (291, True), (292, True), (293, True), (294, True), (295, True), (296, True), (297, False), (298, True), (299, True), (300, True), (301, False), (302, False), (303, True), (304, True), (305, True), (306, True), (307, False), (308, True), (309, True), (310, False), (311, True), (312, True), (313, False), (314, True), (315, False), (316, False), (317, True), (318, False), (319, False), (320, True), (321, False), (322, True), (323, False), (324, True), (325, True), (326, True), (327, True), (328, True), (329, True), (330, False), (331, True), (332, True), (333, False), (334, False), (335, True), (336, False), (337, False), (338, True), (339, True), (340, True), (341, False), (342, False), (343, True), (344, True), (345, True), (346, True), (347, True), (348, False), (349, True), (350, True), (351, True), (352, True), (353, True), (354, True), (355, False), (356, False), (357, True), (358, True), (359, True), (360, True), (361, False), (362, True), (363, True), (364, True), (365, True), (366, False), (367, True), (368, False), (369, True), (370, False), (371, False), (372, False), (373, False), (374, True), (375, False), (376, True), (377, True), (378, True), (379, True), (380, True), (381, True), (382, True), (383, True), (384, True), (385, False), (386, True), (387, False), (388, True), (389, True), (390, True), (391, True), (392, True), (393, True), (394, True), (395, True), (396, False), (397, True), (398, True), (399, True), (400, True), (401, True), (402, True), (403, False), (404, True), (405, True), (406, True), (407, False), (408, True), (409, False), (410, True), (411, True), (412, False), (413, True), (414, True), (415, False), (416, True), (417, False), (418, False), (419, False), (420, True), (421, False), (422, False), (423, True), (424, True), (425, True), (426, True), (427, True), (428, False), (429, True), (430, True), (431, False), (432, True), (433, True), (434, False), (435, True), (436, False), (437, True), (438, True), (439, True), (440, True), (441, False), (442, True), (443, False), (444, True), (445, False), (446, False), (447, False), (448, False), (449, True), (450, True), (451, True), (452, False), (453, True), (454, True), (455, False), (456, True), (457, True), (458, True), (459, True), (460, True), (461, True), (462, True), (463, True), (464, True), (465, False), (466, True), (467, False), (468, True), (469, True), (470, False), (471, False), (472, True), (473, True), (474, False), (475, True), (476, False), (477, True), (478, True), (479, True), (480, False), (481, False), (482, False), (483, True), (484, True), (485, False), (486, False), (487, True), (488, True), (489, False), (490, True), (491, False), (492, True), (493, True), (494, True), (495, True), (496, True), (497, False), (498, True), (499, False), (500, True), (501, True), (502, True), (503, True), (504, False), (505, False), (506, False), (507, True), (508, True), (509, True), (510, True), (511, True), (512, False), (513, True), (514, True), (515, False), (516, False), (517, True), (518, False), (519, False), (520, True), (521, True), (522, True), (523, True), (524, True), (525, True), (526, False), (527, True), (528, True), (529, True), (530, True), (531, False), (532, True), (533, False), (534, True), (535, True), (536, False), (537, True), (538, False), (539, False), (540, True), (541, True), (542, True), (543, True), (544, False), (545, False), (546, True), (547, True), (548, False), (549, False), (550, False), (551, False), (552, True), (553, True), (554, True), (555, False), (556, True), (557, False), (558, False), (559, True), (560, False), (561, False), (562, True), (563, False), (564, True), (565, False), (566, True), (567, True), (568, False), (569, False), (570, False), (571, False), (572, True), (573, True), (574, True), (575, True), (576, False), (577, False), (578, False), (579, True), (580, True), (581, True), (582, True), (583, True), (584, False), (585, True), (586, True), (587, True), (588, True), (589, True), (590, True), (591, True), (592, False), (593, True), (594, True), (595, False), (596, True), (597, True), (598, True), (599, False), (600, True), (601, True), (602, True), (603, True), (604, False), (605, False), (606, True), (607, True), (608, False), (609, True), (610, False), (611, False), (612, True), (613, True), (614, True), (615, True), (616, True), (617, True), (618, False), (619, False), (620, False), (621, True), (622, False), (623, True), (624, True), (625, True), (626, False), (627, False), (628, True), (629, True), (630, True), (631, True), (632, True), (633, True), (634, True), (635, True), (636, False), (637, True)])\n"
     ]
    }
   ],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    # Calculate the accuracy\n",
    "    b_accuracy, b_pred_correctness = flat_accuracy(logits, label_ids, return_predict_correctness = True) # For RobertaForClassification\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    # Add to predictions\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "    # Update tracking variables\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ce44e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4502ddf87b86>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTest Batch 10\n",
      "\t\tTest Batch 20\n",
      "\t\tTest Batch 30\n",
      "\t\tTest Batch 40\n",
      "\t\tTest Batch 50\n",
      "\t\tTest Batch 60\n",
      "\t\tTest Batch 70\n",
      "\t\tTest Batch 80\n",
      "\t\tTest Batch 90\n",
      "\t\tTest Batch 100\n",
      "\t\tTest Batch 110\n",
      "\t\tTest Batch 120\n",
      "\t\tTest Batch 130\n",
      "\t\tTest Batch 140\n",
      "\t\tTest Batch 150\n",
      "\t\tTest Batch 160\n",
      "\t\tTest Batch 170\n",
      "\t\tTest Batch 180\n",
      "\t\tTest Batch 190\n",
      "\t\tTest Batch 200\n",
      "\t\tTest Batch 210\n",
      "\t\tTest Batch 220\n",
      "\t\tTest Batch 230\n",
      "\t\tTest Batch 240\n",
      "\t\tTest Batch 250\n",
      "\t\tTest Batch 260\n",
      "\t\tTest Batch 270\n",
      "\t\tTest Batch 280\n",
      "\t\tTest Batch 290\n",
      "\t\tTest Batch 300\n",
      "\t\tTest Batch 310\n",
      "\t\tTest Batch 320\n",
      "\t\tTest Batch 330\n",
      "\t\tTest Batch 340\n",
      "\t\tTest Batch 350\n",
      "Testing results\n",
      "OrderedDict([(0, True), (1, False), (2, True), (3, True), (4, False), (5, False), (6, False), (7, True), (8, False), (9, True), (10, True), (11, True), (12, False), (13, False), (14, True), (15, True), (16, True), (17, False), (18, False), (19, True), (20, False), (21, False), (22, True), (23, True), (24, False), (25, False), (26, True), (27, False), (28, True), (29, True), (30, True), (31, False), (32, True), (33, False), (34, False), (35, True), (36, False), (37, False), (38, True), (39, False), (40, True), (41, False), (42, True), (43, False), (44, True), (45, True), (46, False), (47, False), (48, False), (49, True), (50, False), (51, True), (52, True), (53, True), (54, True), (55, False), (56, False), (57, False), (58, True), (59, False), (60, True), (61, False), (62, False), (63, False), (64, True), (65, False), (66, False), (67, True), (68, True), (69, True), (70, False), (71, True), (72, True), (73, True), (74, False), (75, False), (76, True), (77, False), (78, False), (79, True), (80, True), (81, True), (82, False), (83, False), (84, True), (85, False), (86, True), (87, False), (88, True), (89, False), (90, False), (91, True), (92, False), (93, True), (94, False), (95, True), (96, True), (97, True), (98, True), (99, False), (100, True), (101, False), (102, True), (103, True), (104, True), (105, False), (106, False), (107, True), (108, True), (109, True), (110, True), (111, True), (112, True), (113, False), (114, False), (115, True), (116, True), (117, False), (118, False), (119, False), (120, True), (121, True), (122, True), (123, False), (124, False), (125, True), (126, True), (127, False), (128, True), (129, False), (130, False), (131, True), (132, True), (133, True), (134, True), (135, True), (136, False), (137, False), (138, False), (139, False), (140, False), (141, True), (142, True), (143, True), (144, True), (145, False), (146, False), (147, True), (148, True), (149, True), (150, True), (151, True), (152, True), (153, False), (154, True), (155, True), (156, True), (157, False), (158, True), (159, False), (160, False), (161, True), (162, False), (163, True), (164, True), (165, True), (166, True), (167, False), (168, False), (169, False), (170, False), (171, False), (172, False), (173, True), (174, False), (175, True), (176, True), (177, False), (178, True), (179, True), (180, True), (181, False), (182, False), (183, False), (184, True), (185, False), (186, True), (187, False), (188, False), (189, False), (190, False), (191, True), (192, True), (193, False), (194, True), (195, True), (196, True), (197, True), (198, False), (199, True), (200, True), (201, False), (202, False), (203, True), (204, False), (205, False), (206, True), (207, False), (208, True), (209, False), (210, True), (211, True), (212, True), (213, False), (214, True), (215, False), (216, False), (217, True), (218, True), (219, True), (220, True), (221, True), (222, True), (223, False), (224, True), (225, False), (226, True), (227, False), (228, True), (229, False), (230, True), (231, True), (232, True), (233, True), (234, False), (235, True), (236, True), (237, True), (238, True), (239, False), (240, True), (241, True), (242, True), (243, True), (244, False), (245, True), (246, False), (247, True), (248, False), (249, True), (250, False), (251, False), (252, False), (253, True), (254, False), (255, True), (256, True), (257, True), (258, True), (259, True), (260, False), (261, True), (262, False), (263, True), (264, True), (265, True), (266, False), (267, True), (268, True), (269, True), (270, True), (271, False), (272, False), (273, True), (274, False), (275, False), (276, False), (277, False), (278, False), (279, True), (280, True), (281, False), (282, False), (283, False), (284, False), (285, False), (286, False), (287, True), (288, True), (289, True), (290, True), (291, True), (292, True), (293, False), (294, False), (295, False), (296, True), (297, True), (298, False), (299, True), (300, True), (301, True), (302, True), (303, False), (304, False), (305, True), (306, False), (307, False), (308, True), (309, True), (310, True), (311, True), (312, True), (313, False), (314, True), (315, False), (316, False), (317, False), (318, True), (319, True), (320, False), (321, True), (322, True), (323, False), (324, True), (325, True), (326, False), (327, False), (328, True), (329, False), (330, True), (331, True), (332, True), (333, False), (334, False), (335, False), (336, False), (337, True), (338, False), (339, False), (340, True), (341, False), (342, True), (343, False), (344, True), (345, False), (346, False), (347, False), (348, False), (349, False), (350, True), (351, False), (352, True), (353, False), (354, True), (355, True), (356, True), (357, False), (358, True), (359, False), (360, True), (361, True), (362, False), (363, True), (364, True), (365, True), (366, False), (367, True), (368, False), (369, False), (370, True), (371, False), (372, True), (373, False), (374, False), (375, False), (376, False), (377, True), (378, False), (379, True), (380, False), (381, True), (382, True), (383, True), (384, False), (385, False), (386, False), (387, False), (388, True), (389, False), (390, False), (391, False), (392, False), (393, False), (394, False), (395, True), (396, True), (397, False), (398, False), (399, True), (400, False), (401, True), (402, True), (403, False), (404, False), (405, True), (406, True), (407, True), (408, True), (409, False), (410, False), (411, False), (412, True), (413, True), (414, True), (415, True), (416, True), (417, True), (418, True), (419, True), (420, True), (421, True), (422, True), (423, False), (424, True), (425, True), (426, False), (427, False), (428, True), (429, False), (430, False), (431, True), (432, True), (433, True), (434, True), (435, True), (436, True), (437, True), (438, True), (439, False), (440, True), (441, False), (442, True), (443, False), (444, False), (445, False), (446, True), (447, False), (448, False), (449, False), (450, True), (451, True), (452, True), (453, True), (454, True), (455, False), (456, False), (457, True), (458, True), (459, False), (460, True), (461, False), (462, False), (463, False), (464, False), (465, True), (466, True), (467, False), (468, True), (469, True), (470, False), (471, True), (472, False), (473, True), (474, False), (475, True), (476, True), (477, False), (478, True), (479, False), (480, False), (481, True), (482, False), (483, False), (484, True), (485, True), (486, False), (487, True), (488, True), (489, False), (490, True), (491, True), (492, False), (493, True), (494, False), (495, True), (496, True), (497, False), (498, False), (499, True), (500, True), (501, True), (502, False), (503, True), (504, True), (505, True), (506, False), (507, True), (508, True), (509, True), (510, False), (511, True), (512, True), (513, True), (514, True), (515, False), (516, True), (517, True), (518, False), (519, False), (520, True), (521, True), (522, True), (523, True), (524, True), (525, True), (526, True), (527, True), (528, True), (529, True), (530, True), (531, True), (532, True), (533, True), (534, False), (535, False), (536, True), (537, False), (538, False), (539, False), (540, True), (541, True), (542, True), (543, False), (544, False), (545, True), (546, False), (547, True), (548, False), (549, True), (550, False), (551, False), (552, False), (553, False), (554, True), (555, False), (556, False), (557, True), (558, True), (559, True), (560, True), (561, False), (562, False), (563, True), (564, False), (565, False), (566, False), (567, True), (568, True), (569, False), (570, False), (571, True), (572, False), (573, True), (574, True), (575, True), (576, True), (577, False), (578, False), (579, True), (580, True), (581, True), (582, False), (583, True), (584, False), (585, False), (586, True), (587, True), (588, True), (589, False), (590, False), (591, True), (592, False), (593, False), (594, False), (595, False), (596, True), (597, True), (598, True), (599, False), (600, True), (601, True), (602, False), (603, False), (604, True), (605, True), (606, True), (607, True), (608, False), (609, True), (610, True), (611, True), (612, False), (613, True), (614, False), (615, True), (616, True), (617, True), (618, False), (619, True), (620, False), (621, False), (622, True), (623, False), (624, True), (625, True), (626, False), (627, True), (628, False), (629, True), (630, False), (631, True), (632, False), (633, True), (634, True), (635, True), (636, False), (637, False), (638, True), (639, True), (640, True), (641, False), (642, True), (643, True), (644, True), (645, True), (646, False), (647, True), (648, False), (649, True), (650, True), (651, False), (652, True), (653, True), (654, False), (655, True), (656, True), (657, True), (658, True), (659, False), (660, False), (661, False), (662, True), (663, False), (664, True), (665, True), (666, True), (667, True), (668, True), (669, True), (670, True), (671, False), (672, False), (673, True), (674, True), (675, True), (676, True), (677, True), (678, True), (679, True), (680, True), (681, False), (682, True), (683, False), (684, True), (685, True), (686, False), (687, False), (688, False), (689, True), (690, True), (691, False), (692, True), (693, False), (694, True), (695, False), (696, False), (697, False), (698, False), (699, False), (700, False), (701, False), (702, True), (703, True), (704, True), (705, False), (706, True), (707, True), (708, True), (709, True), (710, True), (711, False), (712, True), (713, True), (714, False), (715, True), (716, True), (717, True), (718, True), (719, True), (720, True), (721, False), (722, True), (723, True), (724, True), (725, True), (726, True), (727, True), (728, False), (729, False), (730, False), (731, True), (732, True), (733, True), (734, False), (735, True), (736, False), (737, True), (738, False), (739, True), (740, False), (741, False), (742, True), (743, False), (744, True), (745, True), (746, True), (747, True), (748, False), (749, False), (750, True), (751, True), (752, True), (753, True), (754, True), (755, True), (756, True), (757, True), (758, False), (759, True), (760, True), (761, True), (762, True), (763, True), (764, False), (765, True), (766, False), (767, True), (768, False), (769, False), (770, True), (771, True), (772, False), (773, False), (774, True), (775, False), (776, True), (777, False), (778, True), (779, True), (780, True), (781, True), (782, False), (783, False), (784, False), (785, False), (786, True), (787, True), (788, True), (789, True), (790, False), (791, True), (792, False), (793, True), (794, False), (795, False), (796, False), (797, False), (798, False), (799, True), (800, True), (801, False), (802, True), (803, False), (804, True), (805, False), (806, False), (807, True), (808, True), (809, False), (810, True), (811, False), (812, False), (813, True), (814, False), (815, False), (816, False), (817, True), (818, False), (819, True), (820, True), (821, False), (822, True), (823, True), (824, True), (825, False), (826, False), (827, True), (828, False), (829, False), (830, True), (831, False), (832, True), (833, True), (834, True), (835, True), (836, True), (837, False), (838, True), (839, True), (840, True), (841, True), (842, True), (843, False), (844, True), (845, True), (846, True), (847, False), (848, False), (849, True), (850, True), (851, True), (852, True), (853, False), (854, True), (855, False), (856, False), (857, False), (858, True), (859, True), (860, False), (861, False), (862, True), (863, False), (864, True), (865, False), (866, True), (867, True), (868, True), (869, False), (870, False), (871, False), (872, False), (873, False), (874, True), (875, True), (876, False), (877, False), (878, True), (879, False), (880, False), (881, True), (882, True), (883, True), (884, True), (885, True), (886, False), (887, True), (888, True), (889, False), (890, True), (891, False), (892, True), (893, True), (894, False), (895, False), (896, True), (897, False), (898, True), (899, True), (900, True), (901, False), (902, False), (903, True), (904, False), (905, True), (906, True), (907, False), (908, True), (909, False), (910, True), (911, True), (912, False), (913, False), (914, True), (915, True), (916, False), (917, True), (918, False), (919, False), (920, True), (921, False), (922, False), (923, True), (924, True), (925, True), (926, True), (927, True), (928, True), (929, False), (930, False), (931, True), (932, True), (933, True), (934, True), (935, True), (936, True), (937, False), (938, True), (939, True), (940, False), (941, True), (942, True), (943, True), (944, False), (945, True), (946, False), (947, False), (948, True), (949, True), (950, True), (951, True), (952, True), (953, False), (954, True), (955, True), (956, True), (957, False), (958, False), (959, False), (960, False), (961, True), (962, True), (963, True), (964, True), (965, True), (966, True), (967, True), (968, False), (969, True), (970, True), (971, True), (972, False), (973, True), (974, True), (975, False), (976, True), (977, True), (978, False), (979, True), (980, False), (981, False), (982, False), (983, True), (984, False), (985, True), (986, True), (987, True), (988, True), (989, True), (990, False), (991, True), (992, True), (993, False), (994, True), (995, True), (996, True), (997, True), (998, False), (999, True), (1000, True), (1001, True), (1002, False), (1003, True), (1004, True), (1005, False), (1006, False), (1007, True), (1008, False), (1009, False), (1010, True), (1011, False), (1012, False), (1013, True), (1014, True), (1015, True), (1016, False), (1017, True), (1018, True), (1019, True), (1020, False), (1021, True), (1022, True), (1023, True), (1024, True), (1025, True), (1026, True), (1027, False), (1028, False), (1029, True), (1030, False), (1031, False), (1032, False), (1033, True), (1034, True), (1035, False), (1036, True), (1037, True), (1038, False), (1039, True), (1040, False), (1041, False), (1042, True), (1043, True), (1044, True), (1045, True), (1046, True), (1047, True), (1048, True), (1049, False), (1050, True), (1051, False), (1052, False), (1053, True), (1054, True), (1055, False), (1056, False), (1057, False), (1058, False), (1059, False), (1060, False), (1061, True), (1062, True), (1063, True), (1064, False), (1065, False), (1066, False), (1067, True), (1068, True), (1069, False), (1070, False), (1071, True), (1072, False), (1073, False), (1074, True), (1075, False), (1076, True), (1077, True), (1078, True), (1079, True), (1080, True), (1081, True), (1082, False), (1083, True), (1084, True), (1085, False), (1086, True), (1087, False), (1088, False), (1089, True), (1090, False), (1091, False), (1092, True), (1093, False), (1094, True), (1095, False), (1096, True), (1097, False), (1098, True), (1099, False), (1100, True), (1101, True), (1102, False), (1103, False), (1104, True), (1105, True), (1106, True), (1107, True), (1108, True), (1109, False), (1110, True), (1111, True), (1112, False), (1113, False), (1114, True), (1115, False), (1116, False), (1117, True), (1118, False), (1119, True), (1120, True), (1121, True), (1122, True), (1123, False), (1124, True), (1125, True), (1126, True), (1127, True), (1128, True), (1129, True), (1130, True), (1131, True), (1132, True), (1133, False), (1134, False), (1135, False), (1136, True), (1137, False), (1138, False), (1139, True), (1140, False), (1141, True), (1142, False), (1143, True), (1144, True), (1145, True), (1146, True), (1147, True), (1148, False), (1149, False), (1150, True), (1151, True), (1152, False), (1153, True), (1154, True), (1155, True), (1156, True), (1157, True), (1158, False), (1159, False), (1160, True), (1161, False), (1162, False), (1163, False), (1164, False), (1165, True), (1166, True), (1167, True), (1168, True), (1169, True), (1170, False), (1171, False), (1172, False), (1173, True), (1174, True), (1175, True), (1176, False), (1177, False), (1178, True), (1179, True), (1180, True), (1181, True), (1182, True), (1183, False), (1184, True), (1185, True), (1186, False), (1187, True), (1188, False), (1189, False), (1190, True), (1191, False), (1192, True), (1193, True), (1194, True), (1195, False), (1196, False), (1197, True), (1198, False), (1199, True), (1200, True), (1201, False), (1202, False), (1203, True), (1204, True), (1205, False), (1206, True), (1207, False), (1208, True), (1209, False), (1210, True), (1211, True), (1212, True), (1213, False), (1214, False), (1215, False), (1216, True), (1217, False), (1218, True), (1219, False), (1220, False), (1221, True), (1222, False), (1223, True), (1224, True), (1225, False), (1226, True), (1227, False), (1228, True), (1229, True), (1230, False), (1231, False), (1232, False), (1233, True), (1234, False), (1235, True), (1236, False), (1237, True), (1238, False), (1239, True), (1240, True), (1241, True), (1242, True), (1243, True), (1244, True), (1245, True), (1246, False), (1247, True), (1248, True), (1249, False), (1250, False), (1251, False), (1252, True), (1253, True), (1254, True), (1255, True), (1256, False), (1257, True), (1258, False), (1259, True), (1260, True), (1261, True), (1262, False), (1263, True), (1264, False), (1265, False), (1266, False), (1267, False), (1268, True), (1269, True), (1270, True), (1271, False), (1272, False), (1273, True), (1274, False), (1275, True), (1276, True), (1277, True), (1278, True), (1279, True), (1280, False), (1281, True), (1282, True), (1283, True), (1284, False), (1285, True), (1286, True), (1287, True), (1288, True), (1289, False), (1290, False), (1291, True), (1292, False), (1293, True), (1294, True), (1295, True), (1296, False), (1297, True), (1298, True), (1299, False), (1300, True), (1301, False), (1302, False), (1303, False), (1304, True), (1305, False), (1306, True), (1307, True), (1308, False), (1309, True), (1310, True), (1311, True), (1312, True), (1313, True), (1314, True), (1315, True), (1316, True), (1317, False), (1318, False), (1319, True), (1320, True), (1321, True), (1322, True), (1323, False), (1324, True), (1325, False), (1326, True), (1327, False), (1328, True), (1329, True), (1330, False), (1331, False), (1332, False), (1333, False), (1334, False), (1335, True), (1336, False), (1337, False), (1338, True), (1339, False), (1340, True), (1341, True), (1342, False), (1343, True), (1344, True), (1345, True), (1346, True), (1347, True), (1348, True), (1349, True), (1350, True), (1351, True), (1352, False), (1353, False), (1354, True), (1355, False), (1356, False), (1357, False), (1358, True), (1359, False), (1360, True), (1361, True), (1362, True), (1363, True), (1364, True), (1365, True), (1366, False), (1367, True), (1368, False), (1369, False), (1370, True), (1371, True), (1372, True), (1373, False), (1374, True), (1375, True), (1376, False), (1377, True), (1378, True), (1379, True), (1380, True), (1381, True), (1382, True), (1383, True), (1384, True), (1385, True), (1386, True), (1387, False), (1388, False), (1389, False), (1390, False), (1391, False), (1392, True), (1393, False), (1394, False), (1395, True), (1396, False), (1397, True), (1398, False), (1399, False)])\n"
     ]
    }
   ],
   "source": [
    "test_predictions = {}\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_examples, nb_test_steps = 0, 0\n",
    "# Testing\n",
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = class_model(b_input_ids, attention_mask=b_input_mask, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # Get the predictions\n",
    "    b_preds = flat_predictions(logits)\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    for index, pred in zip(indexes, b_preds):\n",
    "        test_predictions[index] = pred\n",
    "    # Update tracking variables\n",
    "    test_loss += loss.item()\n",
    "    test_accuracy += b_accuracy\n",
    "    nb_test_examples += b_input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    if nb_test_steps%10 == 0:\n",
    "        print(\"\\t\\tTest Batch {}\".format(nb_test_steps))\n",
    "# Print final results\n",
    "print(\"Testing results\")\n",
    "test_predictions = collections.OrderedDict(sorted(test_predictions.items()))\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
