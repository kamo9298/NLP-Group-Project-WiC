{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad4d6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9463f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word                                    sentence1  \\\n",
      "0  place  Do you want to come over to my place later?   \n",
      "\n",
      "                                           sentence2  idx  label  start1  \\\n",
      "0  A political system with no place for the less ...    0  False      31   \n",
      "\n",
      "   start2  end1  end2  version  \n",
      "0      27    36    32      1.1  \n"
     ]
    }
   ],
   "source": [
    "#turing data json into pandas dataframe\n",
    "wic_df = pd.read_json('WiC/train.jsonl', lines=True)\n",
    "print(wic_df.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ceedaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a list of words (strings) and a sentence (as a RoBERTa tokenized ID list) and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "#NOTE: it can also apply to a group of words separated by spaces\n",
    "#NOTE: It is important that the list of words given describes a sentence, because the order is relevant to do the matching properly\n",
    "      \n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "  decomposedWord = tokenizer.encode(word)\n",
    "  # Iterate through to find a matching sublist of the token_ids\n",
    "  for i in range(len(token_ids)):\n",
    "    if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "      return (i,i+len(decomposedWord)-1)\n",
    "  # This is the ouput when no matching pattern is found\n",
    "  return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "  intList = []\n",
    "  for word in wordList:\n",
    "    if len(intList) == 0:\n",
    "      intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "    else:\n",
    "      afterLastInterval = intList[-1][1]+1\n",
    "      interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "      actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "      intList.append(actualPositions)\n",
    "  return intList\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  if return_predict_correctness:\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "  else:\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_predictions(preds):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  return pred_flat == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812ecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, roberta_based_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        Keeps a reference to the provided RoBERTa model. \n",
    "        It then adds a linear layer that takes the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = roberta_based_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        Takes in the same argument as RoBERTa forward plus two tensors for the location of the 2 words to compare\n",
    "        \"\"\"\n",
    "        if word1_locs is None or word2_locs is None:\n",
    "          raise ValueError(\"The tensors (word1_locs, word1_locs) containing the location of the words to compare in the input vector must be provided.\")\n",
    "        elif input_ids is None:\n",
    "          raise ValueError(\"The input_ids tensor must be provided.\")\n",
    "        elif word1_locs.shape[0] != input_ids.shape[0] or word2_locs.shape[0] != input_ids.shape[0]:\n",
    "          raise ValueError(\"All provided vectors should have the same batch size.\")\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # Get the embeddings\n",
    "        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get the words\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        diff = word1s - word2s\n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        # Calculate the loss\n",
    "        if labels is not None:\n",
    "            #  We want seperation like a SVM so use Hinge loss\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa355be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c09928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1426adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do all of the imports \n",
    "#load in the data\n",
    "#Use import transformers to get our pretrained model\n",
    "#Tokenize our data using the pretrained BERT model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
