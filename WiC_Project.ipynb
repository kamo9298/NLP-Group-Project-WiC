{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9463f36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turing data json into pandas dataframe\n",
    "# wic_df = pd.read_json('WiC/train.jsonl', lines=True)\n",
    "# print(wic_df.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808bb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_transformers import *\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc07cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d070ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to import json objects from jsonl files\n",
    "def load_json_objects_from_file(filename):\n",
    "    # Array for json objects\n",
    "    json_objects = []\n",
    "    # Read file line by line\n",
    "    with open(filename, mode = \"r\") as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            json_objects.append(json.loads(line))\n",
    "    return json_objects\n",
    "\n",
    "#Takes a list of words (strings) and a sentence (as a RoBERTa tokenized ID list) and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "#NOTE: it can also apply to a group of words separated by spaces\n",
    "#NOTE: It is important that the list of words given describes a sentence, because the order is relevant to do the matching properly\n",
    "      \n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "    decomposedWord = tokenizer.encode(word)\n",
    "    # Iterate through to find a matching sublist of the token_ids\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "            return (i,i+len(decomposedWord)-1)\n",
    "    # This is the ouput when no matching pattern is found\n",
    "    return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "    intList = []\n",
    "    for word in wordList:\n",
    "        if len(intList) == 0:\n",
    "            intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "        else:\n",
    "            afterLastInterval = intList[-1][1]+1\n",
    "            interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "            actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "            intList.append(actualPositions)\n",
    "    return intList\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    if return_predict_correctness:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "    else:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_predictions(preds):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return pred_flat == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7a12ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 970'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "PATIENCE = 2\n",
    "# Prepare Torch to use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdf1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess the WiC data\n",
    "def wic_preprocessing(json_objects, training = True, shuffle_data = False, verbose = False):\n",
    "    wic_sentences = []\n",
    "    wic_encoded = []\n",
    "    wic_labels = []\n",
    "    wic_word_locs = []\n",
    "    wic_indexes = []\n",
    "    for index, example in enumerate(json_objects):\n",
    "        #wic_indexes.append(example['idx']) # Is it the index??\n",
    "        wic_indexes.append(index)\n",
    "        sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "        wic_sentences.append(sentence)\n",
    "        # Then encode the sentences\n",
    "        wic_encoded.append(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        # Find the word in each sentences\n",
    "        word = example['word']\n",
    "        word_locs = (-1, -1)\n",
    "        # Split the 2 sentences on space. (Also, lemmatize and uncapitilize each word)\n",
    "        sent1_split = example['sentence1'].split(' ')\n",
    "        sent2_split = example['sentence2'].split(' ')\n",
    "        # Get the index of word in both sentences\n",
    "        sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "        sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "        # Create a variable to keep track of the number of characters parsed in each sentence as we loop\n",
    "        sent_chars = 0\n",
    "        # Loop over the words in the first sentence\n",
    "        i, j = 0, 0\n",
    "        word1_not_found, word2_not_found = True, True\n",
    "        while word1_not_found and i < len(sent1_split):\n",
    "            word_len = len(sent1_split[i])\n",
    "            if sent_chars >= sent1_word_char_loc[0] or sent_chars + word_len >= sent1_word_char_loc[1]:\n",
    "                word_locs = (i, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            elif sent_chars > sent1_word_char_loc[1]:\n",
    "                # If we somehow got past the word. Assume it was the previous word\n",
    "                word_locs = (i - 1, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            else:\n",
    "                # Look at the next word\n",
    "                sent_chars += word_len + 1 # Plus one for the space\n",
    "                i += 1\n",
    "        # Loop over the words in the second\n",
    "        sent_chars = 0 # Reset\n",
    "        while word2_not_found and j < len(sent2_split):\n",
    "            word_len = len(sent2_split[j])\n",
    "            if sent_chars >= sent2_word_char_loc[0] or sent_chars + word_len >= sent2_word_char_loc[1]:\n",
    "                word_locs = (i, j) # Found the word in the sentence\n",
    "                word2_not_found = False\n",
    "            elif sent_chars > sent2_word_char_loc[1]:\n",
    "                # If we somehow got past the word. Assume it was the previous word\n",
    "                word_locs = (i, j - 1) # Found the word in the sentence\n",
    "                word2_not_found = False\n",
    "            else:\n",
    "                # Look at the next word\n",
    "                sent_chars += word_len + 1 # Plus one for the space\n",
    "                j += 1\n",
    "        # For testing\n",
    "        if verbose:\n",
    "            print(word)\n",
    "            print(sent1_split)\n",
    "            print(sent2_split)\n",
    "            print(word_locs)\n",
    "        # Now to find the word in the tokenized sentences\n",
    "        word1 = sent1_split[word_locs[0]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation (See https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "        word2 = sent2_split[word_locs[1]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n",
    "        token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "        wic_word_locs.append(token_word_locs)\n",
    "        # Get the label if we expect it to be there\n",
    "        if training:\n",
    "            if example['label']:\n",
    "                wic_labels.append(1)\n",
    "            else:\n",
    "                wic_labels.append(0)\n",
    "    # Pad the sequences and find the encoded word location in the combined input\n",
    "    max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "    wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "    for i in range(0, len(wic_encoded)):\n",
    "        enc_sentence = wic_encoded[i]\n",
    "        word_locs = wic_word_locs[i]\n",
    "        # Pad the sequences\n",
    "        ex_len = len(enc_sentence)\n",
    "        padded_sentence = enc_sentence.copy()\n",
    "        padded_sentence.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "        padded_mask = [1] * ex_len\n",
    "        padded_mask.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "        # Create the vector to get back the words after RoBERTa\n",
    "        token_word_locs = wic_word_locs[i]\n",
    "        first_word_loc = []\n",
    "        second_word_loc = []\n",
    "        len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "        len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "        for j in range(0, max_len):\n",
    "            if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "                #Part of the first word\n",
    "                first_word_loc.append(1.0 / len_first_word)\n",
    "            else:\n",
    "                first_word_loc.append(0.0)\n",
    "            if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "                #Part of the second word\n",
    "                second_word_loc.append(1.0 / len_second_word)\n",
    "            else:\n",
    "                second_word_loc.append(0.0)\n",
    "        # We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n",
    "        wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "        wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "        # token_type_ids is a mask that tells where the first and second sentences are\n",
    "        token_type_id = []\n",
    "        first_sentence = True\n",
    "        sentence_start = True\n",
    "        for token in padded_sentence:\n",
    "            if first_sentence and sentence_start and token == 0:\n",
    "                # Allows 0 at the start of the first sentence\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and token > 0:\n",
    "                if sentence_start:\n",
    "                    sentence_start = False\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and not sentence_start and token == 0:\n",
    "                first_sentence = False\n",
    "                # Start of second sentence\n",
    "                token_type_id.append(1)\n",
    "            else:\n",
    "                # Second sentence\n",
    "                token_type_id.append(1)\n",
    "        wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "    if training:\n",
    "        if shuffle_data:\n",
    "            # Shuffle the data\n",
    "            raw_set = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"labels\": [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : []}\n",
    "            raw_set[\"input_ids\"], raw_set[\"token_type_ids\"], raw_set[\"attention_mask\"], raw_set[\"labels\"], raw_set[\"word1_locs\"], raw_set[\"word2_locs\"], raw_set[\"index\"] = shuffle(\n",
    "              wic_padded[\"input_ids\"], wic_padded[\"token_type_ids\"], wic_padded[\"attention_mask\"], wic_labels, wic_padded[\"word1_locs\"], wic_padded[\"word2_locs\"], wic_padded[\"index\"])\n",
    "        else:\n",
    "            raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\n",
    "                     \"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\n",
    "                     \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    else: # No labels present (Testing set)\n",
    "        # Do not shuffle the testing set\n",
    "        raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \n",
    "               \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \n",
    "               \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    # Return the raw data (Need to put them in a PyTorch tensor and dataset)\n",
    "    return raw_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d19f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357.0\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "train_json_objs = load_json_objects_from_file(\"WiC/train.jsonl\")\n",
    "raw_train_set = wic_preprocessing(train_json_objs, shuffle_data=True, verbose = False) # We do not want to shuffle for now.\n",
    "print(len(raw_train_set[\"labels\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfca6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset for it\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(raw_train_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"index\"]).to(device)\n",
    ")\n",
    "# Create a sampler and loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35f4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json objects from each file\n",
    "test_json_objs = load_json_objects_from_file(\"WiC/test.jsonl\")\n",
    "valid_json_objs = load_json_objects_from_file(\"WiC/val.jsonl\")\n",
    "# Process the objects\n",
    "raw_test_set = wic_preprocessing(test_json_objs, training = False) # The labels for the testing set are unknown\n",
    "raw_valid_set = wic_preprocessing(valid_json_objs)\n",
    "# Create PyTorch datasets\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "# Create a sampler and loader for each\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812ecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in model\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, roberta_based_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        Keeps a reference to the provided RoBERTa model. \n",
    "        It then adds a linear layer that takes the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = roberta_based_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        Takes in the same argument as RoBERTa forward plus two tensors for the location of the 2 words to compare\n",
    "        \"\"\"\n",
    "        if word1_locs is None or word2_locs is None:\n",
    "            raise ValueError(\"The tensors (word1_locs, word1_locs) containing the location of the words to compare in the input vector must be provided.\")\n",
    "        elif input_ids is None:\n",
    "            raise ValueError(\"The input_ids tensor must be provided.\")\n",
    "        elif word1_locs.shape[0] != input_ids.shape[0] or word2_locs.shape[0] != input_ids.shape[0]:\n",
    "            raise ValueError(\"All provided vectors should have the same batch size.\")\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # Get the embeddings\n",
    "        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get the words\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        diff = word1s - word2s\n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        # Calculate the loss\n",
    "        if labels is not None:\n",
    "            #  We want seperation like a SVM so use Hinge loss\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa355be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE WiC MODEL\n",
    "class_model = WiC_Head(model, embedding_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c09928",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4502ddf87b86>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n",
      "C:\\Users\\green\\anaconda3\\lib\\site-packages\\pytorch_transformers\\optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1025.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 10: Loss: 0.7329304218292236; Accuracy: 0.25\n",
      "\t\tTraining Batch 20: Loss: 0.7058459520339966; Accuracy: 0.25\n",
      "\t\tTraining Batch 30: Loss: 0.676278293132782; Accuracy: 0.5\n",
      "\t\tTraining Batch 40: Loss: 0.7027392983436584; Accuracy: 0.25\n",
      "\t\tTraining Batch 50: Loss: 0.6773657202720642; Accuracy: 0.75\n",
      "\t\tTraining Batch 60: Loss: 0.7117218971252441; Accuracy: 0.25\n",
      "\t\tTraining Batch 70: Loss: 0.6924958229064941; Accuracy: 0.5\n",
      "\t\tTraining Batch 80: Loss: 0.690434455871582; Accuracy: 0.5\n",
      "\t\tTraining Batch 90: Loss: 0.7032100558280945; Accuracy: 0.25\n",
      "\t\tTraining Batch 100: Loss: 0.6802800893783569; Accuracy: 1.0\n",
      "\t\tTraining Batch 110: Loss: 0.7101489901542664; Accuracy: 0.5\n",
      "\t\tTraining Batch 120: Loss: 0.6915320754051208; Accuracy: 0.5\n",
      "\t\tTraining Batch 130: Loss: 0.685320258140564; Accuracy: 0.5\n",
      "\t\tTraining Batch 140: Loss: 0.696580171585083; Accuracy: 0.25\n",
      "\t\tTraining Batch 150: Loss: 0.6863260269165039; Accuracy: 0.75\n",
      "\t\tTraining Batch 160: Loss: 0.6825138330459595; Accuracy: 0.5\n",
      "\t\tTraining Batch 170: Loss: 0.7076283097267151; Accuracy: 0.5\n",
      "\t\tTraining Batch 180: Loss: 0.6882481575012207; Accuracy: 0.25\n",
      "\t\tTraining Batch 190: Loss: 0.6905341744422913; Accuracy: 0.5\n",
      "\t\tTraining Batch 200: Loss: 0.6695054173469543; Accuracy: 0.75\n",
      "\t\tTraining Batch 210: Loss: 0.6945279836654663; Accuracy: 0.25\n",
      "\t\tTraining Batch 220: Loss: 0.6898311376571655; Accuracy: 0.75\n",
      "\t\tTraining Batch 230: Loss: 0.7407791018486023; Accuracy: 0.25\n",
      "\t\tTraining Batch 240: Loss: 0.6557773351669312; Accuracy: 1.0\n",
      "\t\tTraining Batch 250: Loss: 0.6634547710418701; Accuracy: 0.75\n",
      "\t\tTraining Batch 260: Loss: 0.7315906286239624; Accuracy: 0.5\n",
      "\t\tTraining Batch 270: Loss: 0.7097944021224976; Accuracy: 0.5\n",
      "\t\tTraining Batch 280: Loss: 0.7321679592132568; Accuracy: 0.25\n",
      "\t\tTraining Batch 290: Loss: 0.694137692451477; Accuracy: 0.5\n",
      "\t\tTraining Batch 300: Loss: 0.6200336217880249; Accuracy: 0.75\n",
      "\t\tTraining Batch 310: Loss: 0.6644628047943115; Accuracy: 0.75\n",
      "\t\tTraining Batch 320: Loss: 0.6445451974868774; Accuracy: 1.0\n",
      "\t\tTraining Batch 330: Loss: 0.6217979788780212; Accuracy: 0.75\n",
      "\t\tTraining Batch 340: Loss: 0.6354150176048279; Accuracy: 0.75\n",
      "\t\tTraining Batch 350: Loss: 0.6886727213859558; Accuracy: 0.75\n",
      "\t\tTraining Batch 360: Loss: 0.7070646286010742; Accuracy: 0.25\n",
      "\t\tTraining Batch 370: Loss: 0.6961511373519897; Accuracy: 0.75\n",
      "\t\tTraining Batch 380: Loss: 0.6823985576629639; Accuracy: 0.5\n",
      "\t\tTraining Batch 390: Loss: 0.6855860948562622; Accuracy: 0.75\n",
      "\t\tTraining Batch 400: Loss: 0.6610905528068542; Accuracy: 1.0\n",
      "\t\tTraining Batch 410: Loss: 0.6271175146102905; Accuracy: 1.0\n",
      "\t\tTraining Batch 420: Loss: 0.7211739420890808; Accuracy: 0.25\n",
      "\t\tTraining Batch 430: Loss: 0.6967965364456177; Accuracy: 0.5\n",
      "\t\tTraining Batch 440: Loss: 0.6746504306793213; Accuracy: 0.75\n",
      "\t\tTraining Batch 450: Loss: 0.7070292830467224; Accuracy: 0.5\n",
      "\t\tTraining Batch 460: Loss: 0.6802545785903931; Accuracy: 0.25\n",
      "\t\tTraining Batch 470: Loss: 0.6692934632301331; Accuracy: 0.75\n",
      "\t\tTraining Batch 480: Loss: 0.6863197088241577; Accuracy: 0.25\n",
      "\t\tTraining Batch 490: Loss: 0.688413143157959; Accuracy: 0.25\n",
      "\t\tTraining Batch 500: Loss: 0.6816447973251343; Accuracy: 0.5\n",
      "\t\tTraining Batch 510: Loss: 0.7494473457336426; Accuracy: 0.5\n",
      "\t\tTraining Batch 520: Loss: 0.7011221647262573; Accuracy: 0.75\n",
      "\t\tTraining Batch 530: Loss: 0.6784928441047668; Accuracy: 0.5\n",
      "\t\tTraining Batch 540: Loss: 0.5789868831634521; Accuracy: 1.0\n",
      "\t\tTraining Batch 550: Loss: 0.6532715559005737; Accuracy: 0.5\n",
      "\t\tTraining Batch 560: Loss: 0.7463936805725098; Accuracy: 0.5\n",
      "\t\tTraining Batch 570: Loss: 0.5518621206283569; Accuracy: 0.75\n",
      "\t\tTraining Batch 580: Loss: 0.7521787881851196; Accuracy: 0.5\n",
      "\t\tTraining Batch 590: Loss: 0.7406293749809265; Accuracy: 0.25\n",
      "\t\tTraining Batch 600: Loss: 0.6543141603469849; Accuracy: 0.5\n",
      "\t\tTraining Batch 610: Loss: 0.7104026675224304; Accuracy: 0.25\n",
      "\t\tTraining Batch 620: Loss: 0.7309643626213074; Accuracy: 0.0\n",
      "\t\tTraining Batch 630: Loss: 0.7330977916717529; Accuracy: 0.5\n",
      "\t\tTraining Batch 640: Loss: 0.6992011666297913; Accuracy: 0.75\n",
      "\t\tTraining Batch 650: Loss: 0.5551952123641968; Accuracy: 0.75\n",
      "\t\tTraining Batch 660: Loss: 0.5138483047485352; Accuracy: 1.0\n",
      "\t\tTraining Batch 670: Loss: 0.5492892265319824; Accuracy: 1.0\n",
      "\t\tTraining Batch 680: Loss: 0.6395961046218872; Accuracy: 0.75\n",
      "\t\tTraining Batch 690: Loss: 0.6196969151496887; Accuracy: 0.5\n",
      "\t\tTraining Batch 700: Loss: 0.6497469544410706; Accuracy: 0.75\n",
      "\t\tTraining Batch 710: Loss: 0.5249826908111572; Accuracy: 1.0\n",
      "\t\tTraining Batch 720: Loss: 0.762126624584198; Accuracy: 0.0\n",
      "\t\tTraining Batch 730: Loss: 0.5594666004180908; Accuracy: 1.0\n",
      "\t\tTraining Batch 740: Loss: 0.6103489398956299; Accuracy: 0.75\n",
      "\t\tTraining Batch 750: Loss: 0.6312836408615112; Accuracy: 0.5\n",
      "\t\tTraining Batch 760: Loss: 0.6208488345146179; Accuracy: 0.75\n",
      "\t\tTraining Batch 770: Loss: 0.6455140709877014; Accuracy: 0.75\n",
      "\t\tTraining Batch 780: Loss: 0.6657297611236572; Accuracy: 0.5\n",
      "\t\tTraining Batch 790: Loss: 0.6072368025779724; Accuracy: 0.75\n",
      "\t\tTraining Batch 800: Loss: 0.6895769834518433; Accuracy: 0.5\n",
      "\t\tTraining Batch 810: Loss: 0.6078476905822754; Accuracy: 1.0\n",
      "\t\tTraining Batch 820: Loss: 0.5893508195877075; Accuracy: 0.75\n",
      "\t\tTraining Batch 830: Loss: 0.7942099571228027; Accuracy: 0.5\n",
      "\t\tTraining Batch 840: Loss: 0.6773998737335205; Accuracy: 0.75\n",
      "\t\tTraining Batch 850: Loss: 0.6824969053268433; Accuracy: 0.75\n",
      "\t\tTraining Batch 860: Loss: 0.6751440763473511; Accuracy: 0.25\n",
      "\t\tTraining Batch 870: Loss: 0.5737619400024414; Accuracy: 0.75\n",
      "\t\tTraining Batch 880: Loss: 0.6795269250869751; Accuracy: 0.25\n",
      "\t\tTraining Batch 890: Loss: 0.6387568712234497; Accuracy: 0.5\n",
      "\t\tTraining Batch 900: Loss: 0.5094120502471924; Accuracy: 0.75\n",
      "\t\tTraining Batch 910: Loss: 0.8772129416465759; Accuracy: 0.25\n",
      "\t\tTraining Batch 920: Loss: 0.5359437465667725; Accuracy: 0.75\n",
      "\t\tTraining Batch 930: Loss: 0.6898137927055359; Accuracy: 0.5\n",
      "\t\tTraining Batch 940: Loss: 0.633417010307312; Accuracy: 0.75\n",
      "\t\tTraining Batch 950: Loss: 0.7633208632469177; Accuracy: 0.5\n",
      "\t\tTraining Batch 960: Loss: 0.7195297479629517; Accuracy: 0.5\n",
      "\t\tTraining Batch 970: Loss: 0.7441208362579346; Accuracy: 0.25\n",
      "\t\tTraining Batch 980: Loss: 0.9536056518554688; Accuracy: 0.25\n",
      "\t\tTraining Batch 990: Loss: 0.5742171406745911; Accuracy: 0.75\n",
      "\t\tTraining Batch 1000: Loss: 0.6462181806564331; Accuracy: 0.75\n",
      "\t\tTraining Batch 1010: Loss: 0.46866676211357117; Accuracy: 0.75\n",
      "\t\tTraining Batch 1020: Loss: 0.5548505187034607; Accuracy: 0.75\n",
      "\t\tTraining Batch 1030: Loss: 0.5049253106117249; Accuracy: 1.0\n",
      "\t\tTraining Batch 1040: Loss: 0.716050386428833; Accuracy: 0.5\n",
      "\t\tTraining Batch 1050: Loss: 0.8444886803627014; Accuracy: 0.25\n",
      "\t\tTraining Batch 1060: Loss: 0.9004076719284058; Accuracy: 0.25\n",
      "\t\tTraining Batch 1070: Loss: 0.6553331613540649; Accuracy: 0.75\n",
      "\t\tTraining Batch 1080: Loss: 0.5120314359664917; Accuracy: 0.75\n",
      "\t\tTraining Batch 1090: Loss: 0.7227590084075928; Accuracy: 0.5\n",
      "\t\tTraining Batch 1100: Loss: 0.7433525323867798; Accuracy: 0.25\n",
      "\t\tTraining Batch 1110: Loss: 0.7340560555458069; Accuracy: 0.5\n",
      "\t\tTraining Batch 1120: Loss: 0.5666871666908264; Accuracy: 1.0\n",
      "\t\tTraining Batch 1130: Loss: 0.4878207743167877; Accuracy: 0.75\n",
      "\t\tTraining Batch 1140: Loss: 0.8042854070663452; Accuracy: 0.25\n",
      "\t\tTraining Batch 1150: Loss: 0.5731863379478455; Accuracy: 1.0\n",
      "\t\tTraining Batch 1160: Loss: 0.6929298639297485; Accuracy: 0.75\n",
      "\t\tTraining Batch 1170: Loss: 0.8463451862335205; Accuracy: 0.25\n",
      "\t\tTraining Batch 1180: Loss: 0.5272363424301147; Accuracy: 0.75\n",
      "\t\tTraining Batch 1190: Loss: 0.645056426525116; Accuracy: 0.75\n",
      "\t\tTraining Batch 1200: Loss: 0.33602195978164673; Accuracy: 1.0\n",
      "\t\tTraining Batch 1210: Loss: 1.0544154644012451; Accuracy: 0.25\n",
      "\t\tTraining Batch 1220: Loss: 0.5905328989028931; Accuracy: 0.75\n",
      "\t\tTraining Batch 1230: Loss: 0.8072694540023804; Accuracy: 0.25\n",
      "\t\tTraining Batch 1240: Loss: 0.4885232150554657; Accuracy: 0.75\n",
      "\t\tTraining Batch 1250: Loss: 0.6365454792976379; Accuracy: 0.75\n",
      "\t\tTraining Batch 1260: Loss: 0.4235581159591675; Accuracy: 1.0\n",
      "\t\tTraining Batch 1270: Loss: 0.4665357172489166; Accuracy: 0.75\n",
      "\t\tTraining Batch 1280: Loss: 0.6033422350883484; Accuracy: 0.75\n",
      "\t\tTraining Batch 1290: Loss: 0.6515181064605713; Accuracy: 0.75\n",
      "\t\tTraining Batch 1300: Loss: 0.7524521350860596; Accuracy: 0.5\n",
      "\t\tTraining Batch 1310: Loss: 0.5302836298942566; Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 1320: Loss: 0.5759358406066895; Accuracy: 0.5\n",
      "\t\tTraining Batch 1330: Loss: 0.9518141746520996; Accuracy: 0.25\n",
      "\t\tTraining Batch 1340: Loss: 0.6253158450126648; Accuracy: 0.75\n",
      "\t\tTraining Batch 1350: Loss: 0.8189234733581543; Accuracy: 0.5\n",
      "Training:\n",
      "\tLoss: 0.6545896679712027; Accuracy: 0.6031687546057479\n",
      "\t\tValidation Batch 10: Loss: 0.6808688640594482; Accuracy: 0.5\n",
      "\t\tValidation Batch 20: Loss: 0.5858597755432129; Accuracy: 0.75\n",
      "\t\tValidation Batch 30: Loss: 0.42541372776031494; Accuracy: 1.0\n",
      "\t\tValidation Batch 40: Loss: 0.6444924473762512; Accuracy: 0.75\n",
      "\t\tValidation Batch 50: Loss: 0.8274423480033875; Accuracy: 0.25\n",
      "\t\tValidation Batch 60: Loss: 0.6418731212615967; Accuracy: 0.75\n",
      "\t\tValidation Batch 70: Loss: 0.41608762741088867; Accuracy: 1.0\n",
      "\t\tValidation Batch 80: Loss: 0.600793182849884; Accuracy: 0.75\n",
      "\t\tValidation Batch 90: Loss: 0.794623851776123; Accuracy: 0.5\n",
      "\t\tValidation Batch 100: Loss: 0.5658961534500122; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.4832763075828552; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.7650207281112671; Accuracy: 0.5\n",
      "\t\tValidation Batch 130: Loss: 0.6557164192199707; Accuracy: 0.75\n",
      "\t\tValidation Batch 140: Loss: 0.7797021269798279; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.35323482751846313; Accuracy: 1.0\n",
      "\t\tValidation Batch 160: Loss: 0.4334529638290405; Accuracy: 1.0\n",
      "Validation:\n",
      "\tLoss=0.6509038671851158; Accuracy: 0.6375\n",
      "Training epoch #2\n",
      "\t\tTraining Batch 10: Loss: 0.6116814017295837; Accuracy: 0.75\n",
      "\t\tTraining Batch 20: Loss: 0.8002115488052368; Accuracy: 0.5\n",
      "\t\tTraining Batch 30: Loss: 0.3413742482662201; Accuracy: 1.0\n",
      "\t\tTraining Batch 40: Loss: 0.33916789293289185; Accuracy: 1.0\n",
      "\t\tTraining Batch 50: Loss: 0.656358003616333; Accuracy: 0.5\n",
      "\t\tTraining Batch 60: Loss: 0.7699273824691772; Accuracy: 0.5\n",
      "\t\tTraining Batch 70: Loss: 0.7845911383628845; Accuracy: 0.5\n",
      "\t\tTraining Batch 80: Loss: 0.5768393278121948; Accuracy: 0.75\n",
      "\t\tTraining Batch 90: Loss: 0.6958496570587158; Accuracy: 0.5\n",
      "\t\tTraining Batch 100: Loss: 0.353158175945282; Accuracy: 1.0\n",
      "\t\tTraining Batch 110: Loss: 0.7652965784072876; Accuracy: 0.5\n",
      "\t\tTraining Batch 120: Loss: 0.33156895637512207; Accuracy: 1.0\n",
      "\t\tTraining Batch 130: Loss: 0.7782856225967407; Accuracy: 0.5\n",
      "\t\tTraining Batch 140: Loss: 0.49521327018737793; Accuracy: 0.75\n",
      "\t\tTraining Batch 150: Loss: 0.40491360425949097; Accuracy: 1.0\n",
      "\t\tTraining Batch 160: Loss: 0.524173378944397; Accuracy: 0.75\n",
      "\t\tTraining Batch 170: Loss: 0.7926663160324097; Accuracy: 0.5\n",
      "\t\tTraining Batch 180: Loss: 0.5599511861801147; Accuracy: 0.75\n",
      "\t\tTraining Batch 190: Loss: 0.34142550826072693; Accuracy: 1.0\n",
      "\t\tTraining Batch 200: Loss: 0.5838997960090637; Accuracy: 0.75\n",
      "\t\tTraining Batch 210: Loss: 0.7640466094017029; Accuracy: 0.5\n",
      "\t\tTraining Batch 220: Loss: 0.4901084005832672; Accuracy: 0.75\n",
      "\t\tTraining Batch 230: Loss: 0.785696268081665; Accuracy: 0.5\n",
      "\t\tTraining Batch 240: Loss: 0.3307095170021057; Accuracy: 1.0\n",
      "\t\tTraining Batch 250: Loss: 0.6519997119903564; Accuracy: 0.75\n",
      "\t\tTraining Batch 260: Loss: 0.44478529691696167; Accuracy: 1.0\n",
      "\t\tTraining Batch 270: Loss: 0.45073437690734863; Accuracy: 0.75\n",
      "\t\tTraining Batch 280: Loss: 0.3763951361179352; Accuracy: 1.0\n",
      "\t\tTraining Batch 290: Loss: 0.5836057662963867; Accuracy: 0.75\n",
      "\t\tTraining Batch 300: Loss: 0.5986433029174805; Accuracy: 0.75\n",
      "\t\tTraining Batch 310: Loss: 0.7771874666213989; Accuracy: 0.5\n",
      "\t\tTraining Batch 320: Loss: 0.5890960693359375; Accuracy: 0.75\n",
      "\t\tTraining Batch 330: Loss: 0.43748587369918823; Accuracy: 0.75\n",
      "\t\tTraining Batch 340: Loss: 0.5973987579345703; Accuracy: 0.75\n",
      "\t\tTraining Batch 350: Loss: 0.6827293634414673; Accuracy: 0.5\n",
      "\t\tTraining Batch 360: Loss: 0.537838339805603; Accuracy: 0.75\n",
      "\t\tTraining Batch 370: Loss: 0.5780909657478333; Accuracy: 0.75\n",
      "\t\tTraining Batch 380: Loss: 0.41472315788269043; Accuracy: 1.0\n",
      "\t\tTraining Batch 390: Loss: 0.36720120906829834; Accuracy: 1.0\n",
      "\t\tTraining Batch 400: Loss: 0.4866901636123657; Accuracy: 0.75\n",
      "\t\tTraining Batch 410: Loss: 0.6987860798835754; Accuracy: 0.5\n",
      "\t\tTraining Batch 420: Loss: 0.39585307240486145; Accuracy: 1.0\n",
      "\t\tTraining Batch 430: Loss: 0.3499279320240021; Accuracy: 1.0\n",
      "\t\tTraining Batch 440: Loss: 0.3285067677497864; Accuracy: 1.0\n",
      "\t\tTraining Batch 450: Loss: 0.38797926902770996; Accuracy: 1.0\n",
      "\t\tTraining Batch 460: Loss: 0.5776488780975342; Accuracy: 0.75\n",
      "\t\tTraining Batch 470: Loss: 0.34026604890823364; Accuracy: 1.0\n",
      "\t\tTraining Batch 480: Loss: 0.40947622060775757; Accuracy: 1.0\n",
      "\t\tTraining Batch 490: Loss: 0.33524996042251587; Accuracy: 1.0\n",
      "\t\tTraining Batch 500: Loss: 0.4958345293998718; Accuracy: 0.75\n",
      "\t\tTraining Batch 510: Loss: 0.39747804403305054; Accuracy: 1.0\n",
      "\t\tTraining Batch 520: Loss: 0.7585055232048035; Accuracy: 0.5\n",
      "\t\tTraining Batch 530: Loss: 0.5255892872810364; Accuracy: 0.5\n",
      "\t\tTraining Batch 540: Loss: 0.567964494228363; Accuracy: 0.75\n",
      "\t\tTraining Batch 550: Loss: 0.6322108507156372; Accuracy: 0.75\n",
      "\t\tTraining Batch 560: Loss: 0.4454489052295685; Accuracy: 0.75\n",
      "\t\tTraining Batch 570: Loss: 0.45028260350227356; Accuracy: 1.0\n",
      "\t\tTraining Batch 580: Loss: 0.7733477354049683; Accuracy: 0.5\n",
      "\t\tTraining Batch 590: Loss: 0.3304947018623352; Accuracy: 1.0\n",
      "\t\tTraining Batch 600: Loss: 0.624826967716217; Accuracy: 0.75\n",
      "\t\tTraining Batch 610: Loss: 0.5124069452285767; Accuracy: 0.75\n",
      "\t\tTraining Batch 620: Loss: 0.6765802502632141; Accuracy: 0.5\n",
      "\t\tTraining Batch 630: Loss: 0.7359573841094971; Accuracy: 0.25\n",
      "\t\tTraining Batch 640: Loss: 0.3330344557762146; Accuracy: 1.0\n",
      "\t\tTraining Batch 650: Loss: 0.5313770771026611; Accuracy: 0.75\n",
      "\t\tTraining Batch 660: Loss: 0.8453060984611511; Accuracy: 0.5\n",
      "\t\tTraining Batch 670: Loss: 0.5491929054260254; Accuracy: 1.0\n",
      "\t\tTraining Batch 680: Loss: 0.4691668450832367; Accuracy: 0.75\n",
      "\t\tTraining Batch 690: Loss: 0.3466241657733917; Accuracy: 1.0\n",
      "\t\tTraining Batch 700: Loss: 0.5521411895751953; Accuracy: 0.75\n",
      "\t\tTraining Batch 710: Loss: 0.40819302201271057; Accuracy: 1.0\n",
      "\t\tTraining Batch 720: Loss: 0.47012093663215637; Accuracy: 1.0\n",
      "\t\tTraining Batch 730: Loss: 0.5817682147026062; Accuracy: 0.75\n",
      "\t\tTraining Batch 740: Loss: 0.8307379484176636; Accuracy: 0.5\n",
      "\t\tTraining Batch 750: Loss: 0.5748863220214844; Accuracy: 0.75\n",
      "\t\tTraining Batch 760: Loss: 0.561328649520874; Accuracy: 0.75\n",
      "\t\tTraining Batch 770: Loss: 0.9339916110038757; Accuracy: 0.25\n",
      "\t\tTraining Batch 780: Loss: 0.8886919021606445; Accuracy: 0.25\n",
      "\t\tTraining Batch 790: Loss: 0.4518924355506897; Accuracy: 1.0\n",
      "\t\tTraining Batch 800: Loss: 0.36334851384162903; Accuracy: 1.0\n",
      "\t\tTraining Batch 810: Loss: 0.6493484973907471; Accuracy: 0.5\n",
      "\t\tTraining Batch 820: Loss: 0.401547908782959; Accuracy: 1.0\n",
      "\t\tTraining Batch 830: Loss: 0.5660095810890198; Accuracy: 0.75\n",
      "\t\tTraining Batch 840: Loss: 0.5632595419883728; Accuracy: 0.75\n",
      "\t\tTraining Batch 850: Loss: 0.3873178958892822; Accuracy: 1.0\n",
      "\t\tTraining Batch 860: Loss: 0.6305335164070129; Accuracy: 0.75\n",
      "\t\tTraining Batch 870: Loss: 0.5444177389144897; Accuracy: 0.75\n",
      "\t\tTraining Batch 880: Loss: 0.5581008791923523; Accuracy: 0.75\n",
      "\t\tTraining Batch 890: Loss: 0.330017626285553; Accuracy: 1.0\n",
      "\t\tTraining Batch 900: Loss: 0.9348033666610718; Accuracy: 0.25\n",
      "\t\tTraining Batch 910: Loss: 0.5403679609298706; Accuracy: 0.75\n",
      "\t\tTraining Batch 920: Loss: 0.8154052495956421; Accuracy: 0.5\n",
      "\t\tTraining Batch 930: Loss: 0.7932872772216797; Accuracy: 0.5\n",
      "\t\tTraining Batch 940: Loss: 0.8776315450668335; Accuracy: 0.25\n",
      "\t\tTraining Batch 950: Loss: 0.8901050090789795; Accuracy: 0.5\n",
      "\t\tTraining Batch 960: Loss: 0.5987517237663269; Accuracy: 0.5\n",
      "\t\tTraining Batch 970: Loss: 0.5691714882850647; Accuracy: 0.75\n",
      "\t\tTraining Batch 980: Loss: 0.5212245583534241; Accuracy: 0.75\n",
      "\t\tTraining Batch 990: Loss: 0.9316549301147461; Accuracy: 0.25\n",
      "\t\tTraining Batch 1000: Loss: 0.38713163137435913; Accuracy: 1.0\n",
      "\t\tTraining Batch 1010: Loss: 0.4469708502292633; Accuracy: 0.75\n",
      "\t\tTraining Batch 1020: Loss: 0.34018200635910034; Accuracy: 1.0\n",
      "\t\tTraining Batch 1030: Loss: 0.8014236092567444; Accuracy: 0.5\n",
      "\t\tTraining Batch 1040: Loss: 0.3171791434288025; Accuracy: 1.0\n",
      "\t\tTraining Batch 1050: Loss: 0.7387279272079468; Accuracy: 0.5\n",
      "\t\tTraining Batch 1060: Loss: 0.43371790647506714; Accuracy: 0.75\n",
      "\t\tTraining Batch 1070: Loss: 0.6325872540473938; Accuracy: 0.75\n",
      "\t\tTraining Batch 1080: Loss: 0.6235166788101196; Accuracy: 0.75\n",
      "\t\tTraining Batch 1090: Loss: 0.7137027978897095; Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 1100: Loss: 0.58341383934021; Accuracy: 0.75\n",
      "\t\tTraining Batch 1110: Loss: 0.5694642066955566; Accuracy: 0.75\n",
      "\t\tTraining Batch 1120: Loss: 0.6440480351448059; Accuracy: 0.75\n",
      "\t\tTraining Batch 1130: Loss: 0.3251721262931824; Accuracy: 1.0\n",
      "\t\tTraining Batch 1140: Loss: 0.32447871565818787; Accuracy: 1.0\n",
      "\t\tTraining Batch 1150: Loss: 0.3366040587425232; Accuracy: 1.0\n",
      "\t\tTraining Batch 1160: Loss: 0.916841983795166; Accuracy: 0.25\n",
      "\t\tTraining Batch 1170: Loss: 0.5570857524871826; Accuracy: 0.75\n",
      "\t\tTraining Batch 1180: Loss: 0.4131697118282318; Accuracy: 1.0\n",
      "\t\tTraining Batch 1190: Loss: 0.575005054473877; Accuracy: 0.75\n",
      "\t\tTraining Batch 1200: Loss: 0.33778131008148193; Accuracy: 1.0\n",
      "\t\tTraining Batch 1210: Loss: 0.820148229598999; Accuracy: 0.5\n",
      "\t\tTraining Batch 1220: Loss: 0.5366412401199341; Accuracy: 0.5\n",
      "\t\tTraining Batch 1230: Loss: 0.5592677593231201; Accuracy: 0.75\n",
      "\t\tTraining Batch 1240: Loss: 0.41840028762817383; Accuracy: 1.0\n",
      "\t\tTraining Batch 1250: Loss: 0.6575539112091064; Accuracy: 0.5\n",
      "\t\tTraining Batch 1260: Loss: 0.4168512523174286; Accuracy: 0.75\n",
      "\t\tTraining Batch 1270: Loss: 0.31969040632247925; Accuracy: 1.0\n",
      "\t\tTraining Batch 1280: Loss: 0.5797042846679688; Accuracy: 0.75\n",
      "\t\tTraining Batch 1290: Loss: 0.32275891304016113; Accuracy: 1.0\n",
      "\t\tTraining Batch 1300: Loss: 0.49048182368278503; Accuracy: 0.75\n",
      "\t\tTraining Batch 1310: Loss: 0.3252612352371216; Accuracy: 1.0\n",
      "\t\tTraining Batch 1320: Loss: 0.32911646366119385; Accuracy: 1.0\n",
      "\t\tTraining Batch 1330: Loss: 0.5708826184272766; Accuracy: 0.5\n",
      "\t\tTraining Batch 1340: Loss: 0.738558292388916; Accuracy: 0.5\n",
      "\t\tTraining Batch 1350: Loss: 0.4669049382209778; Accuracy: 0.75\n",
      "Training:\n",
      "\tLoss: 0.545332692898453; Accuracy: 0.7568165070007369\n",
      "\t\tValidation Batch 10: Loss: 0.6158785820007324; Accuracy: 0.75\n",
      "\t\tValidation Batch 20: Loss: 0.579852283000946; Accuracy: 0.75\n",
      "\t\tValidation Batch 30: Loss: 0.32495051622390747; Accuracy: 1.0\n",
      "\t\tValidation Batch 40: Loss: 0.680101752281189; Accuracy: 0.75\n",
      "\t\tValidation Batch 50: Loss: 0.8424241542816162; Accuracy: 0.25\n",
      "\t\tValidation Batch 60: Loss: 0.8166115880012512; Accuracy: 0.5\n",
      "\t\tValidation Batch 70: Loss: 0.49342575669288635; Accuracy: 0.75\n",
      "\t\tValidation Batch 80: Loss: 0.7898516654968262; Accuracy: 0.5\n",
      "\t\tValidation Batch 90: Loss: 0.8325467705726624; Accuracy: 0.5\n",
      "\t\tValidation Batch 100: Loss: 0.5624112486839294; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.5649433135986328; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.8161839842796326; Accuracy: 0.5\n",
      "\t\tValidation Batch 130: Loss: 0.8249009847640991; Accuracy: 0.5\n",
      "\t\tValidation Batch 140: Loss: 0.811100959777832; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.31650128960609436; Accuracy: 1.0\n",
      "\t\tValidation Batch 160: Loss: 0.4213644862174988; Accuracy: 1.0\n",
      "Validation:\n",
      "\tLoss=0.6169437654316425; Accuracy: 0.6828125\n",
      "Best accuracy (0.6828125) obtained at epoch #2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING LOOP\n",
    "# Variable for minimal accuracy\n",
    "MIN_ACCURACY = 0.73 # Based on the average accuracy\n",
    "REACHED_MIN_ACCURACY = False\n",
    "best_weights = class_model.state_dict()\n",
    "# Want to maximize accuracy\n",
    "max_val_acc = (0, 0)\n",
    "# Put the model in CPU\n",
    "class_model.cuda()\n",
    "# Create the optimizer\n",
    "param_optimizer = list(class_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "# I use the one that comes with the models, but any other optimizer could be used\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "# Store our loss and accuracy for plotting\n",
    "fit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "epoch_number = 0\n",
    "epoch_since_max = 0\n",
    "continue_learning = True\n",
    "while epoch_number < EPOCHS and continue_learning:\n",
    "    epoch_number += 1\n",
    "    print(f\"Training epoch #{epoch_number}\")\n",
    "    # Tracking variables\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Training\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    class_model.train()\n",
    "    # Freeze RoBERTa weights\n",
    "    #class_model.embedder.eval()\n",
    "    class_model.embedder.requires_grad_ = False\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)   \n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        # Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "        # Append to fit history\n",
    "        fit_history[\"loss\"].append(loss.item()) \n",
    "        fit_history[\"accuracy\"].append(b_accuracy) \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        tr_accuracy += b_accuracy\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if nb_tr_steps%10 == 0:\n",
    "            print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n",
    "    print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n",
    "    # Validation\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    class_model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        # Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "        # Append to fit history\n",
    "        fit_history[\"val_loss\"].append(loss.item()) \n",
    "        fit_history[\"val_accuracy\"].append(b_accuracy) \n",
    "        # Update tracking variables\n",
    "        eval_loss += loss.item()\n",
    "        eval_accuracy += b_accuracy\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        if nb_eval_steps%10 == 0:\n",
    "            print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "    eval_acc = eval_accuracy/nb_eval_steps\n",
    "    if eval_acc >= max_val_acc[0]:\n",
    "        max_val_acc = (eval_acc, epoch_number)\n",
    "        continue_learning = True\n",
    "        epoch_since_max = 0 # New max\n",
    "        best_weights = copy.deepcopy(class_model.state_dict()) # Keep the best weights\n",
    "        # See if we have reached min_accuracy\n",
    "        if eval_acc >= MIN_ACCURACY:\n",
    "            REACHED_MIN_ACCURACY = True\n",
    "        # Save to file only if it has reached min acc\n",
    "        if REACHED_MIN_ACCURACY:\n",
    "            # Save the best weights to file\n",
    "            torch.save(best_weights, os.path.join(PATH,'WiCHead.pt'))\n",
    "            continue_learning = False # Stop learning. Reached baseline acc for this model\n",
    "    else:\n",
    "        epoch_since_max += 1\n",
    "        if epoch_since_max > PATIENCE:\n",
    "            continue_learning = False # Stop learning, starting to overfit\n",
    "    print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "print(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")\n",
    "# Reload the best weights (from memory)\n",
    "class_model.load_state_dict(best_weights)\n",
    "\n",
    "\n",
    "#I am getting the error CUDA out of memory, I think this is because the batch size is too big, \n",
    "#though I'm not too sure what to change it too or where to find that variable.\n",
    "#I think it might be from lines 41 to 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1426adf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4502ddf87b86>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.6158785820007324; Accuracy: 0.75\n",
      "\t\tValidation Batch 20: Loss: 0.579852283000946; Accuracy: 0.75\n",
      "\t\tValidation Batch 30: Loss: 0.32495051622390747; Accuracy: 1.0\n",
      "\t\tValidation Batch 40: Loss: 0.680101752281189; Accuracy: 0.75\n",
      "\t\tValidation Batch 50: Loss: 0.8424241542816162; Accuracy: 0.25\n",
      "\t\tValidation Batch 60: Loss: 0.8166115880012512; Accuracy: 0.5\n",
      "\t\tValidation Batch 70: Loss: 0.49342575669288635; Accuracy: 0.75\n",
      "\t\tValidation Batch 80: Loss: 0.7898516654968262; Accuracy: 0.5\n",
      "\t\tValidation Batch 90: Loss: 0.8325467705726624; Accuracy: 0.5\n",
      "\t\tValidation Batch 100: Loss: 0.5624112486839294; Accuracy: 0.75\n",
      "\t\tValidation Batch 110: Loss: 0.5649433135986328; Accuracy: 0.75\n",
      "\t\tValidation Batch 120: Loss: 0.8161839842796326; Accuracy: 0.5\n",
      "\t\tValidation Batch 130: Loss: 0.8249009847640991; Accuracy: 0.5\n",
      "\t\tValidation Batch 140: Loss: 0.811100959777832; Accuracy: 0.5\n",
      "\t\tValidation Batch 150: Loss: 0.31650128960609436; Accuracy: 1.0\n",
      "\t\tValidation Batch 160: Loss: 0.4213644862174988; Accuracy: 1.0\n",
      "Validation:\n",
      "\tLoss=0.6169437654316425; Accuracy: 0.6828125\n",
      "OrderedDict([(0, True), (1, True), (2, True), (3, True), (4, True), (5, True), (6, True), (7, False), (8, True), (9, False), (10, True), (11, True), (12, True), (13, False), (14, False), (15, True), (16, True), (17, False), (18, True), (19, True), (20, True), (21, False), (22, True), (23, True), (24, True), (25, True), (26, False), (27, False), (28, False), (29, False), (30, True), (31, True), (32, False), (33, True), (34, False), (35, True), (36, False), (37, True), (38, True), (39, True), (40, False), (41, True), (42, True), (43, True), (44, True), (45, False), (46, False), (47, True), (48, False), (49, True), (50, True), (51, True), (52, True), (53, True), (54, True), (55, True), (56, False), (57, False), (58, False), (59, False), (60, True), (61, True), (62, True), (63, True), (64, True), (65, True), (66, True), (67, False), (68, True), (69, True), (70, False), (71, True), (72, True), (73, False), (74, False), (75, True), (76, False), (77, True), (78, True), (79, True), (80, False), (81, True), (82, True), (83, True), (84, True), (85, True), (86, True), (87, True), (88, True), (89, False), (90, True), (91, True), (92, False), (93, False), (94, False), (95, False), (96, True), (97, True), (98, True), (99, True), (100, True), (101, False), (102, True), (103, True), (104, False), (105, True), (106, True), (107, True), (108, True), (109, True), (110, True), (111, True), (112, True), (113, True), (114, True), (115, True), (116, True), (117, True), (118, True), (119, True), (120, True), (121, True), (122, True), (123, False), (124, True), (125, True), (126, False), (127, False), (128, True), (129, True), (130, True), (131, True), (132, True), (133, True), (134, True), (135, False), (136, True), (137, True), (138, False), (139, False), (140, True), (141, False), (142, True), (143, True), (144, False), (145, True), (146, True), (147, True), (148, False), (149, True), (150, False), (151, False), (152, True), (153, False), (154, False), (155, False), (156, True), (157, True), (158, False), (159, True), (160, True), (161, True), (162, True), (163, True), (164, True), (165, False), (166, False), (167, False), (168, False), (169, True), (170, True), (171, True), (172, True), (173, True), (174, True), (175, False), (176, True), (177, True), (178, True), (179, False), (180, True), (181, True), (182, False), (183, False), (184, True), (185, True), (186, True), (187, True), (188, True), (189, True), (190, False), (191, False), (192, True), (193, False), (194, True), (195, False), (196, True), (197, False), (198, False), (199, False), (200, True), (201, True), (202, True), (203, True), (204, True), (205, True), (206, True), (207, True), (208, True), (209, True), (210, True), (211, False), (212, True), (213, False), (214, True), (215, False), (216, False), (217, True), (218, False), (219, True), (220, True), (221, False), (222, True), (223, True), (224, False), (225, True), (226, False), (227, False), (228, True), (229, True), (230, True), (231, True), (232, True), (233, True), (234, False), (235, True), (236, False), (237, False), (238, True), (239, True), (240, False), (241, True), (242, True), (243, False), (244, True), (245, False), (246, True), (247, True), (248, True), (249, True), (250, False), (251, True), (252, True), (253, True), (254, True), (255, True), (256, True), (257, True), (258, True), (259, False), (260, False), (261, True), (262, True), (263, False), (264, True), (265, True), (266, True), (267, False), (268, True), (269, True), (270, True), (271, True), (272, True), (273, True), (274, True), (275, False), (276, False), (277, True), (278, True), (279, True), (280, True), (281, True), (282, True), (283, False), (284, True), (285, True), (286, True), (287, True), (288, True), (289, True), (290, True), (291, False), (292, True), (293, False), (294, True), (295, True), (296, True), (297, False), (298, True), (299, True), (300, True), (301, False), (302, False), (303, True), (304, True), (305, True), (306, True), (307, True), (308, False), (309, True), (310, False), (311, True), (312, True), (313, False), (314, True), (315, True), (316, False), (317, True), (318, False), (319, True), (320, False), (321, False), (322, True), (323, True), (324, True), (325, True), (326, True), (327, True), (328, True), (329, False), (330, False), (331, True), (332, True), (333, True), (334, False), (335, True), (336, False), (337, False), (338, True), (339, True), (340, True), (341, False), (342, False), (343, True), (344, False), (345, True), (346, True), (347, True), (348, False), (349, True), (350, True), (351, True), (352, False), (353, True), (354, True), (355, False), (356, False), (357, True), (358, False), (359, True), (360, True), (361, False), (362, False), (363, True), (364, True), (365, True), (366, False), (367, True), (368, False), (369, True), (370, True), (371, True), (372, True), (373, False), (374, True), (375, True), (376, True), (377, False), (378, False), (379, False), (380, True), (381, True), (382, True), (383, True), (384, True), (385, False), (386, True), (387, False), (388, True), (389, True), (390, True), (391, True), (392, True), (393, True), (394, True), (395, True), (396, False), (397, True), (398, True), (399, True), (400, True), (401, True), (402, False), (403, False), (404, True), (405, True), (406, True), (407, False), (408, False), (409, True), (410, True), (411, True), (412, False), (413, True), (414, True), (415, False), (416, True), (417, False), (418, False), (419, True), (420, True), (421, False), (422, False), (423, False), (424, True), (425, True), (426, True), (427, True), (428, False), (429, True), (430, True), (431, True), (432, True), (433, True), (434, False), (435, False), (436, False), (437, True), (438, True), (439, True), (440, True), (441, False), (442, True), (443, False), (444, True), (445, False), (446, False), (447, False), (448, False), (449, True), (450, True), (451, True), (452, False), (453, True), (454, True), (455, True), (456, True), (457, True), (458, True), (459, True), (460, True), (461, True), (462, True), (463, True), (464, True), (465, True), (466, True), (467, False), (468, False), (469, True), (470, False), (471, False), (472, True), (473, True), (474, True), (475, True), (476, False), (477, True), (478, True), (479, False), (480, False), (481, True), (482, False), (483, False), (484, True), (485, False), (486, True), (487, True), (488, True), (489, False), (490, True), (491, False), (492, True), (493, True), (494, True), (495, True), (496, False), (497, True), (498, True), (499, False), (500, True), (501, False), (502, True), (503, True), (504, False), (505, False), (506, False), (507, True), (508, True), (509, True), (510, True), (511, True), (512, False), (513, False), (514, True), (515, False), (516, False), (517, True), (518, True), (519, False), (520, True), (521, False), (522, True), (523, True), (524, True), (525, True), (526, False), (527, True), (528, False), (529, True), (530, True), (531, False), (532, True), (533, False), (534, True), (535, False), (536, False), (537, True), (538, True), (539, False), (540, True), (541, True), (542, True), (543, True), (544, True), (545, False), (546, True), (547, True), (548, True), (549, True), (550, True), (551, True), (552, False), (553, False), (554, True), (555, False), (556, True), (557, False), (558, False), (559, True), (560, False), (561, False), (562, True), (563, True), (564, True), (565, False), (566, False), (567, True), (568, False), (569, True), (570, False), (571, False), (572, False), (573, True), (574, True), (575, True), (576, True), (577, False), (578, False), (579, True), (580, True), (581, True), (582, True), (583, True), (584, True), (585, True), (586, True), (587, True), (588, True), (589, True), (590, False), (591, True), (592, True), (593, True), (594, False), (595, False), (596, True), (597, True), (598, True), (599, True), (600, True), (601, True), (602, True), (603, True), (604, False), (605, True), (606, True), (607, True), (608, True), (609, True), (610, False), (611, False), (612, True), (613, True), (614, True), (615, True), (616, True), (617, True), (618, False), (619, False), (620, False), (621, True), (622, False), (623, True), (624, True), (625, True), (626, False), (627, True), (628, True), (629, True), (630, True), (631, True), (632, True), (633, True), (634, True), (635, True), (636, True), (637, True)])\n"
     ]
    }
   ],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    # Calculate the accuracy\n",
    "    b_accuracy, b_pred_correctness = flat_accuracy(logits, label_ids, return_predict_correctness = True) # For RobertaForClassification\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    # Add to predictions\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "    # Update tracking variables\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765e200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
