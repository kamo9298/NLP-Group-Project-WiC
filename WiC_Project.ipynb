{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9463f36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turing data json into pandas dataframe\n",
    "# wic_df = pd.read_json('WiC/train.jsonl', lines=True)\n",
    "# print(wic_df.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808bb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_transformers import *\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc07cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d09989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determine how many elements we want to train during each iteration\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "# Prepare Torch to use GPU, and use CPU when it's not available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "#Get the GPU device name\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d070ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to import json objects from jsonl files line by line\n",
    "def parse_file_to_JSON(filename):\n",
    "    sorted_json_objs = []\n",
    "    \n",
    "    #grab each line and add it as an element in json objs arr\n",
    "    with open(filename, mode = \"r\") as jsonl_file:\n",
    "        for i in jsonl_file:\n",
    "            sorted_json_objs.append(json.loads(i))\n",
    "\n",
    "    return sorted_json_objs\n",
    "\n",
    "#Take a list of words (strings) and a sentence (as a RoBERTa tokenized ID list) and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "#Create a function that matches word in tokenized sentence\n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "    decomposedWord = tokenizer.encode(word)\n",
    "   # Iterate through to find a matching sublist of the token_ids\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "            return (i,i+len(decomposedWord)-1)\n",
    "    #finalize the output if there is no matching pattern found\n",
    "    return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "    #Create a intList that marks the positions of words\n",
    "    intList = []\n",
    "    #if intList is empty, call the previous function as no matching pattern found\n",
    "    for word in wordList:\n",
    "        if len(intList) == 0:\n",
    "            intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "        else:\n",
    "            afterLastInterval = intList[-1][1]+1\n",
    "            interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "            actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "            intList.append(actualPositions)\n",
    "    return intList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdf1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess the WiC data\n",
    "def wic_preprocessing(json_objects, training = True, shuffle_data = False, verbose = False):\n",
    "    wic_sentences = []\n",
    "    wic_encoded = []\n",
    "    wic_labels = []\n",
    "    wic_word_locs = []\n",
    "    wic_indexes = []\n",
    "    for index, example in enumerate(json_objects):\n",
    "        #wic_indexes.append(example['idx']) # Is it the index??\n",
    "        wic_indexes.append(index)\n",
    "        sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "        wic_sentences.append(sentence)\n",
    "        # Then encode the sentences\n",
    "        wic_encoded.append(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        # Find the word in each sentences\n",
    "        word = example['word']\n",
    "        word_locs = (-1, -1)\n",
    "        # Split the 2 sentences on space. (Also, lemmatize and uncapitilize each word)\n",
    "        sent1_split = example['sentence1'].split(' ')\n",
    "        sent2_split = example['sentence2'].split(' ')\n",
    "        # Get the index of word in both sentences\n",
    "        sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "        sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "        # Create a variable to keep track of the number of characters parsed in each sentence as we loop\n",
    "        sent_chars = 0\n",
    "        # Loop over the words in the first sentence\n",
    "        i, j = 0, 0\n",
    "        word1_not_found, word2_not_found = True, True\n",
    "        while word1_not_found and i < len(sent1_split):\n",
    "            word_len = len(sent1_split[i])\n",
    "            if sent_chars >= sent1_word_char_loc[0] or sent_chars + word_len >= sent1_word_char_loc[1]:\n",
    "                word_locs = (i, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            elif sent_chars > sent1_word_char_loc[1]:\n",
    "                # If we somehow got past the word. Assume it was the previous word\n",
    "                word_locs = (i - 1, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            else:\n",
    "                # Look at the next word\n",
    "                sent_chars += word_len + 1 # Plus one for the space\n",
    "                i += 1\n",
    "        # Loop over the words in the second\n",
    "        sent_chars = 0 # Reset\n",
    "        while word2_not_found and j < len(sent2_split):\n",
    "            word_len = len(sent2_split[j])\n",
    "            if sent_chars >= sent2_word_char_loc[0] or sent_chars + word_len >= sent2_word_char_loc[1]:\n",
    "                word_locs = (i, j) # Found the word in the sentence\n",
    "                word2_not_found = False\n",
    "            elif sent_chars > sent2_word_char_loc[1]:\n",
    "                # If we somehow got past the word. Assume it was the previous word\n",
    "                word_locs = (i, j - 1) # Found the word in the sentence\n",
    "                word2_not_found = False\n",
    "            else:\n",
    "                # Look at the next word\n",
    "                sent_chars += word_len + 1 # Plus one for the space\n",
    "                j += 1\n",
    "        # For testing\n",
    "        if verbose:\n",
    "            print(word)\n",
    "            print(sent1_split)\n",
    "            print(sent2_split)\n",
    "            print(word_locs)\n",
    "        # Now to find the word in the tokenized sentences\n",
    "        word1 = sent1_split[word_locs[0]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation (See https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "        word2 = sent2_split[word_locs[1]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n",
    "        token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "        wic_word_locs.append(token_word_locs)\n",
    "        # Get the label if we expect it to be there\n",
    "        if training:\n",
    "            if example['label']:\n",
    "                wic_labels.append(1)\n",
    "            else:\n",
    "                wic_labels.append(0)\n",
    "    # Pad the sequences and find the encoded word location in the combined input\n",
    "    max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "    wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "    for i in range(0, len(wic_encoded)):\n",
    "        enc_sentence = wic_encoded[i]\n",
    "        word_locs = wic_word_locs[i]\n",
    "        # Pad the sequences\n",
    "        ex_len = len(enc_sentence)\n",
    "        padded_sentence = enc_sentence.copy()\n",
    "        padded_sentence.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "        padded_mask = [1] * ex_len\n",
    "        padded_mask.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "        # Create the vector to get back the words after RoBERTa\n",
    "        token_word_locs = wic_word_locs[i]\n",
    "        first_word_loc = []\n",
    "        second_word_loc = []\n",
    "        len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "        len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "        for j in range(0, max_len):\n",
    "            if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "                #Part of the first word\n",
    "                first_word_loc.append(1.0 / len_first_word)\n",
    "            else:\n",
    "                first_word_loc.append(0.0)\n",
    "            if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "                #Part of the second word\n",
    "                second_word_loc.append(1.0 / len_second_word)\n",
    "            else:\n",
    "                second_word_loc.append(0.0)\n",
    "        # We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n",
    "        wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "        wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "        # token_type_ids is a mask that tells where the first and second sentences are\n",
    "        token_type_id = []\n",
    "        first_sentence = True\n",
    "        sentence_start = True\n",
    "        for token in padded_sentence:\n",
    "            if first_sentence and sentence_start and token == 0:\n",
    "                # Allows 0 at the start of the first sentence\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and token > 0:\n",
    "                if sentence_start:\n",
    "                    sentence_start = False\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and not sentence_start and token == 0:\n",
    "                first_sentence = False\n",
    "                # Start of second sentence\n",
    "                token_type_id.append(1)\n",
    "            else:\n",
    "                # Second sentence\n",
    "                token_type_id.append(1)\n",
    "        wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "    if training:\n",
    "        if shuffle_data:\n",
    "            # Shuffle the data\n",
    "            raw_set = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"labels\": [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : []}\n",
    "            raw_set[\"input_ids\"], raw_set[\"token_type_ids\"], raw_set[\"attention_mask\"], raw_set[\"labels\"], raw_set[\"word1_locs\"], raw_set[\"word2_locs\"], raw_set[\"index\"] = shuffle(\n",
    "              wic_padded[\"input_ids\"], wic_padded[\"token_type_ids\"], wic_padded[\"attention_mask\"], wic_labels, wic_padded[\"word1_locs\"], wic_padded[\"word2_locs\"], wic_padded[\"index\"])\n",
    "        else:\n",
    "            raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\n",
    "                     \"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\n",
    "                     \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    else: # No labels present (Testing set)\n",
    "        # Do not shuffle the testing set\n",
    "        raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \n",
    "               \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \n",
    "               \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    # Return the raw data (Need to put them in a PyTorch tensor and dataset)\n",
    "    return raw_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d19f44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.625\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "train_json_objs = parse_file_to_JSON(\"WiC/train.jsonl\")\n",
    "raw_train_set = wic_preprocessing(train_json_objs, shuffle_data=True, verbose = False) # We do not want to shuffle for now.\n",
    "print(len(raw_train_set[\"labels\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfca6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset for it\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(raw_train_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"index\"]).to(device)\n",
    ")\n",
    "# Create a sampler and loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35f4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json objects from each file\n",
    "test_json_objs = parse_file_to_JSON(\"WiC/test.jsonl\")\n",
    "valid_json_objs = parse_file_to_JSON(\"WiC/val.jsonl\")\n",
    "# Process the objects\n",
    "raw_test_set = wic_preprocessing(test_json_objs, training = False) # The labels for the testing set are unknown\n",
    "raw_valid_set = wic_preprocessing(valid_json_objs)\n",
    "# Create PyTorch datasets\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "# Create a sampler and loader for each\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812ecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in model\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, roberta_based_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        We are using a roBERTa model, adding a linear layer to take the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = roberta_based_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        same parameters as RoBERTa forward adding two tensors for the location of the 2 words to compare them\n",
    "        \"\"\"\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # get the embeddings (numerical representation)\n",
    "        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # The words from the sentences\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        \n",
    "        # seeing how different are the words by substracting the numbers that represent the words\n",
    "        diff = word1s - word2s\n",
    "        \n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        \n",
    "        # Calculate prediction label\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa355be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING THE MODEL\n",
    "class_model = WiC_Head(model, embedding_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f1b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the accuracy of our model\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    if return_predict_correctness:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "    else:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c09928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard\\AppData\\Local\\Temp/ipykernel_22200/3205015112.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n",
      "C:\\Users\\Howard\\anaconda3\\lib\\site-packages\\pytorch_transformers\\optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 10: Loss: 0.695138156414032; Accuracy: 0.40625\n",
      "\t\tTraining Batch 20: Loss: 0.7104807496070862; Accuracy: 0.46875\n",
      "\t\tTraining Batch 30: Loss: 0.6912863254547119; Accuracy: 0.5625\n",
      "\t\tTraining Batch 40: Loss: 0.694648027420044; Accuracy: 0.5\n",
      "\t\tTraining Batch 50: Loss: 0.6784292459487915; Accuracy: 0.625\n",
      "\t\tTraining Batch 60: Loss: 0.6779091358184814; Accuracy: 0.625\n",
      "\t\tTraining Batch 70: Loss: 0.6882362365722656; Accuracy: 0.625\n",
      "\t\tTraining Batch 80: Loss: 0.687254011631012; Accuracy: 0.4375\n",
      "\t\tTraining Batch 90: Loss: 0.6883943676948547; Accuracy: 0.40625\n",
      "\t\tTraining Batch 100: Loss: 0.6761201620101929; Accuracy: 0.5\n",
      "\t\tTraining Batch 110: Loss: 0.7097586989402771; Accuracy: 0.4375\n",
      "\t\tTraining Batch 120: Loss: 0.664814829826355; Accuracy: 0.75\n",
      "\t\tTraining Batch 130: Loss: 0.6893993020057678; Accuracy: 0.59375\n",
      "\t\tTraining Batch 140: Loss: 0.6757382154464722; Accuracy: 0.5\n",
      "\t\tTraining Batch 150: Loss: 0.6679637432098389; Accuracy: 0.5625\n",
      "\t\tTraining Batch 160: Loss: 0.6646366119384766; Accuracy: 0.65625\n",
      "\t\tTraining Batch 170: Loss: 0.7048983573913574; Accuracy: 0.55\n",
      "Training:\n",
      "\tLoss: 0.6837000289384056; Accuracy: 0.5449632352941176\n",
      "\t\tValidation Batch 10: Loss: 0.7130627036094666; Accuracy: 0.53125\n",
      "\t\tValidation Batch 20: Loss: 0.6812010407447815; Accuracy: 0.6333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6740464955568314; Accuracy: 0.6207291666666667\n",
      "Training epoch #2\n",
      "\t\tTraining Batch 10: Loss: 0.6697111129760742; Accuracy: 0.6875\n",
      "\t\tTraining Batch 20: Loss: 0.5988389849662781; Accuracy: 0.75\n",
      "\t\tTraining Batch 30: Loss: 0.5763674378395081; Accuracy: 0.8125\n",
      "\t\tTraining Batch 40: Loss: 0.6403560042381287; Accuracy: 0.65625\n",
      "\t\tTraining Batch 50: Loss: 0.5921453237533569; Accuracy: 0.6875\n",
      "\t\tTraining Batch 60: Loss: 0.564879834651947; Accuracy: 0.71875\n",
      "\t\tTraining Batch 70: Loss: 0.6340450644493103; Accuracy: 0.625\n",
      "\t\tTraining Batch 80: Loss: 0.6236376166343689; Accuracy: 0.6875\n",
      "\t\tTraining Batch 90: Loss: 0.6265732645988464; Accuracy: 0.65625\n",
      "\t\tTraining Batch 100: Loss: 0.6037397980690002; Accuracy: 0.6875\n",
      "\t\tTraining Batch 110: Loss: 0.4854755401611328; Accuracy: 0.84375\n",
      "\t\tTraining Batch 120: Loss: 0.5819040536880493; Accuracy: 0.71875\n",
      "\t\tTraining Batch 130: Loss: 0.5160112977027893; Accuracy: 0.78125\n",
      "\t\tTraining Batch 140: Loss: 0.5893969535827637; Accuracy: 0.71875\n",
      "\t\tTraining Batch 150: Loss: 0.5812702178955078; Accuracy: 0.6875\n",
      "\t\tTraining Batch 160: Loss: 0.6093342304229736; Accuracy: 0.6875\n",
      "\t\tTraining Batch 170: Loss: 0.5403035879135132; Accuracy: 0.8\n",
      "Training:\n",
      "\tLoss: 0.5990885376930237; Accuracy: 0.7076470588235294\n",
      "\t\tValidation Batch 10: Loss: 0.7184007167816162; Accuracy: 0.5\n",
      "\t\tValidation Batch 20: Loss: 0.6265003085136414; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6585444271564483; Accuracy: 0.6240625\n",
      "Training epoch #3\n",
      "\t\tTraining Batch 10: Loss: 0.5594502687454224; Accuracy: 0.6875\n",
      "\t\tTraining Batch 20: Loss: 0.5683409571647644; Accuracy: 0.71875\n",
      "\t\tTraining Batch 30: Loss: 0.515346884727478; Accuracy: 0.8125\n",
      "\t\tTraining Batch 40: Loss: 0.4395538568496704; Accuracy: 0.90625\n",
      "\t\tTraining Batch 50: Loss: 0.5506650805473328; Accuracy: 0.78125\n",
      "\t\tTraining Batch 60: Loss: 0.48015162348747253; Accuracy: 0.84375\n",
      "\t\tTraining Batch 70: Loss: 0.50582355260849; Accuracy: 0.8125\n",
      "\t\tTraining Batch 80: Loss: 0.5781251192092896; Accuracy: 0.75\n",
      "\t\tTraining Batch 90: Loss: 0.5292919278144836; Accuracy: 0.78125\n",
      "\t\tTraining Batch 100: Loss: 0.5506249070167542; Accuracy: 0.75\n",
      "\t\tTraining Batch 110: Loss: 0.565584123134613; Accuracy: 0.78125\n",
      "\t\tTraining Batch 120: Loss: 0.5328708291053772; Accuracy: 0.78125\n",
      "\t\tTraining Batch 130: Loss: 0.6303843259811401; Accuracy: 0.625\n",
      "\t\tTraining Batch 140: Loss: 0.4672214388847351; Accuracy: 0.84375\n",
      "\t\tTraining Batch 150: Loss: 0.4238121807575226; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.533313512802124; Accuracy: 0.8125\n",
      "\t\tTraining Batch 170: Loss: 0.5171582698822021; Accuracy: 0.8\n",
      "Training:\n",
      "\tLoss: 0.5162881139446708; Accuracy: 0.7896323529411765\n",
      "\t\tValidation Batch 10: Loss: 0.6541190147399902; Accuracy: 0.625\n",
      "\t\tValidation Batch 20: Loss: 0.5622974038124084; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6229939639568329; Accuracy: 0.6726041666666667\n",
      "Training epoch #4\n",
      "\t\tTraining Batch 10: Loss: 0.5079205632209778; Accuracy: 0.75\n",
      "\t\tTraining Batch 20: Loss: 0.5383966565132141; Accuracy: 0.78125\n",
      "\t\tTraining Batch 30: Loss: 0.39062628149986267; Accuracy: 0.90625\n",
      "\t\tTraining Batch 40: Loss: 0.4388906955718994; Accuracy: 0.90625\n",
      "\t\tTraining Batch 50: Loss: 0.46511194109916687; Accuracy: 0.875\n",
      "\t\tTraining Batch 60: Loss: 0.45553702116012573; Accuracy: 0.875\n",
      "\t\tTraining Batch 70: Loss: 0.4860963821411133; Accuracy: 0.8125\n",
      "\t\tTraining Batch 80: Loss: 0.4605407416820526; Accuracy: 0.8125\n",
      "\t\tTraining Batch 90: Loss: 0.4303748607635498; Accuracy: 0.875\n",
      "\t\tTraining Batch 100: Loss: 0.3557620942592621; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.5105468034744263; Accuracy: 0.75\n",
      "\t\tTraining Batch 120: Loss: 0.3714043200016022; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.4079558849334717; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.36866965889930725; Accuracy: 0.96875\n",
      "\t\tTraining Batch 150: Loss: 0.5067378282546997; Accuracy: 0.75\n",
      "\t\tTraining Batch 160: Loss: 0.5127893090248108; Accuracy: 0.75\n",
      "\t\tTraining Batch 170: Loss: 0.4425547122955322; Accuracy: 0.9\n",
      "Training:\n",
      "\tLoss: 0.4660670050803353; Accuracy: 0.8424264705882353\n",
      "\t\tValidation Batch 10: Loss: 0.6216596364974976; Accuracy: 0.71875\n",
      "\t\tValidation Batch 20: Loss: 0.5285640954971313; Accuracy: 0.7666666666666667\n",
      "Validation:\n",
      "\tLoss=0.6306583136320114; Accuracy: 0.6680208333333334\n",
      "Training epoch #5\n",
      "\t\tTraining Batch 10: Loss: 0.43218305706977844; Accuracy: 0.90625\n",
      "\t\tTraining Batch 20: Loss: 0.39844810962677; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.5662115216255188; Accuracy: 0.71875\n",
      "\t\tTraining Batch 40: Loss: 0.4217650890350342; Accuracy: 0.875\n",
      "\t\tTraining Batch 50: Loss: 0.3895767331123352; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.42652297019958496; Accuracy: 0.875\n",
      "\t\tTraining Batch 70: Loss: 0.41369199752807617; Accuracy: 0.90625\n",
      "\t\tTraining Batch 80: Loss: 0.37010544538497925; Accuracy: 0.9375\n",
      "\t\tTraining Batch 90: Loss: 0.4528903663158417; Accuracy: 0.875\n",
      "\t\tTraining Batch 100: Loss: 0.4654005765914917; Accuracy: 0.8125\n",
      "\t\tTraining Batch 110: Loss: 0.4615305960178375; Accuracy: 0.84375\n",
      "\t\tTraining Batch 120: Loss: 0.45378971099853516; Accuracy: 0.84375\n",
      "\t\tTraining Batch 130: Loss: 0.41304337978363037; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.31542760133743286; Accuracy: 1.0\n",
      "\t\tTraining Batch 150: Loss: 0.35535672307014465; Accuracy: 0.96875\n",
      "\t\tTraining Batch 160: Loss: 0.347974568605423; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.5967501401901245; Accuracy: 0.7\n",
      "Training:\n",
      "\tLoss: 0.4276011782533982; Accuracy: 0.886470588235294\n",
      "\t\tValidation Batch 10: Loss: 0.6291544437408447; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.5236746668815613; Accuracy: 0.8\n",
      "Validation:\n",
      "\tLoss=0.6277263015508652; Accuracy: 0.6775\n",
      "Training epoch #6\n",
      "\t\tTraining Batch 10: Loss: 0.34042951464653015; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.40173670649528503; Accuracy: 0.90625\n",
      "\t\tTraining Batch 30: Loss: 0.4347306787967682; Accuracy: 0.875\n",
      "\t\tTraining Batch 40: Loss: 0.44994792342185974; Accuracy: 0.8125\n",
      "\t\tTraining Batch 50: Loss: 0.36646735668182373; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.4249767065048218; Accuracy: 0.875\n",
      "\t\tTraining Batch 70: Loss: 0.4602692723274231; Accuracy: 0.8125\n",
      "\t\tTraining Batch 80: Loss: 0.4029321074485779; Accuracy: 0.90625\n",
      "\t\tTraining Batch 90: Loss: 0.4289778769016266; Accuracy: 0.875\n",
      "\t\tTraining Batch 100: Loss: 0.44023972749710083; Accuracy: 0.875\n",
      "\t\tTraining Batch 110: Loss: 0.42381221055984497; Accuracy: 0.90625\n",
      "\t\tTraining Batch 120: Loss: 0.3274087905883789; Accuracy: 1.0\n",
      "\t\tTraining Batch 130: Loss: 0.38613903522491455; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.38790163397789; Accuracy: 0.9375\n",
      "\t\tTraining Batch 150: Loss: 0.3307652473449707; Accuracy: 1.0\n",
      "\t\tTraining Batch 160: Loss: 0.36261555552482605; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.3814857602119446; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.404494161290281; Accuracy: 0.9074264705882352\n",
      "\t\tValidation Batch 10: Loss: 0.6074990034103394; Accuracy: 0.71875\n",
      "\t\tValidation Batch 20: Loss: 0.5495671629905701; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6028224766254425; Accuracy: 0.7007291666666666\n",
      "Training epoch #7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 10: Loss: 0.41100645065307617; Accuracy: 0.90625\n",
      "\t\tTraining Batch 20: Loss: 0.44908612966537476; Accuracy: 0.875\n",
      "\t\tTraining Batch 30: Loss: 0.36861786246299744; Accuracy: 0.9375\n",
      "\t\tTraining Batch 40: Loss: 0.37996602058410645; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.43932008743286133; Accuracy: 0.90625\n",
      "\t\tTraining Batch 60: Loss: 0.47695887088775635; Accuracy: 0.84375\n",
      "\t\tTraining Batch 70: Loss: 0.3753184676170349; Accuracy: 0.90625\n",
      "\t\tTraining Batch 80: Loss: 0.4390370547771454; Accuracy: 0.875\n",
      "\t\tTraining Batch 90: Loss: 0.35407841205596924; Accuracy: 0.96875\n",
      "\t\tTraining Batch 100: Loss: 0.40192481875419617; Accuracy: 0.875\n",
      "\t\tTraining Batch 110: Loss: 0.3586043417453766; Accuracy: 0.96875\n",
      "\t\tTraining Batch 120: Loss: 0.34670835733413696; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.44111141562461853; Accuracy: 0.875\n",
      "\t\tTraining Batch 140: Loss: 0.3993891179561615; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.4420548677444458; Accuracy: 0.875\n",
      "\t\tTraining Batch 160: Loss: 0.35413315892219543; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.4553813338279724; Accuracy: 0.85\n",
      "Training:\n",
      "\tLoss: 0.3929312855005264; Accuracy: 0.9211764705882353\n",
      "\t\tValidation Batch 10: Loss: 0.6866306662559509; Accuracy: 0.59375\n",
      "\t\tValidation Batch 20: Loss: 0.5127145648002625; Accuracy: 0.8333333333333334\n",
      "Validation:\n",
      "\tLoss=0.6256452322006225; Accuracy: 0.6807291666666667\n",
      "Training epoch #8\n",
      "\t\tTraining Batch 10: Loss: 0.36419588327407837; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.41439110040664673; Accuracy: 0.90625\n",
      "\t\tTraining Batch 30: Loss: 0.4715353548526764; Accuracy: 0.84375\n",
      "\t\tTraining Batch 40: Loss: 0.34941554069519043; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.340450257062912; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.33939477801322937; Accuracy: 0.96875\n",
      "\t\tTraining Batch 70: Loss: 0.43871670961380005; Accuracy: 0.84375\n",
      "\t\tTraining Batch 80: Loss: 0.32891231775283813; Accuracy: 1.0\n",
      "\t\tTraining Batch 90: Loss: 0.46353426575660706; Accuracy: 0.8125\n",
      "\t\tTraining Batch 100: Loss: 0.4929082989692688; Accuracy: 0.8125\n",
      "\t\tTraining Batch 110: Loss: 0.3912125825881958; Accuracy: 0.90625\n",
      "\t\tTraining Batch 120: Loss: 0.453727126121521; Accuracy: 0.84375\n",
      "\t\tTraining Batch 130: Loss: 0.34186437726020813; Accuracy: 0.96875\n",
      "\t\tTraining Batch 140: Loss: 0.3855989873409271; Accuracy: 0.9375\n",
      "\t\tTraining Batch 150: Loss: 0.4134158492088318; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.495249480009079; Accuracy: 0.78125\n",
      "\t\tTraining Batch 170: Loss: 0.3782883584499359; Accuracy: 0.9\n",
      "Training:\n",
      "\tLoss: 0.38502962782102473; Accuracy: 0.9266176470588235\n",
      "\t\tValidation Batch 10: Loss: 0.6467627882957458; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.5705843567848206; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6305553168058395; Accuracy: 0.6741666666666666\n",
      "Training epoch #9\n",
      "\t\tTraining Batch 10: Loss: 0.36193597316741943; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.37393975257873535; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.4372052252292633; Accuracy: 0.875\n",
      "\t\tTraining Batch 40: Loss: 0.37927940487861633; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.3496488034725189; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.4761279821395874; Accuracy: 0.84375\n",
      "\t\tTraining Batch 70: Loss: 0.35303646326065063; Accuracy: 0.9375\n",
      "\t\tTraining Batch 80: Loss: 0.33318090438842773; Accuracy: 1.0\n",
      "\t\tTraining Batch 90: Loss: 0.3983883857727051; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.34241095185279846; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.3230968117713928; Accuracy: 1.0\n",
      "\t\tTraining Batch 120: Loss: 0.3541809618473053; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.4057703912258148; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.39112767577171326; Accuracy: 0.875\n",
      "\t\tTraining Batch 150: Loss: 0.3562191426753998; Accuracy: 0.96875\n",
      "\t\tTraining Batch 160: Loss: 0.3770564794540405; Accuracy: 0.90625\n",
      "\t\tTraining Batch 170: Loss: 0.5145613551139832; Accuracy: 0.8\n",
      "Training:\n",
      "\tLoss: 0.37524644329267387; Accuracy: 0.938529411764706\n",
      "\t\tValidation Batch 10: Loss: 0.6265714764595032; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.5686305165290833; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6069507628679276; Accuracy: 0.6960416666666667\n",
      "Training epoch #10\n",
      "\t\tTraining Batch 10: Loss: 0.33597099781036377; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.38875553011894226; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.3721085786819458; Accuracy: 0.90625\n",
      "\t\tTraining Batch 40: Loss: 0.379364937543869; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.34326040744781494; Accuracy: 1.0\n",
      "\t\tTraining Batch 60: Loss: 0.3313654065132141; Accuracy: 1.0\n",
      "\t\tTraining Batch 70: Loss: 0.31347668170928955; Accuracy: 1.0\n",
      "\t\tTraining Batch 80: Loss: 0.37619897723197937; Accuracy: 0.9375\n",
      "\t\tTraining Batch 90: Loss: 0.34467798471450806; Accuracy: 0.96875\n",
      "\t\tTraining Batch 100: Loss: 0.3464968800544739; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.3203433156013489; Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#TRAINING LOOP\n",
    "# minimal accuracy\n",
    "MIN_ACCURACY = 0.73 # Based on the average accuracy on the training loop\n",
    "REACHED_MIN_ACCURACY = False\n",
    "best_weights = class_model.state_dict()\n",
    "# for maximizing the accuracy\n",
    "max_val_acc = (0, 0)\n",
    "# model in CPU\n",
    "class_model.cuda()\n",
    "# Optimizer: changing the weights to make the moder optimal\n",
    "param_optimizer = list(class_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "# Optimizer comes from hugging bert models\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "# Storing accuracy and loss\n",
    "fit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "epoch_number = 0\n",
    "epoch_since_max = 0\n",
    "continue_learning = True\n",
    "while epoch_number < EPOCHS and continue_learning:\n",
    "    epoch_number += 1\n",
    "    print(f\"Training epoch #{epoch_number}\")\n",
    "    # Tracking variables\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # training our model\n",
    "    class_model.train()\n",
    "    # Set the final weights\n",
    "    class_model.embedder.requires_grad_ = False\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward training\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n",
    "        # Backward training\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Update data to the CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        # Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "        # Append to fit history\n",
    "        fit_history[\"loss\"].append(loss.item()) \n",
    "        fit_history[\"accuracy\"].append(b_accuracy) \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        tr_accuracy += b_accuracy\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if nb_tr_steps%10 == 0:\n",
    "            print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n",
    "    print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n",
    "    # Evaluation\n",
    "    class_model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        # not computing gradients\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        # Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "        # Append to fit history\n",
    "        fit_history[\"val_loss\"].append(loss.item()) \n",
    "        fit_history[\"val_accuracy\"].append(b_accuracy) \n",
    "        # Update tracking variables\n",
    "        eval_loss += loss.item()\n",
    "        eval_accuracy += b_accuracy\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        if nb_eval_steps%10 == 0:\n",
    "            print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "    eval_acc = eval_accuracy/nb_eval_steps\n",
    "    if eval_acc >= max_val_acc[0]:\n",
    "        max_val_acc = (eval_acc, epoch_number)\n",
    "        continue_learning = True\n",
    "        epoch_since_max = 0 # New max\n",
    "        best_weights = copy.deepcopy(class_model.state_dict()) # Keep the best weights\n",
    "        # See if we have reached min_accuracy\n",
    "        if eval_acc >= MIN_ACCURACY:\n",
    "            REACHED_MIN_ACCURACY = True\n",
    "        # when it has reached min accuracy we want it to store the parameters and weights\n",
    "        if REACHED_MIN_ACCURACY:\n",
    "            # saving the best weights\n",
    "            #torch.save(best_weights, os.path.join(PATH,'WiCHead.pt'))\n",
    "            continue_learning = False # No necessary to continue learning\n",
    "    else:\n",
    "        epoch_since_max += 1\n",
    "        if epoch_since_max > PATIENCE:\n",
    "            continue_learning = False # It will not arrive to the desired accuracy, so we stop it\n",
    "    print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "print(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")\n",
    "# Reload the best weights (from memory)\n",
    "class_model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad570f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_predictions(preds):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return pred_flat == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1426adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    # Calculate the accuracy\n",
    "    b_accuracy, b_pred_correctness = flat_accuracy(logits, label_ids, return_predict_correctness = True) # For RobertaForClassification\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    # Add to predictions\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "    # Update tracking variables\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce44e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = {}\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_examples, nb_test_steps = 0, 0\n",
    "# Testing\n",
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = class_model(b_input_ids, attention_mask=b_input_mask, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # Get the predictions\n",
    "    b_preds = flat_predictions(logits)\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    for index, pred in zip(indexes, b_preds):\n",
    "        test_predictions[index] = pred\n",
    "    # Update tracking variables\n",
    "    test_loss += loss.item()\n",
    "    test_accuracy += b_accuracy\n",
    "    nb_test_examples += b_input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    if nb_test_steps%10 == 0:\n",
    "        print(\"\\t\\tTest Batch {}\".format(nb_test_steps))\n",
    "# Print final results\n",
    "print(\"Testing results\")\n",
    "test_predictions = collections.OrderedDict(sorted(test_predictions.items()))\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
