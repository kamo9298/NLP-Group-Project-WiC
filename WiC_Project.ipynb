{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2866d45",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "WiC Project \\\n",
    "Kaleb, Maria, Garrett, Howard\n",
    "\n",
    "Works Cited: \\\n",
    "https://github.com/llightts/CSI5138_Project/blob/master/RoBERTa_WiC_baseline.ipynb \\\n",
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808bb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_transformers import *\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc07cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74a810",
   "metadata": {},
   "source": [
    "TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d09989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Determine how many elements we want to train during each iteration\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "PATIENCE = 10\n",
    "# Prepare Torch to use GPU, and use CPU when it's not available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "#Get the GPU device name\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a267a34",
   "metadata": {},
   "source": [
    "Helper functions for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d070ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readfile function to take all the objects out of jsonl files\n",
    "def parse_file_to_JSON(filename):\n",
    "    serparated_json_objs = []\n",
    "    \n",
    "    #grab each line and add it as an element in json objs arr\n",
    "    with open(filename, mode = \"r\") as jsonl_file:\n",
    "        for i in jsonl_file:\n",
    "            serparated_json_objs.append(json.loads(i))\n",
    "\n",
    "    return serparated_json_objs\n",
    "\n",
    "#Take a list of words (strings) and a sentence (as a RoBERTa tokenized ID list) and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "#Create a function that matches word in tokenized sentence\n",
    "def find_word_in_tokenized_sentence(word,token_ids):\n",
    "    decomposedWord = tokenizer.encode(word)\n",
    "   #Iterate through to find a matching sublist of the token_ids\n",
    "    for i in range(len(token_ids)):\n",
    "        if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "            return (i,i+len(decomposedWord)-1)\n",
    "    #finalize the output if there is no matching pattern found\n",
    "    return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids):\n",
    "    #Create a intList that marks the positions of words\n",
    "    intList = []\n",
    "    #if intList is empty, call the previous function as no matching pattern found\n",
    "    for word in wordList:\n",
    "        if len(intList) == 0:\n",
    "            intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "        else:\n",
    "            afterLastInterval = intList[-1][1]+1\n",
    "            interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "            actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "            intList.append(actualPositions)\n",
    "    return intList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdf1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(json_objects, training = True):\n",
    "    \n",
    "    wic_sentences, wic_encoded, wic_labels, wic_word_locs, wic_indexes = [], [], [] ,[] ,[]\n",
    "    \n",
    "    for index, example in enumerate(json_objects):\n",
    "        \n",
    "        wic_indexes.append(index)\n",
    "        sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "        wic_sentences.append(sentence)\n",
    "\n",
    "        wic_encoded.append(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        \n",
    "        # locate word in context\n",
    "        word = example['word']\n",
    "        location_of_word = (-1, -1)\n",
    "        sent1_split = example['sentence1'].split(' ')\n",
    "        sent2_split = example['sentence2'].split(' ')\n",
    "        \n",
    "        # wic indx\n",
    "        sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "        sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "        \n",
    "        num_characters = 0\n",
    "        \n",
    "        i, j = 0, 0\n",
    "        word1_not_found, word2_not_found = True, True\n",
    "        \n",
    "        #locate word one\n",
    "        while word1_not_found and i < len(sent1_split):\n",
    "            word_len = len(sent1_split[i])\n",
    "            if num_characters >= sent1_word_char_loc[0] or num_characters + word_len >= sent1_word_char_loc[1]:\n",
    "                location_of_word = (i, -1) # Found the word in the sentence\n",
    "                word1_not_found = False\n",
    "            elif num_characters > sent1_word_char_loc[1]:\n",
    "                location_of_word = (i - 1, -1)\n",
    "                word1_not_found = False\n",
    "            else:\n",
    "                num_characters += word_len + 1 \n",
    "                i += 1\n",
    "                \n",
    "        #locate word two\n",
    "        num_characters = 0\n",
    "        \n",
    "        while word2_not_found and j < len(sent2_split):\n",
    "            word_len = len(sent2_split[j])\n",
    "            if num_characters >= sent2_word_char_loc[0] or num_characters + word_len >= sent2_word_char_loc[1]:\n",
    "                location_of_word = (i, j)\n",
    "                word2_not_found = False\n",
    "            elif num_characters > sent2_word_char_loc[1]:\n",
    "                location_of_word = (i, j - 1)\n",
    "                word2_not_found = False\n",
    "            else:\n",
    "                num_characters += word_len + 1\n",
    "                j += 1\n",
    "                \n",
    "        # Now to find the word in the tokenized sentences\n",
    "        word1 = sent1_split[location_of_word[0]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation (See https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "        word2 = sent2_split[location_of_word[1]].translate(str.maketrans('', '', string.punctuation)) #Remove punctuation\n",
    "        token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "        wic_word_locs.append(token_word_locs)\n",
    "        \n",
    "        # Get the label if we expect it to be there\n",
    "        if training:\n",
    "            if example['label']:\n",
    "                wic_labels.append(1)\n",
    "            else:\n",
    "                wic_labels.append(0)\n",
    "                \n",
    "    # Pad the sequences and find the encoded word location in the combined input\n",
    "    max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "    wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "    for i in range(0, len(wic_encoded)):\n",
    "        enc_sentence = wic_encoded[i]\n",
    "        location_of_word = wic_word_locs[i]\n",
    "        # Pad the sequences\n",
    "        ex_len = len(enc_sentence)\n",
    "        padded_sentence = enc_sentence.copy()\n",
    "        padded_sentence.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "        padded_mask = [1] * ex_len\n",
    "        padded_mask.extend([0]*(max_len - ex_len))\n",
    "        wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "        # Create the vector to get back the words after RoBERTa\n",
    "        token_word_locs = wic_word_locs[i]\n",
    "        first_word_loc = []\n",
    "        second_word_loc = []\n",
    "        len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "        len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "        for j in range(0, max_len):\n",
    "            if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "                #Part of the first word\n",
    "                first_word_loc.append(1.0 / len_first_word)\n",
    "            else:\n",
    "                first_word_loc.append(0.0)\n",
    "            if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "                #Part of the second word\n",
    "                second_word_loc.append(1.0 / len_second_word)\n",
    "            else:\n",
    "                second_word_loc.append(0.0)\n",
    "        #We want to append a [1, max_len] vector instead of a [max_len] vector so wrap in an array\n",
    "        wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "        wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "        #token_type_ids is a mask that tells where the first and second sentences are\n",
    "        token_type_id = []\n",
    "        first_sentence = True\n",
    "        sentence_start = True\n",
    "        for token in padded_sentence:\n",
    "            if first_sentence and sentence_start and token == 0:\n",
    "                #Allows 0 at the start of the first sentence\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and token > 0:\n",
    "                if sentence_start:\n",
    "                    sentence_start = False\n",
    "                token_type_id.append(0)\n",
    "            elif first_sentence and not sentence_start and token == 0:\n",
    "                first_sentence = False\n",
    "                #Start of second sentence\n",
    "                token_type_id.append(1)\n",
    "            else:\n",
    "                #Second sentence\n",
    "                token_type_id.append(1)\n",
    "        wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "        \n",
    "    if training:\n",
    "        for_tensor = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "    else:\n",
    "        for_tensor = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "\n",
    "    return for_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acf692",
   "metadata": {},
   "source": [
    "Read in jsonl, process our data for creating PyTorch dataset for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d19f44a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_file_to_JSON' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16932/3167793880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_json_objs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_file_to_JSON\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./WiC/train.jsonl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw_train_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_json_objs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_train_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parse_file_to_JSON' is not defined"
     ]
    }
   ],
   "source": [

    "#Data processing\n",
    "train_json_objs = parse_file_to_JSON(\"WiC/train.jsonl\")\n",
    "raw_train_set = preprocessing(train_json_objs, debug = False)\n",

    "print(len(raw_train_set[\"labels\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfca6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This makes our training data set for the training loop\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(raw_train_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_train_set[\"index\"]).to(device)\n",
    ")\n",
    "\n",
    "#This makes the sampler and data loader for our training loop\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35f4e3c",
   "metadata": {},
   "outputs": [],
   "source": [

    "#This loads the jsonl files and make json using the helper functions\n",
    "test_json_objs = parse_file_to_JSON(\"WiC/test.jsonl\")\n",
    "valid_json_objs = parse_file_to_JSON(\"WiC/val.jsonl\")\n",
    "\n",
    "#This does the preprocessing step for our json objects\n",
    "raw_test_set = preprocessing(test_json_objs, training = False)\n",

    "raw_valid_set = preprocessing(valid_json_objs)\n",
    "\n",
    "#These are out test and validation data sets to be used to get our final accuracy and our results\n",
    "#for the test.jsonl file.\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(raw_test_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_test_set[\"index\"]).to(device)\n",
    ")\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]).to(device),\n",
    "    torch.tensor(raw_valid_set[\"index\"]).to(device)\n",
    ")\n",
    "\n",
    "#This makes the sampler and data loader for the end of our program\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bfb45",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812ecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in model\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, roberta_based_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        We are using a roBERTa model, adding a linear layer to take the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = roberta_based_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
    "                word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        same parameters as RoBERTa forward adding two tensors for the location of the 2 words to compare them\n",
    "        \"\"\"\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # get the embeddings (numerical representation)\n",
    "        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # The words from the sentences\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        \n",
    "        # seeing how different are the words by substracting the numbers that represent the words\n",
    "        diff = word1s - word2s\n",
    "        \n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        \n",
    "        # Calculate prediction label\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa355be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model to be used\n",
    "class_model = WiC_Head(model, embedding_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f1b94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the accuracy of our model\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    if return_predict_correctness:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "    else:\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f1738",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c09928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard\\AppData\\Local\\Temp/ipykernel_5108/3205015112.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n",
      "C:\\Users\\Howard\\anaconda3\\lib\\site-packages\\pytorch_transformers\\optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 10: Loss: 0.6941076517105103; Accuracy: 0.40625\n",
      "\t\tTraining Batch 20: Loss: 0.6903315186500549; Accuracy: 0.40625\n",
      "\t\tTraining Batch 30: Loss: 0.6995317339897156; Accuracy: 0.53125\n",
      "\t\tTraining Batch 40: Loss: 0.6926210522651672; Accuracy: 0.46875\n",
      "\t\tTraining Batch 50: Loss: 0.68914395570755; Accuracy: 0.4375\n",
      "\t\tTraining Batch 60: Loss: 0.6916593909263611; Accuracy: 0.53125\n",
      "\t\tTraining Batch 70: Loss: 0.6883968114852905; Accuracy: 0.5625\n",
      "\t\tTraining Batch 80: Loss: 0.6814053058624268; Accuracy: 0.65625\n",
      "\t\tTraining Batch 90: Loss: 0.6853395700454712; Accuracy: 0.5625\n",
      "\t\tTraining Batch 100: Loss: 0.6658540368080139; Accuracy: 0.71875\n",
      "\t\tTraining Batch 110: Loss: 0.6752045154571533; Accuracy: 0.59375\n",
      "\t\tTraining Batch 120: Loss: 0.6800318360328674; Accuracy: 0.65625\n",
      "\t\tTraining Batch 130: Loss: 0.6959060430526733; Accuracy: 0.46875\n",
      "\t\tTraining Batch 140: Loss: 0.6177184581756592; Accuracy: 0.65625\n",
      "\t\tTraining Batch 150: Loss: 0.6642955541610718; Accuracy: 0.71875\n",
      "\t\tTraining Batch 160: Loss: 0.6568031311035156; Accuracy: 0.6875\n",
      "\t\tTraining Batch 170: Loss: 0.628015398979187; Accuracy: 0.75\n",
      "Training:\n",
      "\tLoss: 0.6802430152893066; Accuracy: 0.5612132352941176\n",
      "\t\tValidation Batch 10: Loss: 0.677268922328949; Accuracy: 0.5625\n",
      "\t\tValidation Batch 20: Loss: 0.6256638169288635; Accuracy: 0.6666666666666666\n",
      "Validation:\n",
      "\tLoss=0.6596589535474777; Accuracy: 0.6098958333333333\n",
      "Training epoch #2\n",
      "\t\tTraining Batch 10: Loss: 0.6094797849655151; Accuracy: 0.71875\n",
      "\t\tTraining Batch 20: Loss: 0.6326151490211487; Accuracy: 0.75\n",
      "\t\tTraining Batch 30: Loss: 0.6115437746047974; Accuracy: 0.71875\n",
      "\t\tTraining Batch 40: Loss: 0.5084702968597412; Accuracy: 0.875\n",
      "\t\tTraining Batch 50: Loss: 0.6188003420829773; Accuracy: 0.59375\n",
      "\t\tTraining Batch 60: Loss: 0.6227871179580688; Accuracy: 0.71875\n",
      "\t\tTraining Batch 70: Loss: 0.5624014735221863; Accuracy: 0.78125\n",
      "\t\tTraining Batch 80: Loss: 0.5952939987182617; Accuracy: 0.75\n",
      "\t\tTraining Batch 90: Loss: 0.607902467250824; Accuracy: 0.71875\n",
      "\t\tTraining Batch 100: Loss: 0.6422836780548096; Accuracy: 0.6875\n",
      "\t\tTraining Batch 110: Loss: 0.5181591510772705; Accuracy: 0.84375\n",
      "\t\tTraining Batch 120: Loss: 0.5625960826873779; Accuracy: 0.78125\n",
      "\t\tTraining Batch 130: Loss: 0.5830225348472595; Accuracy: 0.6875\n",
      "\t\tTraining Batch 140: Loss: 0.6605712175369263; Accuracy: 0.65625\n",
      "\t\tTraining Batch 150: Loss: 0.5748217701911926; Accuracy: 0.75\n",
      "\t\tTraining Batch 160: Loss: 0.48989927768707275; Accuracy: 0.875\n",
      "\t\tTraining Batch 170: Loss: 0.5203585028648376; Accuracy: 0.85\n",
      "Training:\n",
      "\tLoss: 0.5909572820453083; Accuracy: 0.7174999999999999\n",
      "\t\tValidation Batch 10: Loss: 0.6905608177185059; Accuracy: 0.5625\n",
      "\t\tValidation Batch 20: Loss: 0.5716080069541931; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6211627721786499; Accuracy: 0.6710416666666666\n",
      "Training epoch #3\n",
      "\t\tTraining Batch 10: Loss: 0.4757780134677887; Accuracy: 0.8125\n",
      "\t\tTraining Batch 20: Loss: 0.515549898147583; Accuracy: 0.78125\n",
      "\t\tTraining Batch 30: Loss: 0.5257098078727722; Accuracy: 0.78125\n",
      "\t\tTraining Batch 40: Loss: 0.45704200863838196; Accuracy: 0.875\n",
      "\t\tTraining Batch 50: Loss: 0.49446001648902893; Accuracy: 0.78125\n",
      "\t\tTraining Batch 60: Loss: 0.5731951594352722; Accuracy: 0.71875\n",
      "\t\tTraining Batch 70: Loss: 0.4645036458969116; Accuracy: 0.84375\n",
      "\t\tTraining Batch 80: Loss: 0.4946051836013794; Accuracy: 0.8125\n",
      "\t\tTraining Batch 90: Loss: 0.48366492986679077; Accuracy: 0.84375\n",
      "\t\tTraining Batch 100: Loss: 0.5455260276794434; Accuracy: 0.71875\n",
      "\t\tTraining Batch 110: Loss: 0.5544361472129822; Accuracy: 0.71875\n",
      "\t\tTraining Batch 120: Loss: 0.46604278683662415; Accuracy: 0.84375\n",
      "\t\tTraining Batch 130: Loss: 0.5462503433227539; Accuracy: 0.75\n",
      "\t\tTraining Batch 140: Loss: 0.5729393362998962; Accuracy: 0.71875\n",
      "\t\tTraining Batch 150: Loss: 0.43288978934288025; Accuracy: 0.875\n",
      "\t\tTraining Batch 160: Loss: 0.5100354552268982; Accuracy: 0.8125\n",
      "\t\tTraining Batch 170: Loss: 0.5052603483200073; Accuracy: 0.8\n",
      "Training:\n",
      "\tLoss: 0.5050424894865821; Accuracy: 0.8041544117647059\n",
      "\t\tValidation Batch 10: Loss: 0.7577629089355469; Accuracy: 0.5\n",
      "\t\tValidation Batch 20: Loss: 0.5639598965644836; Accuracy: 0.7666666666666667\n",
      "Validation:\n",
      "\tLoss=0.6397520720958709; Accuracy: 0.6555208333333333\n",
      "Training epoch #4\n",
      "\t\tTraining Batch 10: Loss: 0.45334330201148987; Accuracy: 0.84375\n",
      "\t\tTraining Batch 20: Loss: 0.44050800800323486; Accuracy: 0.875\n",
      "\t\tTraining Batch 30: Loss: 0.4485074281692505; Accuracy: 0.875\n",
      "\t\tTraining Batch 40: Loss: 0.4628664553165436; Accuracy: 0.84375\n",
      "\t\tTraining Batch 50: Loss: 0.41662755608558655; Accuracy: 0.90625\n",
      "\t\tTraining Batch 60: Loss: 0.37876349687576294; Accuracy: 0.90625\n",
      "\t\tTraining Batch 70: Loss: 0.4018956422805786; Accuracy: 0.9375\n",
      "\t\tTraining Batch 80: Loss: 0.4524914026260376; Accuracy: 0.84375\n",
      "\t\tTraining Batch 90: Loss: 0.421750545501709; Accuracy: 0.90625\n",
      "\t\tTraining Batch 100: Loss: 0.45208224654197693; Accuracy: 0.875\n",
      "\t\tTraining Batch 110: Loss: 0.4375295042991638; Accuracy: 0.84375\n",
      "\t\tTraining Batch 120: Loss: 0.38750842213630676; Accuracy: 0.9375\n",
      "\t\tTraining Batch 130: Loss: 0.4702111482620239; Accuracy: 0.84375\n",
      "\t\tTraining Batch 140: Loss: 0.5349140763282776; Accuracy: 0.75\n",
      "\t\tTraining Batch 150: Loss: 0.44140684604644775; Accuracy: 0.875\n",
      "\t\tTraining Batch 160: Loss: 0.5082428455352783; Accuracy: 0.8125\n",
      "\t\tTraining Batch 170: Loss: 0.3271382451057434; Accuracy: 1.0\n",
      "Training:\n",
      "\tLoss: 0.4534260258955114; Accuracy: 0.8580882352941176\n",
      "\t\tValidation Batch 10: Loss: 0.6938066482543945; Accuracy: 0.625\n",
      "\t\tValidation Batch 20: Loss: 0.5978574752807617; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6183334827423096; Accuracy: 0.6803125\n",
      "Training epoch #5\n",
      "\t\tTraining Batch 10: Loss: 0.38743558526039124; Accuracy: 0.9375\n",
      "\t\tTraining Batch 20: Loss: 0.3561493456363678; Accuracy: 0.96875\n",
      "\t\tTraining Batch 30: Loss: 0.39498600363731384; Accuracy: 0.90625\n",
      "\t\tTraining Batch 40: Loss: 0.48978379368782043; Accuracy: 0.8125\n",
      "\t\tTraining Batch 50: Loss: 0.4721815586090088; Accuracy: 0.8125\n",
      "\t\tTraining Batch 60: Loss: 0.47492191195487976; Accuracy: 0.84375\n",
      "\t\tTraining Batch 70: Loss: 0.3671657145023346; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.46783754229545593; Accuracy: 0.78125\n",
      "\t\tTraining Batch 90: Loss: 0.36751532554626465; Accuracy: 0.96875\n",
      "\t\tTraining Batch 100: Loss: 0.42793217301368713; Accuracy: 0.875\n",
      "\t\tTraining Batch 110: Loss: 0.3521910011768341; Accuracy: 0.96875\n",
      "\t\tTraining Batch 120: Loss: 0.39864376187324524; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.3958169221878052; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.4134458303451538; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.44541627168655396; Accuracy: 0.875\n",
      "\t\tTraining Batch 160: Loss: 0.36048683524131775; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.442431777715683; Accuracy: 0.9\n",
      "Training:\n",
      "\tLoss: 0.4280585338087643; Accuracy: 0.8808455882352941\n",
      "\t\tValidation Batch 10: Loss: 0.6763426661491394; Accuracy: 0.59375\n",
      "\t\tValidation Batch 20: Loss: 0.5637509226799011; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6086072862148285; Accuracy: 0.6929166666666666\n",
      "Training epoch #6\n",
      "\t\tTraining Batch 10: Loss: 0.4556018114089966; Accuracy: 0.875\n",
      "\t\tTraining Batch 20: Loss: 0.39613455533981323; Accuracy: 0.875\n",
      "\t\tTraining Batch 30: Loss: 0.44614526629447937; Accuracy: 0.84375\n",
      "\t\tTraining Batch 40: Loss: 0.39876052737236023; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.3790317177772522; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.3751367926597595; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.36338239908218384; Accuracy: 0.9375\n",
      "\t\tTraining Batch 80: Loss: 0.4167720377445221; Accuracy: 0.875\n",
      "\t\tTraining Batch 90: Loss: 0.4265764355659485; Accuracy: 0.875\n",
      "\t\tTraining Batch 100: Loss: 0.3835199177265167; Accuracy: 0.90625\n",
      "\t\tTraining Batch 110: Loss: 0.4610360264778137; Accuracy: 0.84375\n",
      "\t\tTraining Batch 120: Loss: 0.45455116033554077; Accuracy: 0.84375\n",
      "\t\tTraining Batch 130: Loss: 0.42411521077156067; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.5678935050964355; Accuracy: 0.6875\n",
      "\t\tTraining Batch 150: Loss: 0.4182263910770416; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.3700716495513916; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.5137253999710083; Accuracy: 0.8\n",
      "Training:\n",
      "\tLoss: 0.405229897534146; Accuracy: 0.9061764705882354\n",
      "\t\tValidation Batch 10: Loss: 0.7295666933059692; Accuracy: 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 20: Loss: 0.5810916423797607; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.633550038933754; Accuracy: 0.6710416666666666\n",
      "Training epoch #7\n",
      "\t\tTraining Batch 10: Loss: 0.3935996890068054; Accuracy: 0.90625\n",
      "\t\tTraining Batch 20: Loss: 0.4016377627849579; Accuracy: 0.90625\n",
      "\t\tTraining Batch 30: Loss: 0.34877708554267883; Accuracy: 0.96875\n",
      "\t\tTraining Batch 40: Loss: 0.3976299464702606; Accuracy: 0.90625\n",
      "\t\tTraining Batch 50: Loss: 0.3636586666107178; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.3721553087234497; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.33298975229263306; Accuracy: 1.0\n",
      "\t\tTraining Batch 80: Loss: 0.428194135427475; Accuracy: 0.875\n",
      "\t\tTraining Batch 90: Loss: 0.5179925560951233; Accuracy: 0.78125\n",
      "\t\tTraining Batch 100: Loss: 0.34735286235809326; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.3472777307033539; Accuracy: 0.96875\n",
      "\t\tTraining Batch 120: Loss: 0.39437413215637207; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.3602434992790222; Accuracy: 0.96875\n",
      "\t\tTraining Batch 140: Loss: 0.4716462790966034; Accuracy: 0.84375\n",
      "\t\tTraining Batch 150: Loss: 0.38071951270103455; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.4132322072982788; Accuracy: 0.90625\n",
      "\t\tTraining Batch 170: Loss: 0.3645573556423187; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.3917516519041622; Accuracy: 0.9213970588235293\n",
      "\t\tValidation Batch 10: Loss: 0.7347893714904785; Accuracy: 0.53125\n",
      "\t\tValidation Batch 20: Loss: 0.6110077500343323; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6365365862846375; Accuracy: 0.6678124999999999\n",
      "Training epoch #8\n",
      "\t\tTraining Batch 10: Loss: 0.4175679683685303; Accuracy: 0.875\n",
      "\t\tTraining Batch 20: Loss: 0.3154984414577484; Accuracy: 1.0\n",
      "\t\tTraining Batch 30: Loss: 0.41097337007522583; Accuracy: 0.875\n",
      "\t\tTraining Batch 40: Loss: 0.3492981195449829; Accuracy: 0.96875\n",
      "\t\tTraining Batch 50: Loss: 0.3459833562374115; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.33976349234580994; Accuracy: 0.96875\n",
      "\t\tTraining Batch 70: Loss: 0.34944379329681396; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.37451833486557007; Accuracy: 0.9375\n",
      "\t\tTraining Batch 90: Loss: 0.3893894851207733; Accuracy: 0.90625\n",
      "\t\tTraining Batch 100: Loss: 0.35361307859420776; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.35912951827049255; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.4158226251602173; Accuracy: 0.84375\n",
      "\t\tTraining Batch 130: Loss: 0.431998610496521; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.364492267370224; Accuracy: 0.9375\n",
      "\t\tTraining Batch 150: Loss: 0.4478722810745239; Accuracy: 0.875\n",
      "\t\tTraining Batch 160: Loss: 0.40903276205062866; Accuracy: 0.90625\n",
      "\t\tTraining Batch 170: Loss: 0.3838178515434265; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.38302148825982035; Accuracy: 0.9316911764705882\n",
      "\t\tValidation Batch 10: Loss: 0.6378378868103027; Accuracy: 0.625\n",
      "\t\tValidation Batch 20: Loss: 0.6408255696296692; Accuracy: 0.6666666666666666\n",
      "Validation:\n",
      "\tLoss=0.6043470218777657; Accuracy: 0.6973958333333333\n",
      "Training epoch #9\n",
      "\t\tTraining Batch 10: Loss: 0.4223163425922394; Accuracy: 0.90625\n",
      "\t\tTraining Batch 20: Loss: 0.42033860087394714; Accuracy: 0.90625\n",
      "\t\tTraining Batch 30: Loss: 0.3273686468601227; Accuracy: 1.0\n",
      "\t\tTraining Batch 40: Loss: 0.3227056562900543; Accuracy: 1.0\n",
      "\t\tTraining Batch 50: Loss: 0.3769546449184418; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.37692052125930786; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.4134911000728607; Accuracy: 0.90625\n",
      "\t\tTraining Batch 80: Loss: 0.3777453303337097; Accuracy: 0.9375\n",
      "\t\tTraining Batch 90: Loss: 0.34343165159225464; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.38608092069625854; Accuracy: 0.90625\n",
      "\t\tTraining Batch 110: Loss: 0.3764086067676544; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.34008705615997314; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.39066484570503235; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.3245965838432312; Accuracy: 1.0\n",
      "\t\tTraining Batch 150: Loss: 0.3695072531700134; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.33303481340408325; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.3691352903842926; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.3730178606860778; Accuracy: 0.9405147058823529\n",
      "\t\tValidation Batch 10: Loss: 0.6751286387443542; Accuracy: 0.625\n",
      "\t\tValidation Batch 20: Loss: 0.5854241251945496; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6248875260353088; Accuracy: 0.67875\n",
      "Training epoch #10\n",
      "\t\tTraining Batch 10: Loss: 0.3442045748233795; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.347709596157074; Accuracy: 0.96875\n",
      "\t\tTraining Batch 30: Loss: 0.3835723400115967; Accuracy: 0.9375\n",
      "\t\tTraining Batch 40: Loss: 0.34743061661720276; Accuracy: 0.96875\n",
      "\t\tTraining Batch 50: Loss: 0.3265039920806885; Accuracy: 1.0\n",
      "\t\tTraining Batch 60: Loss: 0.38455504179000854; Accuracy: 0.90625\n",
      "\t\tTraining Batch 70: Loss: 0.34937578439712524; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.34706392884254456; Accuracy: 0.96875\n",
      "\t\tTraining Batch 90: Loss: 0.35806888341903687; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.40970557928085327; Accuracy: 0.90625\n",
      "\t\tTraining Batch 110: Loss: 0.3297000825405121; Accuracy: 0.96875\n",
      "\t\tTraining Batch 120: Loss: 0.4134160876274109; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.37752172350883484; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.3674253821372986; Accuracy: 0.96875\n",
      "\t\tTraining Batch 150: Loss: 0.37100765109062195; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.31600314378738403; Accuracy: 1.0\n",
      "\t\tTraining Batch 170: Loss: 0.31539005041122437; Accuracy: 1.0\n",
      "Training:\n",
      "\tLoss: 0.3693616691757651; Accuracy: 0.9441176470588235\n",
      "\t\tValidation Batch 10: Loss: 0.6037158370018005; Accuracy: 0.6875\n",
      "\t\tValidation Batch 20: Loss: 0.5955640077590942; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.5981480151414871; Accuracy: 0.71\n",
      "Training epoch #11\n",
      "\t\tTraining Batch 10: Loss: 0.38015446066856384; Accuracy: 0.9375\n",
      "\t\tTraining Batch 20: Loss: 0.3776642680168152; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.3362756073474884; Accuracy: 0.96875\n",
      "\t\tTraining Batch 40: Loss: 0.3623810410499573; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.3629232943058014; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.3774203658103943; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.3179953098297119; Accuracy: 1.0\n",
      "\t\tTraining Batch 80: Loss: 0.3415105640888214; Accuracy: 0.96875\n",
      "\t\tTraining Batch 90: Loss: 0.38223138451576233; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.3446636199951172; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.43043673038482666; Accuracy: 0.875\n",
      "\t\tTraining Batch 120: Loss: 0.36219915747642517; Accuracy: 0.9375\n",
      "\t\tTraining Batch 130: Loss: 0.3870217800140381; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.34824609756469727; Accuracy: 0.96875\n",
      "\t\tTraining Batch 150: Loss: 0.39486825466156006; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.33438947796821594; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.36406779289245605; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.36574761061107414; Accuracy: 0.9460294117647058\n",
      "\t\tValidation Batch 10: Loss: 0.5911756753921509; Accuracy: 0.71875\n",
      "\t\tValidation Batch 20: Loss: 0.5463651418685913; Accuracy: 0.7666666666666667\n",
      "Validation:\n",
      "\tLoss=0.6181709736585617; Accuracy: 0.6867708333333333\n",
      "Training epoch #12\n",
      "\t\tTraining Batch 10: Loss: 0.32826703786849976; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.3137226700782776; Accuracy: 1.0\n",
      "\t\tTraining Batch 30: Loss: 0.37813863158226013; Accuracy: 0.9375\n",
      "\t\tTraining Batch 40: Loss: 0.37705788016319275; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.350525826215744; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.31506744027137756; Accuracy: 1.0\n",
      "\t\tTraining Batch 70: Loss: 0.40922263264656067; Accuracy: 0.90625\n",
      "\t\tTraining Batch 80: Loss: 0.4006960690021515; Accuracy: 0.90625\n",
      "\t\tTraining Batch 90: Loss: 0.31512588262557983; Accuracy: 1.0\n",
      "\t\tTraining Batch 100: Loss: 0.3962518870830536; Accuracy: 0.90625\n",
      "\t\tTraining Batch 110: Loss: 0.3426712453365326; Accuracy: 0.96875\n",
      "\t\tTraining Batch 120: Loss: 0.3539349436759949; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.31803131103515625; Accuracy: 1.0\n",
      "\t\tTraining Batch 140: Loss: 0.37592023611068726; Accuracy: 0.9375\n",
      "\t\tTraining Batch 150: Loss: 0.3997581899166107; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.4066247045993805; Accuracy: 0.90625\n",
      "\t\tTraining Batch 170: Loss: 0.3210236430168152; Accuracy: 1.0\n",
      "Training:\n",
      "\tLoss: 0.361707902655882; Accuracy: 0.9512867647058824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.6913913488388062; Accuracy: 0.59375\n",
      "\t\tValidation Batch 20: Loss: 0.5779201984405518; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.6185237228870392; Accuracy: 0.6882291666666667\n",
      "Training epoch #13\n",
      "\t\tTraining Batch 10: Loss: 0.3241686522960663; Accuracy: 1.0\n",
      "\t\tTraining Batch 20: Loss: 0.386933833360672; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.3533742427825928; Accuracy: 0.9375\n",
      "\t\tTraining Batch 40: Loss: 0.3537701368331909; Accuracy: 0.96875\n",
      "\t\tTraining Batch 50: Loss: 0.3453270494937897; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.35276681184768677; Accuracy: 0.96875\n",
      "\t\tTraining Batch 70: Loss: 0.35093554854393005; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.3136276602745056; Accuracy: 1.0\n",
      "\t\tTraining Batch 90: Loss: 0.35614413022994995; Accuracy: 0.96875\n",
      "\t\tTraining Batch 100: Loss: 0.41678306460380554; Accuracy: 0.90625\n",
      "\t\tTraining Batch 110: Loss: 0.3753722012042999; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.35758674144744873; Accuracy: 0.9375\n",
      "\t\tTraining Batch 130: Loss: 0.3475813865661621; Accuracy: 0.96875\n",
      "\t\tTraining Batch 140: Loss: 0.39240533113479614; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.37801671028137207; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.34788778424263; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.3139072358608246; Accuracy: 1.0\n",
      "Training:\n",
      "\tLoss: 0.3558321384822621; Accuracy: 0.9584558823529412\n",
      "\t\tValidation Batch 10: Loss: 0.6127738356590271; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.5293018817901611; Accuracy: 0.8\n",
      "Validation:\n",
      "\tLoss=0.6141239047050476; Accuracy: 0.6915625000000001\n",
      "Training epoch #14\n",
      "\t\tTraining Batch 10: Loss: 0.31461301445961; Accuracy: 1.0\n",
      "\t\tTraining Batch 20: Loss: 0.31449583172798157; Accuracy: 1.0\n",
      "\t\tTraining Batch 30: Loss: 0.3183109760284424; Accuracy: 1.0\n",
      "\t\tTraining Batch 40: Loss: 0.3860280513763428; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.34503769874572754; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.37553614377975464; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.3710528314113617; Accuracy: 0.9375\n",
      "\t\tTraining Batch 80: Loss: 0.3464139699935913; Accuracy: 0.96875\n",
      "\t\tTraining Batch 90: Loss: 0.3253539502620697; Accuracy: 1.0\n",
      "\t\tTraining Batch 100: Loss: 0.38416868448257446; Accuracy: 0.9375\n",
      "\t\tTraining Batch 110: Loss: 0.37662172317504883; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.3464350402355194; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.34941405057907104; Accuracy: 0.96875\n",
      "\t\tTraining Batch 140: Loss: 0.33016592264175415; Accuracy: 0.96875\n",
      "\t\tTraining Batch 150: Loss: 0.34394770860671997; Accuracy: 0.96875\n",
      "\t\tTraining Batch 160: Loss: 0.3583083748817444; Accuracy: 0.90625\n",
      "\t\tTraining Batch 170: Loss: 0.36800479888916016; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.35590136244016535; Accuracy: 0.9577941176470588\n",
      "\t\tValidation Batch 10: Loss: 0.6118580102920532; Accuracy: 0.6875\n",
      "\t\tValidation Batch 20: Loss: 0.5478518605232239; Accuracy: 0.7666666666666667\n",
      "Validation:\n",
      "\tLoss=0.6063503503799439; Accuracy: 0.7023958333333333\n",
      "Training epoch #15\n",
      "\t\tTraining Batch 10: Loss: 0.35621345043182373; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.35642048716545105; Accuracy: 0.96875\n",
      "\t\tTraining Batch 30: Loss: 0.3910631835460663; Accuracy: 0.90625\n",
      "\t\tTraining Batch 40: Loss: 0.3427913188934326; Accuracy: 0.96875\n",
      "\t\tTraining Batch 50: Loss: 0.35080257058143616; Accuracy: 0.96875\n",
      "\t\tTraining Batch 60: Loss: 0.34493178129196167; Accuracy: 0.96875\n",
      "\t\tTraining Batch 70: Loss: 0.3558397889137268; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.38012391328811646; Accuracy: 0.9375\n",
      "\t\tTraining Batch 90: Loss: 0.37776845693588257; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.3450285494327545; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.3135301172733307; Accuracy: 1.0\n",
      "\t\tTraining Batch 120: Loss: 0.42396050691604614; Accuracy: 0.875\n",
      "\t\tTraining Batch 130: Loss: 0.349041223526001; Accuracy: 0.96875\n",
      "\t\tTraining Batch 140: Loss: 0.40636321902275085; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.3290480077266693; Accuracy: 0.96875\n",
      "\t\tTraining Batch 160: Loss: 0.34608206152915955; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.3137468695640564; Accuracy: 1.0\n",
      "Training:\n",
      "\tLoss: 0.3539998594452353; Accuracy: 0.9590073529411764\n",
      "\t\tValidation Batch 10: Loss: 0.6462494134902954; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.5317288041114807; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.61251220703125; Accuracy: 0.6929166666666666\n",
      "Best accuracy (0.71) obtained at epoch #10.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [

    "#Accuracy desired for an A\n",
    "MIN_ACCURACY = 0.70 #Based on the average accuracy on the training loop\n",

    "REACHED_MIN_ACCURACY = False\n",
    "best_weights = class_model.state_dict()\n",
    "max_val_acc = (0, 0)\n",
    "#Put the model in the GPU\n",
    "class_model.cuda()\n",
    "\n",
    "#Optimizer: changing the weights to make the moder optimal\n",
    "param_optimizer = list(class_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "#Optimizer comes from hugging bert models\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "#Storing accuracy and loss\n",
    "fit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "epoch_number = 0\n",
    "epoch_since_max = 0\n",
    "continue_learning = True\n",
    "\n",
    "#This loop goes through the training process for each of the epochs\n",
    "while epoch_number < EPOCHS and continue_learning:\n",
    "    epoch_number += 1\n",
    "    print(f\"Training epoch #{epoch_number}\")\n",
    "    #Tracking variables\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    #Set the model to training mode so we can train it\n",
    "    class_model.train()\n",
    "    #Set the final weights\n",
    "    class_model.embedder.requires_grad_ = False\n",
    "    \n",
    "    #This for loop goes through each of the batches in the epochs for training \n",
    "    #This loop trains each batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #Use the GPU to train the batch\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        #Get the items to be used from the data loader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        #Clear out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Forward training\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n",
    "        #Backward training\n",
    "        loss.backward()\n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        #Update data to the CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        #Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids)\n",
    "        #Append to fit history\n",
    "        fit_history[\"loss\"].append(loss.item()) \n",
    "        fit_history[\"accuracy\"].append(b_accuracy) \n",
    "        #Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        tr_accuracy += b_accuracy\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        #This prints the current batch's loss and accuracy\n",
    "        if nb_tr_steps%10 == 0:\n",
    "            print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n",
    "    print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n",
    "    #Set model to evaluation mode so we can evaluate without training\n",
    "    \n",
    "    class_model.eval()\n",
    "    #Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        #Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        #Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "        #not computing gradients\n",
    "        with torch.no_grad():\n",
    "            #Forward pass, calculate logit predictions\n",
    "            loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "        #Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        #Calculate the accuracy\n",
    "        b_accuracy = flat_accuracy(logits, label_ids)\n",
    "        #Append to fit history\n",
    "        fit_history[\"val_loss\"].append(loss.item()) \n",
    "        fit_history[\"val_accuracy\"].append(b_accuracy) \n",
    "        #Update tracking variables\n",
    "        eval_loss += loss.item()\n",
    "        eval_accuracy += b_accuracy\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        if nb_eval_steps%10 == 0:\n",
    "            print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "            \n",
    "    #This section of the code is to determine whether we need to keep training or not\n",
    "    #i.e. if we exceed the min acuracy needed we stop training or if we meet the epoch number we specified previously\n",
    "    eval_acc = eval_accuracy/nb_eval_steps\n",
    "    if eval_acc >= max_val_acc[0]:\n",
    "        max_val_acc = (eval_acc, epoch_number)\n",
    "        continue_learning = True\n",
    "        epoch_since_max = 0\n",
    "        #This records the best weights to be added to the trained model\n",
    "        best_weights = copy.deepcopy(class_model.state_dict())\n",
    "        #See if we have reached min_accuracy\n",
    "        if eval_acc >= MIN_ACCURACY:\n",
    "            REACHED_MIN_ACCURACY = True\n",
    "        #When it has reached min accuracy we want to end the learning process\n",
    "        if REACHED_MIN_ACCURACY:\n",
    "            continue_learning = False # No necessary to continue learning\n",
    "    else:\n",
    "        epoch_since_max += 1\n",
    "        #If the desired accuracy isn't met, then we stop it with the patience value\n",
    "        if epoch_since_max > PATIENCE:\n",
    "            continue_learning = False\n",
    "    print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "print(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")\n",
    "#Reload the best weights (from memory)\n",
    "class_model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad570f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_predictions(preds):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    return pred_flat == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1426adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard\\AppData\\Local\\Temp/ipykernel_5108/3205015112.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tValidation Batch 10: Loss: 0.6037158370018005; Accuracy: 0.6875\n",
      "\t\tValidation Batch 20: Loss: 0.5955640077590942; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.5981480151414871; Accuracy: 0.71\n",
      "OrderedDict([(0, True), (1, True), (2, False), (3, True), (4, True), (5, True), (6, True), (7, False), (8, True), (9, False), (10, True), (11, True), (12, True), (13, False), (14, False), (15, True), (16, True), (17, False), (18, True), (19, True), (20, True), (21, False), (22, True), (23, True), (24, True), (25, True), (26, False), (27, False), (28, True), (29, True), (30, True), (31, True), (32, True), (33, True), (34, False), (35, True), (36, False), (37, True), (38, True), (39, True), (40, False), (41, True), (42, True), (43, True), (44, True), (45, True), (46, False), (47, True), (48, False), (49, True), (50, True), (51, True), (52, False), (53, True), (54, True), (55, True), (56, True), (57, False), (58, False), (59, True), (60, True), (61, True), (62, True), (63, False), (64, False), (65, False), (66, True), (67, False), (68, True), (69, True), (70, False), (71, True), (72, True), (73, False), (74, False), (75, True), (76, False), (77, True), (78, True), (79, True), (80, False), (81, True), (82, True), (83, True), (84, True), (85, True), (86, True), (87, True), (88, True), (89, False), (90, True), (91, True), (92, False), (93, False), (94, False), (95, False), (96, True), (97, False), (98, True), (99, True), (100, True), (101, False), (102, True), (103, False), (104, False), (105, True), (106, False), (107, True), (108, True), (109, True), (110, True), (111, True), (112, True), (113, True), (114, True), (115, True), (116, True), (117, True), (118, True), (119, True), (120, True), (121, True), (122, True), (123, False), (124, False), (125, False), (126, False), (127, False), (128, True), (129, True), (130, True), (131, True), (132, True), (133, True), (134, True), (135, False), (136, True), (137, True), (138, False), (139, False), (140, True), (141, False), (142, True), (143, True), (144, False), (145, True), (146, True), (147, True), (148, True), (149, True), (150, True), (151, True), (152, True), (153, False), (154, False), (155, False), (156, True), (157, True), (158, True), (159, True), (160, True), (161, True), (162, True), (163, True), (164, True), (165, True), (166, False), (167, False), (168, False), (169, True), (170, True), (171, True), (172, True), (173, True), (174, True), (175, True), (176, True), (177, True), (178, False), (179, False), (180, True), (181, True), (182, False), (183, False), (184, True), (185, True), (186, True), (187, True), (188, True), (189, True), (190, True), (191, False), (192, True), (193, True), (194, True), (195, False), (196, True), (197, False), (198, True), (199, False), (200, True), (201, True), (202, True), (203, True), (204, True), (205, True), (206, True), (207, True), (208, True), (209, True), (210, True), (211, False), (212, True), (213, False), (214, True), (215, True), (216, False), (217, True), (218, True), (219, True), (220, True), (221, True), (222, True), (223, True), (224, False), (225, True), (226, False), (227, False), (228, True), (229, True), (230, True), (231, True), (232, True), (233, False), (234, True), (235, False), (236, False), (237, True), (238, True), (239, True), (240, False), (241, True), (242, True), (243, False), (244, True), (245, False), (246, True), (247, True), (248, True), (249, True), (250, False), (251, True), (252, True), (253, True), (254, False), (255, True), (256, True), (257, False), (258, True), (259, True), (260, True), (261, True), (262, True), (263, False), (264, True), (265, False), (266, True), (267, False), (268, True), (269, True), (270, True), (271, False), (272, True), (273, True), (274, True), (275, True), (276, True), (277, True), (278, True), (279, True), (280, True), (281, True), (282, True), (283, False), (284, True), (285, True), (286, True), (287, True), (288, False), (289, True), (290, True), (291, False), (292, False), (293, False), (294, True), (295, True), (296, True), (297, False), (298, True), (299, True), (300, True), (301, True), (302, True), (303, True), (304, True), (305, True), (306, False), (307, True), (308, True), (309, True), (310, False), (311, True), (312, True), (313, False), (314, True), (315, True), (316, False), (317, True), (318, True), (319, False), (320, True), (321, False), (322, True), (323, True), (324, True), (325, True), (326, True), (327, True), (328, True), (329, True), (330, False), (331, True), (332, True), (333, True), (334, True), (335, True), (336, False), (337, False), (338, True), (339, True), (340, True), (341, False), (342, False), (343, True), (344, True), (345, False), (346, True), (347, True), (348, False), (349, True), (350, True), (351, True), (352, True), (353, True), (354, True), (355, False), (356, False), (357, True), (358, False), (359, True), (360, True), (361, True), (362, True), (363, True), (364, True), (365, True), (366, False), (367, False), (368, False), (369, True), (370, False), (371, False), (372, True), (373, False), (374, True), (375, False), (376, True), (377, True), (378, False), (379, True), (380, True), (381, True), (382, True), (383, True), (384, True), (385, False), (386, True), (387, True), (388, True), (389, True), (390, True), (391, True), (392, True), (393, True), (394, True), (395, True), (396, True), (397, True), (398, True), (399, True), (400, True), (401, True), (402, False), (403, False), (404, True), (405, True), (406, True), (407, False), (408, True), (409, False), (410, False), (411, True), (412, True), (413, True), (414, True), (415, False), (416, True), (417, True), (418, True), (419, True), (420, True), (421, False), (422, True), (423, True), (424, False), (425, True), (426, False), (427, False), (428, False), (429, True), (430, True), (431, True), (432, True), (433, True), (434, False), (435, True), (436, False), (437, True), (438, True), (439, True), (440, True), (441, False), (442, True), (443, False), (444, True), (445, True), (446, False), (447, False), (448, False), (449, True), (450, True), (451, True), (452, False), (453, True), (454, True), (455, True), (456, True), (457, True), (458, True), (459, True), (460, True), (461, True), (462, True), (463, True), (464, True), (465, True), (466, True), (467, False), (468, True), (469, True), (470, False), (471, False), (472, True), (473, True), (474, True), (475, True), (476, False), (477, True), (478, True), (479, True), (480, False), (481, False), (482, False), (483, True), (484, True), (485, False), (486, True), (487, True), (488, True), (489, False), (490, True), (491, False), (492, True), (493, True), (494, True), (495, True), (496, False), (497, True), (498, True), (499, False), (500, True), (501, True), (502, True), (503, True), (504, False), (505, False), (506, False), (507, True), (508, True), (509, True), (510, True), (511, True), (512, False), (513, True), (514, True), (515, False), (516, False), (517, True), (518, True), (519, False), (520, True), (521, False), (522, True), (523, True), (524, True), (525, True), (526, False), (527, True), (528, False), (529, True), (530, False), (531, True), (532, True), (533, True), (534, True), (535, False), (536, False), (537, True), (538, True), (539, False), (540, True), (541, True), (542, False), (543, True), (544, True), (545, True), (546, True), (547, True), (548, True), (549, True), (550, False), (551, False), (552, False), (553, False), (554, True), (555, False), (556, True), (557, False), (558, False), (559, True), (560, True), (561, False), (562, False), (563, True), (564, True), (565, True), (566, True), (567, True), (568, False), (569, True), (570, False), (571, True), (572, False), (573, True), (574, True), (575, True), (576, True), (577, False), (578, False), (579, True), (580, True), (581, False), (582, True), (583, True), (584, True), (585, True), (586, True), (587, False), (588, True), (589, True), (590, True), (591, True), (592, True), (593, False), (594, False), (595, False), (596, True), (597, True), (598, True), (599, True), (600, True), (601, True), (602, True), (603, True), (604, False), (605, True), (606, True), (607, True), (608, True), (609, True), (610, False), (611, False), (612, True), (613, True), (614, True), (615, True), (616, True), (617, True), (618, False), (619, False), (620, False), (621, True), (622, False), (623, True), (624, True), (625, False), (626, False), (627, True), (628, True), (629, True), (630, True), (631, True), (632, True), (633, True), (634, True), (635, True), (636, False), (637, True)])\n"
     ]
    }
   ],
   "source": [
    "validation_predictions_correctness = {}\n",
    "# Validation\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Put model in evaluation mode\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        #loss, logits = class_model(b_input_ids, token_type_ids=b_token_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = class_model(b_input_ids, attention_mask=b_input_mask, \n",
    "                                    labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.cpu().numpy()\n",
    "    # Calculate the accuracy\n",
    "    b_accuracy, b_pred_correctness = flat_accuracy(logits, label_ids, return_predict_correctness = True) # For RobertaForClassification\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    # Add to predictions\n",
    "    for index, pred in zip(indexes, b_pred_correctness):\n",
    "        validation_predictions_correctness[index] = pred\n",
    "    # Update tracking variables\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    if nb_eval_steps%10 == 0:\n",
    "        print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "validation_predictions_correctness = collections.OrderedDict(sorted(validation_predictions_correctness.items()))\n",
    "print(validation_predictions_correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce44e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard\\AppData\\Local\\Temp/ipykernel_5108/3205015112.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTest Batch 10\n",
      "\t\tTest Batch 20\n",
      "\t\tTest Batch 30\n",
      "\t\tTest Batch 40\n",
      "Testing results\n",
      "OrderedDict([(0, True), (1, False), (2, True), (3, True), (4, False), (5, False), (6, False), (7, False), (8, True), (9, True), (10, True), (11, False), (12, False), (13, False), (14, True), (15, True), (16, True), (17, False), (18, True), (19, True), (20, False), (21, False), (22, True), (23, True), (24, False), (25, False), (26, True), (27, True), (28, True), (29, True), (30, True), (31, False), (32, False), (33, False), (34, False), (35, False), (36, False), (37, False), (38, True), (39, False), (40, True), (41, False), (42, True), (43, True), (44, True), (45, True), (46, False), (47, False), (48, False), (49, True), (50, True), (51, True), (52, True), (53, True), (54, True), (55, False), (56, False), (57, False), (58, True), (59, False), (60, True), (61, False), (62, True), (63, False), (64, True), (65, False), (66, False), (67, True), (68, True), (69, True), (70, False), (71, False), (72, False), (73, True), (74, False), (75, False), (76, False), (77, False), (78, True), (79, True), (80, True), (81, True), (82, False), (83, False), (84, False), (85, False), (86, True), (87, False), (88, True), (89, True), (90, True), (91, True), (92, False), (93, True), (94, False), (95, True), (96, True), (97, True), (98, True), (99, False), (100, True), (101, False), (102, True), (103, False), (104, True), (105, False), (106, False), (107, True), (108, True), (109, True), (110, True), (111, True), (112, True), (113, False), (114, False), (115, True), (116, True), (117, False), (118, False), (119, True), (120, True), (121, True), (122, True), (123, False), (124, False), (125, True), (126, True), (127, False), (128, True), (129, False), (130, False), (131, True), (132, False), (133, True), (134, True), (135, True), (136, False), (137, False), (138, False), (139, True), (140, False), (141, True), (142, True), (143, True), (144, True), (145, False), (146, True), (147, True), (148, True), (149, False), (150, False), (151, True), (152, True), (153, False), (154, False), (155, True), (156, True), (157, True), (158, True), (159, False), (160, True), (161, True), (162, False), (163, True), (164, True), (165, True), (166, False), (167, False), (168, False), (169, False), (170, False), (171, False), (172, True), (173, True), (174, False), (175, True), (176, True), (177, False), (178, True), (179, True), (180, True), (181, False), (182, False), (183, False), (184, True), (185, True), (186, True), (187, False), (188, False), (189, True), (190, False), (191, True), (192, True), (193, False), (194, True), (195, True), (196, True), (197, True), (198, False), (199, True), (200, True), (201, False), (202, True), (203, True), (204, False), (205, False), (206, True), (207, False), (208, True), (209, False), (210, True), (211, True), (212, True), (213, True), (214, True), (215, False), (216, False), (217, True), (218, True), (219, True), (220, True), (221, True), (222, False), (223, False), (224, True), (225, True), (226, True), (227, False), (228, False), (229, False), (230, True), (231, True), (232, False), (233, True), (234, True), (235, False), (236, True), (237, True), (238, False), (239, False), (240, False), (241, True), (242, True), (243, True), (244, True), (245, False), (246, False), (247, True), (248, False), (249, True), (250, True), (251, False), (252, True), (253, True), (254, False), (255, True), (256, True), (257, True), (258, True), (259, True), (260, False), (261, True), (262, True), (263, True), (264, False), (265, True), (266, False), (267, True), (268, True), (269, True), (270, True), (271, False), (272, False), (273, False), (274, True), (275, False), (276, False), (277, False), (278, False), (279, True), (280, True), (281, False), (282, True), (283, False), (284, True), (285, False), (286, False), (287, True), (288, True), (289, True), (290, True), (291, False), (292, True), (293, False), (294, False), (295, False), (296, True), (297, True), (298, True), (299, False), (300, True), (301, False), (302, True), (303, False), (304, False), (305, True), (306, False), (307, False), (308, True), (309, True), (310, True), (311, True), (312, True), (313, False), (314, True), (315, False), (316, False), (317, False), (318, True), (319, True), (320, False), (321, True), (322, False), (323, False), (324, True), (325, True), (326, False), (327, False), (328, True), (329, False), (330, True), (331, True), (332, True), (333, False), (334, False), (335, False), (336, False), (337, False), (338, False), (339, False), (340, True), (341, False), (342, False), (343, True), (344, True), (345, False), (346, False), (347, False), (348, False), (349, False), (350, False), (351, False), (352, True), (353, False), (354, True), (355, True), (356, True), (357, True), (358, True), (359, False), (360, True), (361, True), (362, False), (363, True), (364, True), (365, True), (366, False), (367, True), (368, False), (369, False), (370, True), (371, False), (372, True), (373, False), (374, False), (375, False), (376, True), (377, True), (378, False), (379, True), (380, True), (381, True), (382, True), (383, True), (384, False), (385, False), (386, False), (387, True), (388, True), (389, False), (390, False), (391, False), (392, False), (393, False), (394, True), (395, True), (396, True), (397, True), (398, False), (399, True), (400, False), (401, True), (402, True), (403, False), (404, True), (405, True), (406, True), (407, True), (408, True), (409, False), (410, False), (411, False), (412, True), (413, True), (414, False), (415, False), (416, True), (417, True), (418, False), (419, True), (420, True), (421, True), (422, True), (423, False), (424, True), (425, True), (426, False), (427, True), (428, True), (429, True), (430, False), (431, True), (432, True), (433, True), (434, True), (435, False), (436, True), (437, False), (438, False), (439, False), (440, True), (441, False), (442, True), (443, False), (444, False), (445, False), (446, True), (447, False), (448, False), (449, True), (450, True), (451, False), (452, True), (453, True), (454, True), (455, False), (456, False), (457, True), (458, True), (459, False), (460, True), (461, False), (462, False), (463, False), (464, False), (465, True), (466, True), (467, False), (468, True), (469, True), (470, False), (471, False), (472, False), (473, True), (474, False), (475, False), (476, True), (477, True), (478, True), (479, True), (480, False), (481, True), (482, False), (483, False), (484, True), (485, True), (486, False), (487, False), (488, True), (489, False), (490, True), (491, True), (492, False), (493, True), (494, False), (495, True), (496, True), (497, False), (498, False), (499, True), (500, True), (501, True), (502, False), (503, True), (504, True), (505, True), (506, False), (507, True), (508, True), (509, False), (510, False), (511, True), (512, True), (513, True), (514, True), (515, False), (516, True), (517, False), (518, True), (519, False), (520, True), (521, True), (522, True), (523, True), (524, True), (525, True), (526, True), (527, True), (528, True), (529, True), (530, True), (531, True), (532, True), (533, True), (534, False), (535, False), (536, True), (537, False), (538, False), (539, False), (540, True), (541, True), (542, True), (543, False), (544, True), (545, True), (546, False), (547, True), (548, True), (549, True), (550, False), (551, True), (552, True), (553, False), (554, True), (555, False), (556, False), (557, False), (558, True), (559, True), (560, True), (561, False), (562, False), (563, False), (564, False), (565, True), (566, False), (567, True), (568, True), (569, False), (570, False), (571, True), (572, True), (573, False), (574, True), (575, True), (576, False), (577, True), (578, False), (579, True), (580, True), (581, True), (582, False), (583, True), (584, False), (585, True), (586, True), (587, True), (588, True), (589, False), (590, True), (591, True), (592, False), (593, False), (594, False), (595, True), (596, False), (597, False), (598, True), (599, False), (600, False), (601, True), (602, False), (603, False), (604, True), (605, True), (606, True), (607, True), (608, False), (609, True), (610, False), (611, True), (612, False), (613, True), (614, False), (615, False), (616, True), (617, False), (618, False), (619, True), (620, True), (621, False), (622, True), (623, False), (624, True), (625, True), (626, True), (627, True), (628, False), (629, True), (630, False), (631, False), (632, True), (633, True), (634, True), (635, True), (636, False), (637, False), (638, True), (639, True), (640, True), (641, True), (642, True), (643, True), (644, False), (645, True), (646, True), (647, True), (648, False), (649, True), (650, True), (651, False), (652, True), (653, True), (654, False), (655, True), (656, True), (657, True), (658, True), (659, False), (660, False), (661, False), (662, True), (663, False), (664, False), (665, True), (666, False), (667, True), (668, True), (669, False), (670, False), (671, True), (672, False), (673, True), (674, False), (675, True), (676, False), (677, True), (678, True), (679, True), (680, True), (681, False), (682, True), (683, False), (684, True), (685, True), (686, False), (687, False), (688, False), (689, True), (690, True), (691, False), (692, True), (693, True), (694, True), (695, False), (696, False), (697, False), (698, True), (699, True), (700, False), (701, False), (702, True), (703, True), (704, True), (705, False), (706, True), (707, True), (708, True), (709, True), (710, True), (711, False), (712, True), (713, False), (714, False), (715, True), (716, True), (717, True), (718, True), (719, True), (720, False), (721, False), (722, True), (723, True), (724, True), (725, True), (726, False), (727, True), (728, False), (729, False), (730, False), (731, True), (732, True), (733, True), (734, False), (735, True), (736, True), (737, True), (738, False), (739, True), (740, False), (741, False), (742, True), (743, False), (744, True), (745, True), (746, True), (747, True), (748, False), (749, False), (750, True), (751, True), (752, True), (753, True), (754, True), (755, True), (756, True), (757, True), (758, True), (759, True), (760, True), (761, False), (762, True), (763, True), (764, False), (765, True), (766, True), (767, True), (768, False), (769, False), (770, True), (771, True), (772, False), (773, False), (774, True), (775, False), (776, True), (777, False), (778, True), (779, True), (780, False), (781, True), (782, True), (783, False), (784, True), (785, True), (786, True), (787, True), (788, False), (789, True), (790, False), (791, False), (792, False), (793, True), (794, False), (795, True), (796, False), (797, False), (798, False), (799, True), (800, True), (801, False), (802, True), (803, False), (804, True), (805, True), (806, False), (807, True), (808, True), (809, False), (810, True), (811, False), (812, False), (813, True), (814, False), (815, True), (816, False), (817, True), (818, True), (819, True), (820, True), (821, False), (822, True), (823, True), (824, True), (825, False), (826, False), (827, False), (828, False), (829, False), (830, False), (831, True), (832, True), (833, True), (834, True), (835, True), (836, True), (837, False), (838, True), (839, True), (840, True), (841, False), (842, True), (843, False), (844, False), (845, True), (846, False), (847, False), (848, False), (849, True), (850, False), (851, True), (852, True), (853, False), (854, True), (855, False), (856, True), (857, False), (858, True), (859, True), (860, True), (861, True), (862, False), (863, False), (864, True), (865, True), (866, True), (867, True), (868, True), (869, False), (870, True), (871, False), (872, True), (873, False), (874, True), (875, True), (876, False), (877, True), (878, True), (879, False), (880, False), (881, True), (882, True), (883, True), (884, True), (885, True), (886, False), (887, True), (888, False), (889, False), (890, True), (891, False), (892, True), (893, True), (894, False), (895, True), (896, False), (897, False), (898, True), (899, True), (900, True), (901, True), (902, True), (903, True), (904, False), (905, False), (906, True), (907, False), (908, False), (909, False), (910, True), (911, True), (912, False), (913, False), (914, True), (915, True), (916, False), (917, True), (918, False), (919, True), (920, True), (921, False), (922, True), (923, True), (924, True), (925, False), (926, True), (927, True), (928, True), (929, True), (930, False), (931, True), (932, True), (933, True), (934, True), (935, True), (936, True), (937, True), (938, True), (939, True), (940, False), (941, True), (942, True), (943, False), (944, False), (945, True), (946, True), (947, False), (948, True), (949, True), (950, True), (951, False), (952, True), (953, False), (954, True), (955, True), (956, True), (957, False), (958, True), (959, False), (960, False), (961, True), (962, True), (963, True), (964, True), (965, False), (966, True), (967, False), (968, False), (969, False), (970, True), (971, True), (972, True), (973, True), (974, True), (975, False), (976, True), (977, True), (978, False), (979, True), (980, False), (981, False), (982, False), (983, True), (984, False), (985, True), (986, True), (987, True), (988, True), (989, True), (990, False), (991, True), (992, True), (993, False), (994, False), (995, True), (996, True), (997, True), (998, True), (999, True), (1000, True), (1001, True), (1002, False), (1003, False), (1004, True), (1005, False), (1006, True), (1007, True), (1008, False), (1009, False), (1010, True), (1011, False), (1012, False), (1013, True), (1014, True), (1015, False), (1016, False), (1017, True), (1018, True), (1019, False), (1020, False), (1021, True), (1022, True), (1023, True), (1024, True), (1025, True), (1026, True), (1027, False), (1028, False), (1029, False), (1030, False), (1031, False), (1032, True), (1033, True), (1034, True), (1035, True), (1036, True), (1037, True), (1038, False), (1039, False), (1040, False), (1041, True), (1042, True), (1043, True), (1044, True), (1045, True), (1046, True), (1047, True), (1048, True), (1049, False), (1050, False), (1051, False), (1052, False), (1053, True), (1054, False), (1055, False), (1056, True), (1057, False), (1058, False), (1059, False), (1060, True), (1061, True), (1062, False), (1063, True), (1064, False), (1065, False), (1066, False), (1067, True), (1068, True), (1069, False), (1070, True), (1071, True), (1072, False), (1073, True), (1074, True), (1075, True), (1076, False), (1077, True), (1078, False), (1079, True), (1080, True), (1081, True), (1082, False), (1083, True), (1084, True), (1085, False), (1086, True), (1087, False), (1088, True), (1089, True), (1090, False), (1091, False), (1092, True), (1093, False), (1094, True), (1095, False), (1096, True), (1097, False), (1098, True), (1099, False), (1100, True), (1101, False), (1102, False), (1103, False), (1104, True), (1105, True), (1106, True), (1107, True), (1108, False), (1109, False), (1110, True), (1111, True), (1112, False), (1113, False), (1114, True), (1115, False), (1116, False), (1117, False), (1118, False), (1119, True), (1120, True), (1121, True), (1122, False), (1123, False), (1124, True), (1125, True), (1126, False), (1127, True), (1128, False), (1129, True), (1130, True), (1131, True), (1132, True), (1133, True), (1134, False), (1135, False), (1136, True), (1137, False), (1138, False), (1139, False), (1140, False), (1141, True), (1142, False), (1143, False), (1144, True), (1145, True), (1146, True), (1147, True), (1148, False), (1149, False), (1150, True), (1151, True), (1152, False), (1153, True), (1154, False), (1155, True), (1156, True), (1157, False), (1158, False), (1159, False), (1160, True), (1161, False), (1162, False), (1163, False), (1164, False), (1165, True), (1166, True), (1167, True), (1168, True), (1169, True), (1170, False), (1171, False), (1172, False), (1173, True), (1174, True), (1175, True), (1176, False), (1177, True), (1178, True), (1179, True), (1180, True), (1181, True), (1182, True), (1183, False), (1184, True), (1185, True), (1186, False), (1187, True), (1188, False), (1189, False), (1190, True), (1191, False), (1192, True), (1193, True), (1194, True), (1195, True), (1196, True), (1197, True), (1198, False), (1199, True), (1200, True), (1201, False), (1202, True), (1203, False), (1204, True), (1205, False), (1206, True), (1207, True), (1208, True), (1209, False), (1210, True), (1211, True), (1212, True), (1213, False), (1214, False), (1215, True), (1216, True), (1217, False), (1218, False), (1219, False), (1220, False), (1221, True), (1222, False), (1223, True), (1224, True), (1225, False), (1226, True), (1227, False), (1228, True), (1229, False), (1230, False), (1231, False), (1232, False), (1233, True), (1234, False), (1235, True), (1236, True), (1237, True), (1238, False), (1239, False), (1240, False), (1241, True), (1242, True), (1243, True), (1244, True), (1245, True), (1246, True), (1247, True), (1248, True), (1249, False), (1250, False), (1251, False), (1252, True), (1253, True), (1254, True), (1255, True), (1256, False), (1257, False), (1258, False), (1259, True), (1260, True), (1261, True), (1262, True), (1263, True), (1264, False), (1265, False), (1266, True), (1267, False), (1268, True), (1269, True), (1270, True), (1271, False), (1272, False), (1273, True), (1274, False), (1275, False), (1276, True), (1277, True), (1278, True), (1279, True), (1280, False), (1281, True), (1282, True), (1283, True), (1284, False), (1285, True), (1286, True), (1287, True), (1288, True), (1289, False), (1290, False), (1291, True), (1292, False), (1293, True), (1294, True), (1295, False), (1296, True), (1297, False), (1298, True), (1299, False), (1300, True), (1301, False), (1302, False), (1303, False), (1304, False), (1305, False), (1306, True), (1307, True), (1308, False), (1309, True), (1310, True), (1311, True), (1312, False), (1313, True), (1314, True), (1315, True), (1316, True), (1317, False), (1318, False), (1319, True), (1320, False), (1321, False), (1322, True), (1323, False), (1324, True), (1325, False), (1326, True), (1327, False), (1328, True), (1329, True), (1330, False), (1331, False), (1332, False), (1333, False), (1334, False), (1335, True), (1336, False), (1337, False), (1338, True), (1339, True), (1340, False), (1341, True), (1342, False), (1343, True), (1344, True), (1345, True), (1346, True), (1347, True), (1348, True), (1349, False), (1350, True), (1351, True), (1352, True), (1353, True), (1354, False), (1355, False), (1356, False), (1357, False), (1358, True), (1359, False), (1360, True), (1361, True), (1362, True), (1363, True), (1364, True), (1365, False), (1366, False), (1367, True), (1368, False), (1369, False), (1370, True), (1371, True), (1372, True), (1373, False), (1374, True), (1375, True), (1376, False), (1377, True), (1378, True), (1379, True), (1380, True), (1381, True), (1382, True), (1383, False), (1384, True), (1385, True), (1386, False), (1387, False), (1388, False), (1389, False), (1390, False), (1391, False), (1392, False), (1393, False), (1394, False), (1395, True), (1396, False), (1397, True), (1398, True), (1399, False)])\n"
     ]
    }
   ],
   "source": [
    "test_predictions = {}\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_examples, nb_test_steps = 0, 0\n",
    "# Testing\n",
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "class_model.eval()\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_word1, b_word2, b_index = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = class_model(b_input_ids, attention_mask=b_input_mask, word1_locs = b_word1, word2_locs = b_word2)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # Get the predictions\n",
    "    b_preds = flat_predictions(logits)\n",
    "    indexes = b_index.detach().cpu().numpy() # Get the indexes\n",
    "    for index, pred in zip(indexes, b_preds):\n",
    "        test_predictions[index] = pred\n",
    "    # Update tracking variables\n",
    "    test_loss += loss.item()\n",
    "    test_accuracy += b_accuracy\n",
    "    nb_test_examples += b_input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    if nb_test_steps%10 == 0:\n",
    "        print(\"\\t\\tTest Batch {}\".format(nb_test_steps))\n",
    "# Print final results\n",
    "print(\"Testing results\")\n",
    "test_predictions = collections.OrderedDict(sorted(test_predictions.items()))\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80cf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",

   "version": "3.8.11"
    
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
